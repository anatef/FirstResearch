{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Basic imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import pickle\n",
    "from collections import defaultdict\n",
    "from os import environ, getcwd\n",
    "import sys\n",
    "\n",
    "#Classifier imports\n",
    "from sklearn.neighbors import KNeighborsClassifier \n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "#ML framework imports\n",
    "from sklearn.metrics import auc, roc_auc_score, precision_recall_curve\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.preprocessing import scale\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "# Neural Net imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "\n",
    "#Import utils functions\n",
    "curr_dir = !pwd\n",
    "\n",
    "sys.path.append(curr_dir[0]+\"/utils\")\n",
    "from prop_threshold_funcs import create_negatives_datasets_combined, create_positives_datasets_combined\n",
    "from prediction_general_funcs import ligands, score_cols_suffix, get_features_cols, remove_unimportant_features\n",
    "from CV_funcs import add_domain_name_from_table_idx, calc_CV_idx_iterative\n",
    "from generate_hyperparameter_trials import *\n",
    "\n",
    "\n",
    "from IPython.core.display import HTML\n",
    "HTML(\"<style>.container { width:100% !important; }</style>\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading the input dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all samples positions #: 42535\n"
     ]
    }
   ],
   "source": [
    "curr_dir = !pwd\n",
    "pfam_version = \"31\"\n",
    "datafile_date = \"06.20.18\"\n",
    "input_path = curr_dir[0]+\"/domains_similarity/filtered_features_table/\"\n",
    "filename = \"windowed_positions_features_mediode_filter_\"+datafile_date+\".csv\"\n",
    "out_dir = \"mediode_NegLigand_NoFilter\"\n",
    "\n",
    "#flags for creating negatives\n",
    "zero_prop = True\n",
    "no_prop = True\n",
    "all_ligands = False\n",
    "prec_th_str = \"dna0.5_rna0.25_ion0.75\"\n",
    "folds_num = 5\n",
    "\n",
    "#Features table\n",
    "features_all = pd.read_csv(input_path+filename, sep='\\t', index_col=0)\n",
    "#Features columns names, without the labels (the binding scores)\n",
    "features_cols = get_features_cols(features_all)\n",
    "remove_unimportant_features(features_all, features_cols)\n",
    "\n",
    "print \"all samples positions #: \"+str(features_all.shape[0])\n",
    "\n",
    "#CV splits dictionary\n",
    "# with open(curr_dir[0]+\"/CV_splits/pfam-v\"+pfam_version+\"/domain_\"+str(folds_num)+\"_folds_\"+str(prec_th)+\"_prec_dict.pik\", 'rb') as handle:\n",
    "#         splits_dict = pickle.load(handle)\n",
    "with open(curr_dir[0]+\"/CV_splits/pfam-v\"+pfam_version+\"/domain_\"+str(folds_num)+\"_folds_combined_\"+prec_th_str+\"_prec_dict.pik\", 'rb') as handle:\n",
    "        splits_dict = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataset of negative examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dna non-binding #:41680\n",
      "dnabase non-binding #:42089\n",
      "dnabackbone non-binding #:41689\n",
      "dna combined non binding #: 41555\n",
      "rna non-binding #:41613\n",
      "rnabase non-binding #:41828\n",
      "rnabackbone non-binding #:41619\n",
      "rna combined non binding #: 41401\n",
      "peptide non-binding #:38794\n",
      "ion non-binding #:37525\n",
      "metabolite non-binding #:37463\n",
      "sm non-binding #:30978\n"
     ]
    }
   ],
   "source": [
    "ligands_negatives_df = create_negatives_datasets_combined(zero_prop, no_prop, features_all, features_cols, all_ligands)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Datasets of positive examples by ligand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dna #: 239\n",
      "dnabase #: 170\n",
      "dnabackbone #: 244\n",
      "dna combined #: 353\n",
      "rna #: 360\n",
      "rnabase #: 246\n",
      "rnabackbone #: 346\n",
      "rna combined #: 468\n",
      "peptide #: 462\n",
      "ion #: 350\n",
      "metabolite #: 504\n",
      "sm #: 708\n"
     ]
    }
   ],
   "source": [
    "ligands_positives_df = create_positives_datasets_combined(features_all, features_cols, all_ligands)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading env input for downsampler technique, ligand and classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ligand = ion\n",
      "fold = 1\n",
      "classifier_method = NN\n",
      "trial idx = 0\n"
     ]
    }
   ],
   "source": [
    "#Reading the ligand input\n",
    "try:\n",
    "    ligand = environ['ligand']\n",
    "except:\n",
    "    ligand = \"ion\"\n",
    "print \"ligand = \"+ligand\n",
    "    \n",
    "#Reading the downsampler input\n",
    "try: \n",
    "    fold = environ['fold']\n",
    "except:\n",
    "    fold = \"1\"\n",
    "print \"fold = \"+fold\n",
    "\n",
    "#Reading the classifier input\n",
    "try: \n",
    "    classifier_method = environ['classifier']\n",
    "except:\n",
    "    classifier_method = \"NN\"\n",
    "print \"classifier_method = \"+classifier_method\n",
    "\n",
    "# Reading the index to generate model\n",
    "try:\n",
    "    trial_idx = int(environ[\"trial\"])\n",
    "except:\n",
    "    trial_idx = 0\n",
    "print \"trial idx = \"+ str(trial_idx)\n",
    "\n",
    "if classifier_method == \"NN\":\n",
    "    try:        \n",
    "        learning_rate_ub = int(environ['learning_rate_ub'])\n",
    "        learning_rate_lb = int(environ['learning_rate_lb'])\n",
    "        batch_size_ub = int(environ['batch_size_ub'])\n",
    "        batch_size_lb = int(environ['batch_size_lb'])\n",
    "        weight_decay_ub = int(environ['weight_decay_ub'])\n",
    "        weight_decay_lb = int(environ['weight_decay_lb'])\n",
    "        beta_ub = float(environ['beta_ub'])\n",
    "        beta_lb = float(environ['beta_lb'])\n",
    "        hidden_units_1_ub = int(environ['hidden_units_1_ub'])\n",
    "        hidden_units_1_lb = int(environ['hidden_units_1_lb'])\n",
    "        hidden_units_2_ub = int(environ['hidden_units_2_ub'])\n",
    "        hidden_units_2_lb = int(environ['hidden_units_2_lb'])\n",
    "        \n",
    "        try:\n",
    "            sec_learning_rate_ub = int(environ['sec_learning_rate_ub'])\n",
    "            sec_learning_rate_lb = int(environ['sec_learning_rate_lb'])\n",
    "            lr_weight_1 = float(environ['lr_weight_1'])\n",
    "            lr_weight_2 = float(environ['lr_weight_2'])\n",
    "        except:\n",
    "            sec_learning_rate_ub = learning_rate_ub \n",
    "            sec_learning_rate_lb = learning_rate_lb\n",
    "            lr_weight_1 = 1\n",
    "            lr_weight_2 = 1\n",
    "            \n",
    "        try:\n",
    "            sec_batch_size_ub = int(environ['sec_batch_size_ub'])\n",
    "            sec_batch_size_lb = int(environ['sec_batch_size_lb'])\n",
    "            batch_size_weight_1 = float(environ['batch_size_weight_1'])\n",
    "            batch_size_weight_2 = float(environ['batch_size_weight_2'])\n",
    "            \n",
    "        except:\n",
    "            sec_batch_size_ub = batch_size_ub\n",
    "            sec_batch_size_lb = batch_size_lb\n",
    "            batch_size_weight_1 = 1\n",
    "            batch_size_weight_2 = 1\n",
    "            \n",
    "        try:\n",
    "            sec_weight_decay_ub = int(environ['sec_weight_decay_ub'])\n",
    "            sec_weight_decay_lb = int(environ['sec_weight_decay_lb'])\n",
    "            weight_decay_weight_1 = float(environ['weight_decay_weight_1'])\n",
    "            weight_decay_weight_2 = float(environ['weight_decay_weight_2'])\n",
    "            \n",
    "        except:\n",
    "            sec_weight_decay_ub = weight_decay_ub\n",
    "            sec_weight_decay_lb = weight_decay_lb\n",
    "            weight_decay_weight_1 = 1\n",
    "            weight_decay_weight_2 = 1\n",
    "        try:\n",
    "            sec_beta_ub = float(environ['sec_beta_ub'])\n",
    "            sec_beta_lb = float(environ['sec_beta_lb'])\n",
    "            beta_weight_1 = float(environ['beta_weight_1'])\n",
    "            beta_weight_2 = float(environ['beta_weight_2'])\n",
    "        except:\n",
    "            sec_beta_ub = beta_ub\n",
    "            sec_beta_lb = beta_lb\n",
    "            beta_weight_1 = 1\n",
    "            beta_weight_2 = 1\n",
    "            \n",
    "        try:\n",
    "            sec_hidden_units_1_ub = int(environ['sec_hidden_units_1_ub'])\n",
    "            sec_hidden_units_1_lb = int(environ['sec_hidden_units_1_lb'])\n",
    "            hidden_units_1_weight_1 = float(environ['hidden_units_1_weight_1'])\n",
    "            hidden_units_1_weight_2 = float(environ['hidden_units_1_weight_2'])\n",
    "        except:\n",
    "            sec_hidden_units_1_ub = hidden_units_1_ub\n",
    "            sec_hidden_units_1_lb = hidden_units_1_lb\n",
    "            hidden_units_1_weight_1 = 1\n",
    "            hidden_units_1_weight_2 = 1\n",
    "            \n",
    "        try:\n",
    "            sec_hidden_units_2_ub = int(environ['sec_hidden_units_2_ub'])\n",
    "            sec_hidden_units_2_lb = int(environ['sec_hidden_units_2_lb'])\n",
    "            hidden_units_2_weight_1 = float(environ['hidden_units_2_weight_1'])\n",
    "            hidden_units_2_weight_2 = float(environ['hidden_units_2_weight_2'])\n",
    "            \n",
    "        except: \n",
    "            sec_hidden_units_2_ub = hidden_units_2_ub\n",
    "            sec_hidden_units_2_lb = hidden_units_2_lb\n",
    "            hidden_units_2_weight_1 = 1\n",
    "            hidden_units_2_weight_2 = 1\n",
    "            \n",
    "            \n",
    "            \n",
    "\n",
    "    except:        \n",
    "        learning_rate_ub = -4\n",
    "        learning_rate_lb = -5\n",
    "        batch_size_ub = 150\n",
    "        batch_size_lb = 30\n",
    "        weight_decay_ub = -7\n",
    "        weight_decay_lb = -17\n",
    "        beta_ub = 0.95\n",
    "        beta_lb = 0.85\n",
    "        hidden_units_1_ub = 300\n",
    "        hidden_units_1_lb = 50\n",
    "        hidden_units_2_ub = 800\n",
    "        hidden_units_2_lb = 350\n",
    "        \n",
    "        sec_learning_rate_ub = -4\n",
    "        sec_learning_rate_lb = -5\n",
    "        sec_batch_size_ub = 400\n",
    "        sec_batch_size_lb = 200\n",
    "        sec_weight_decay_ub = -7\n",
    "        sec_weight_decay_lb = -17\n",
    "        sec_beta_ub = 0.95\n",
    "        sec_beta_lb = 0.85\n",
    "        sec_hidden_units_1_ub = 300\n",
    "        sec_hidden_units_1_lb = 50\n",
    "        sec_hidden_units_2_ub = 800\n",
    "        sec_hidden_units_2_lb = 350\n",
    "        \n",
    "        lr_weight_1 = 1\n",
    "        lr_weight_2 = 1\n",
    "        batch_size_weight_1 = 1\n",
    "        batch_size_weight_2 = 1\n",
    "        weight_decay_weight_1 = 1\n",
    "        weight_decay_weight_2 = 1\n",
    "        beta_weight_1 = 1\n",
    "        beta_weight_2 = 1\n",
    "        hidden_units_1_weight_1 = 1\n",
    "        hidden_units_1_weight_2 = 1\n",
    "        hidden_units_2_weight_1 = 1\n",
    "        hidden_units_2_weight_2 = 1\n",
    "        \n",
    "         \n",
    "        \n",
    "    \n",
    "\n",
    "elif classifier_method == \"XGB\":\n",
    "    \n",
    "    try:\n",
    "        max_depth_ub = int(environ[\"max_depth_ub\"])\n",
    "        max_depth_lb = int(environ[\"max_depth_lb\"])\n",
    "        min_child_weight_ub = int(environ[\"min_child_weight_ub\"])\n",
    "        min_child_weight_lb = int(environ[\"min_child_weight_lb\"])\n",
    "        colsample_bytree_ub = float(environ[\"colsample_bytree_ub\"])\n",
    "        colsample_bytree_lb = float(environ[\"colsample_bytree_lb\"])\n",
    "        gamma_ub = int(environ[\"gamma_ub\"])\n",
    "        gamma_lb = int(environ[\"gamma_lb\"])\n",
    "        learning_rate_ub = float(environ[\"learning_rate_ub\"])\n",
    "        learning_rate_lb = int(environ[\"learning_rate_lb\"])\n",
    "\n",
    "        try:\n",
    "            sec_max_depth_ub = int(environ[\"sec_max_depth_ub\"])\n",
    "            sec_max_depth_lb = int(environ[\"sec_max_depth_lb\"])\n",
    "            max_depth_weight_1 = float(environ[\"max_depth_weight_1\"])\n",
    "            max_depth_weight_2 = float(environ[\"max_depth_weight_2\"])\n",
    "        except:\n",
    "            sec_max_depth_ub = max_depth_ub\n",
    "            sec_max_depth_lb = max_depth_lb\n",
    "            max_depth_weight_1 = 1\n",
    "            max_depth_weight_2 = 1\n",
    "        try:\n",
    "            sec_min_child_weight_ub = int(environ['sec_min_child_weight_ub'])\n",
    "            sec_min_child_weight_lb = int(environ['sec_min_child_weight_lb'])\n",
    "            min_child_weight_weight_1 = float(environ[\"min_child_weight_weight_1\"])\n",
    "            min_child_weight_weight_2 = float(environ[\"min_child_weight_weight_2\"])\n",
    "            \n",
    "        except:\n",
    "            sec_min_child_weight_ub = min_child_weight_ub\n",
    "            sec_min_child_weight_lb = min_child_weight_lb\n",
    "            min_child_weight_weight_1 = 1\n",
    "            min_child_weight_weight_2 = 1\n",
    "        try:\n",
    "            sec_colsample_bytree_ub = float(environ['sec_colsample_bytree_ub'])\n",
    "            sec_colsample_bytree_lb = float(environ['sec_colsample_bytree_lb'])\n",
    "            colsample_bytree_weight_1 = float(environ['colsample_bytree_weight_1'])\n",
    "            colsample_bytree_weight_2 = float(environ['colsample_bytree_weight_2'])\n",
    "        except:\n",
    "            sec_colsample_bytree_ub = colsample_bytree_ub\n",
    "            sec_colsample_bytree_lb = colsample_bytree_lb\n",
    "            colsample_bytree_weight_1 = 1\n",
    "            colsample_bytree_weight_2 = 1\n",
    "            \n",
    "        try:\n",
    "            sec_gamma_ub = int(environ['sec_gamma_ub'])\n",
    "            sec_gamma_lb = int(environ['sec_gamma_lb'])\n",
    "            gamma_weight_1 = float(environ['gamma_weight_1'])\n",
    "            gamma_weight_2 = float(environ['gamma_weight_2'])\n",
    "        except:\n",
    "            sec_gamma_ub = gamma_ub\n",
    "            sec_gamma_lb = gamma_lb\n",
    "            gamma_weight_1 = 1\n",
    "            gamma_weight_2 = 1\n",
    "        try:\n",
    "            sec_learning_rate_ub = int(environ['sec_learning_rate_ub'])\n",
    "            sec_learning_rate_lb = int(environ['sec_learning_rate_lb'])\n",
    "            lr_weight_1 = float(environ['lr_weight_1'])\n",
    "            lr_weight_2 = float(environ['lr_weight_2'])\n",
    "        except:\n",
    "            sec_learning_rate_ub = learning_rate_ub\n",
    "            sec_learning_rate_lb = learning_rate_lb\n",
    "            lr_weight_1 = 1\n",
    "            lr_weight_2 = 1\n",
    "\n",
    "    except:    \n",
    "        \n",
    "        max_depth_ub = 1500\n",
    "        max_depth_lb = 100\n",
    "        min_child_weight_ub = 2\n",
    "        min_child_weight_lb = 0\n",
    "        colsample_bytree_ub = 1\n",
    "        colsample_bytree_lb = 0.25\n",
    "        gamma_ub = 0\n",
    "        gamma_lb = -3\n",
    "        learning_rate_ub = -0.5\n",
    "        learning_rate_lb = -3\n",
    "        \n",
    "        sec_max_depth_ub = 4000\n",
    "        sec_max_depth_lb = 2000\n",
    "        sec_min_child_weight_ub = 5\n",
    "        sec_min_child_weight_lb = 3\n",
    "        sec_colsample_bytree_ub = 0.25\n",
    "        sec_colsample_bytree_lb = 0\n",
    "        sec_gamma_ub = -4\n",
    "        sec_gamma_lb = -6\n",
    "        sec_learning_rate_ub = -1\n",
    "        sec_learning_rate_lb = -2\n",
    "        \n",
    "        max_depth_weight_1 = 1\n",
    "        max_depth_weight_2 = 1\n",
    "        min_child_weight_weight_1 = 1\n",
    "        min_child_weight_weight_2 = 1\n",
    "        colsample_bytree_weight_1 = 1\n",
    "        colsample_bytree_weight_2 = 1\n",
    "        gamma_weight_1 = 1\n",
    "        gamma_weight_2 = 1\n",
    "        lr_weight_1 = 1\n",
    "        lr_weight_2 = 1\n",
    "        \n",
    "    print max_depth_weight_1   \n",
    "        \n",
    "        \n",
    "\n",
    "elif classifier_method == \"RF\":  \n",
    "    try:\n",
    "        n_estimators_ub = int(environ[\"n_estimators_ub\"])\n",
    "        n_estimators_lb = int(environ[\"n_estimators_lb\"])\n",
    "        max_depth_ub = int(environ[\"max_depth_ub\"])\n",
    "        max_depth_lb = int(environ[\"max_depth_lb\"])\n",
    "        min_samples_leaf_ub = int(environ[\"min_samples_leaf_ub\"])\n",
    "        min_samples_leaf_lb = int(environ[\"min_samples_leaf_lb\"])\n",
    "        min_samples_split_ub = int(environ[\"min_samples_split_ub\"])\n",
    "        min_samples_split_lb = int(environ[\"min_samples_split_lb\"])\n",
    "\n",
    "        try:\n",
    "            sec_n_estimators_ub = int(environ['sec_n_estimators_ub'])\n",
    "            sec_n_estimators_lb = int(environ['sec_n_estimators_lb'])\n",
    "            n_estimators_weight_1 = float(environ['n_estimators_weight_1'])\n",
    "            n_estimators_weight_2 = float(environ['n_estimators_weight_2'])\n",
    "        except:\n",
    "            sec_n_estimators_ub = n_estimators_ub\n",
    "            sec_n_estimators_lb = n_estimators_lb\n",
    "            n_estimators_weight_1 = 1\n",
    "            n_estimators_weight_2 = 1\n",
    "            \n",
    "        try:\n",
    "            sec_max_depth_ub = int(environ['sec_max_depth_ub'])\n",
    "            sec_max_depth_lb = int(environ['sec_max_depth_lb'])\n",
    "            max_depth_weight_1 = float(environ['max_depth_weight_1'])\n",
    "            max_depth_weight_2 = float(environ['max_depth_weight_2'])\n",
    "        except:\n",
    "            sec_max_depth_ub = max_depth_ub\n",
    "            sec_max_depth_lb = max_depth_lb\n",
    "            max_depth_weight_1 = 1\n",
    "            max_depth_weight_2 = 1\n",
    "            \n",
    "        try:\n",
    "            sec_min_samples_leaf_ub = int(environ['sec_min_samples_leaf_ub'])\n",
    "            sec_min_samples_leaf_lb = int(environ['sec_min_samples_leaf_lb'])\n",
    "            min_samples_leaf_weight_1 = float(environ['min_samples_leaf_weight_1'])\n",
    "            min_samples_leaf_weight_2 = float(environ['min_samples_leaf_weight_2'])\n",
    "        except:\n",
    "            sec_min_samples_leaf_ub = min_samples_leaf_ub\n",
    "            sec_min_samples_leaf_lb = min_samples_leaf_lb\n",
    "            min_samples_leaf_weight_1 = 1\n",
    "            min_samples_leaf_weight_2 = 1\n",
    "        try:\n",
    "            sec_min_samples_split_ub = int(environ['sec_min_samples_split_ub'])\n",
    "            sec_min_samples_split_lb = int(environ['sec_min_samples_split_lb'])\n",
    "            min_samples_split_weight_1 = float(environ['min_samples_split_weight_1'])\n",
    "            min_samples_split_weight_2 = float(environ['min_samples_split_weight_2'])\n",
    "            \n",
    "        except:\n",
    "            sec_min_samples_split_ub = min_samples_split_ub\n",
    "            sec_min_samples_split_lb = min_samples_split_lb\n",
    "            min_samples_split_weight_1 = 1\n",
    "            min_samples_split_weight_2 = 1\n",
    "\n",
    "    except:\n",
    "        n_estimators_ub = 1500\n",
    "        n_estimators_lb = 100\n",
    "        max_depth_ub = 20\n",
    "        max_depth_lb = 2\n",
    "        min_samples_leaf_ub = 50\n",
    "        min_samples_leaf_lb = 1\n",
    "        min_samples_split_ub = 50\n",
    "        min_samples_split_lb = 2\n",
    "\n",
    "        sec_n_estimators_ub = 3000\n",
    "        sec_n_estimators_lb = 2000\n",
    "        sec_max_depth_ub = 100\n",
    "        sec_max_depth_lb = 50\n",
    "        sec_min_samples_leaf_ub = 100\n",
    "        sec_min_samples_leaf_lb = 60\n",
    "        sec_min_samples_split_ub = 100\n",
    "        sec_min_samples_split_lb = 60\n",
    "        \n",
    "        n_estimators_weight_1 = 1\n",
    "        n_estimators_weight_2 = 1\n",
    "        max_depth_weight_1 = 1\n",
    "        max_depth_weight_2 = 1\n",
    "        min_samples_leaf_weight_1 = 1\n",
    "        min_samples_leaf_weight_2 = 1\n",
    "        min_samples_split_weight_1 = 1\n",
    "        min_samples_split_weight_2 = 1\n",
    "\n",
    "\n",
    "elif classifier_method == \"Logistic\":\n",
    "    try:        \n",
    "        C_ub = int(environ[\"C_ub\"])\n",
    "        C_lb = int(environ[\"C_lb\"])\n",
    "\n",
    "        try:\n",
    "            sec_C_ub = int(environ[\"sec_C_ub\"])\n",
    "            sec_C_lb = int(environ[\"sec_C_lb\"])\n",
    "            C_weight_1 = float(environ[\"C_weight_1\"])\n",
    "            C_weight_2 = float(environ[\"C_weight_2\"])\n",
    "            \n",
    "        except:\n",
    "            sec_C_ub = C_ub\n",
    "            sec_C_lb = C_lb\n",
    "            C_weight_1 = 1\n",
    "            C_weight_2 = 1\n",
    "\n",
    "    except:       \n",
    "        C_ub = 3\n",
    "        C_lb = 1\n",
    "        \n",
    "        sec_C_ub = 7\n",
    "        sec_C_lb = 6\n",
    "        \n",
    "        C_weight_1 = 1\n",
    "        C_weight_2 = 1\n",
    "        \n",
    "\n",
    "elif classifier_method == \"KNN\":\n",
    "    try:\n",
    "        n_neighbors_ub = int(environ[\"n_neighbors_ub\"])\n",
    "        n_neighbors_lb = int(environ[\"n_neighbors_lb\"])\n",
    "        \n",
    "        try:\n",
    "            sec_n_neighbors_ub = int(environ[\"sec_n_neighbors_ub\"]) \n",
    "            sec_n_neighbors_lb = int(environ[\"sec_n_neighbors_lb\"]) \n",
    "            n_neighbors_weight_1 = float(environ[\"n_neighbors_weight_1\"])\n",
    "            n_neighbors_weight_2 = float(environ[\"n_neighbors_weight_2\"])\n",
    "        except:\n",
    "            sec_n_neighbors_ub = n_neighbors_ub\n",
    "            sec_n_neighbors_lb = n_neighbors_lb\n",
    "            n_neighbors_weight_1 = 1\n",
    "            n_neighbors_weight_2 = 1\n",
    "            \n",
    "\n",
    "    except:\n",
    "        n_neighbors_ub = 100\n",
    "        n_neighbors_lb = 5\n",
    "        \n",
    "        sec_n_neighbors_ub = 200\n",
    "        sec_n_neighbors_lb = 150\n",
    "        \n",
    "        n_neighbors_weight_1 = 1\n",
    "        n_neighbors_weight_2 = 1\n",
    "        \n",
    "        \n",
    "        \n",
    "elif classifier_method == \"ADA\":\n",
    "    try:\n",
    "        n_estimators_ub = int(environ[\"n_estimators_ub\"])\n",
    "        n_estimators_lb = int(environ[\"n_estimators_lb\"])\n",
    "        learning_rate_ub = int(environ[\"learning_rate_ub\"])\n",
    "        learning_rate_lb = int(environ[\"learning_rate_lb\"])\n",
    "        \n",
    "        try:         \n",
    "            sec_n_estimators_ub = int(environ[\"sec_n_estimators_ub\"]) \n",
    "            sec_n_estimators_lb = int(environ[\"sec_n_estimators_lb\"])\n",
    "            n_estimators_weight_1 = float(environ['n_estimators_weight_1'])\n",
    "            n_estimators_weight_2 = float(environ['n_estimators_weight_2'])\n",
    "            \n",
    "        except:\n",
    "            sec_n_estimators_ub = n_estimators_ub\n",
    "            sec_n_estimators_lb = n_estimators_lb\n",
    "            n_estimators_weight_1 = 1\n",
    "            n_estimators_weight_2 = 1\n",
    "        try:\n",
    "            sec_learning_rate_ub = int(environ[\"sec_learning_rate_ub\"])\n",
    "            sec_learning_rate_lb = int(environ[\"sec_learning_rate_lb\"]) \n",
    "            lr_weight_1 = float(environ['lr_weight_1'])\n",
    "            lr_weight_2 = float(environ['lr_weight_2'])\n",
    "        except:\n",
    "            sec_learning_rate_ub = learning_rate_ub\n",
    "            sec_learning_rate_lb = learning_rate_lb         \n",
    "            lr_weight_1 = 1\n",
    "            lr_weight_2 = 1\n",
    "            \n",
    "    except:\n",
    "        \n",
    "        n_estimators_ub = 6\n",
    "        n_estimators_lb = 3\n",
    "        learning_rate_ub = 0\n",
    "        learning_rate_lb = -4\n",
    "        \n",
    "        sec_n_estimators_ub = 12\n",
    "        sec_n_estimators_lb = 9\n",
    "        sec_learning_rate_ub = -14\n",
    "        sec_learning_rate_lb = -15\n",
    "        \n",
    "        n_estimators_weight_1 = 1\n",
    "        n_estimators_weight_2 = 1\n",
    "        lr_weight_1 = 1\n",
    "        lr_weight_2 = 1\n",
    "        \n",
    "        \n",
    "        \n",
    "elif classifier_method == \"SVM\":\n",
    "    try:\n",
    "        C_ub = int(environ[\"C_ub\"])\n",
    "        C_lb = int(environ[\"C_lb\"])\n",
    "        gamma_ub = int(environ[\"gamma_ub\"])\n",
    "        gamma_lb = int(environ[\"gamma_lb\"])\n",
    "        \n",
    "        try:\n",
    "            sec_C_ub = int(environ[\"sec_C_ub\"]) \n",
    "            sec_C_lb = int(environ[\"sec_C_lb\"]) \n",
    "            C_weight_1 = float(environ[\"C_weight_1\"])\n",
    "            C_weight_2 = float(environ[\"C_weight_2\"])\n",
    "        \n",
    "        except:\n",
    "            sec_C_ub = C_ub\n",
    "            sec_C_lb = C_lb\n",
    "            C_weight_1 = 1\n",
    "            C_weight_2 = 1\n",
    "        \n",
    "        try:\n",
    "            sec_gamma_ub = int(environ[\"sec_gamma_ub\"]) \n",
    "            sec_gamma_lb = int(environ[\"sec_gamma_lb\"])\n",
    "            gamma_weight_1 = float(environ['gamma_weight_1'])\n",
    "            gamma_weight_2 = float(environ['gamma_weight_2'])\n",
    "        except:\n",
    "            sec_gamma_ub = gamma_ub\n",
    "            sec_gamma_lb = gamma_lb\n",
    "            gamma_weight_1 = 1\n",
    "            gamma_weight_2 = 1            \n",
    "            \n",
    "    except:\n",
    "        \n",
    "        C_ub = 4\n",
    "        C_lb = 2\n",
    "        gamma_ub = -4\n",
    "        gamma_lb = -6\n",
    "        \n",
    "        sec_C_ub = 10\n",
    "        sec_C_lb = 8\n",
    "        sec_gamma_ub = -13\n",
    "        sec_gamma_lb = -15\n",
    "        \n",
    "        C_weight_1 = 1\n",
    "        C_weight_2 = 1\n",
    "        gamma_weight_1 = 1\n",
    "        gamma_weight_2 = 1  \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate hyperparameter trials\n",
    "\n",
    "Choose hyperparameters and generate hyperparameters through random search in a grid, as explained by this video: https://www.youtube.com/watch?v=WrICwRrvuIc&index=66&list=PLBAGcD3siRDguyYYzhVwZ3tLvOyyG5k6K\n",
    "\n",
    "Use logarithmic scale for search for learnining rate and weight decay for NN, as explained by this video: https://www.youtube.com/watch?v=VUbrW8OK3uo&index=67&list=PLBAGcD3siRDguyYYzhVwZ3tLvOyyG5k6K\n",
    "\n",
    "Utilize nested cross validation to choose between models, as described here: https://stats.stackexchange.com/questions/266225/step-by-step-explanation-of-k-fold-cross-validation-with-grid-search-to-optimise/266229"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'weight': 'None', 'hidden_units_2': 359, 'learning_rate': 3.53845359090655e-05, 'batch_size': 317, 'beta': 0.9357945617622756, 'weight_decay': 2.7711337379161135e-09, 'hidden_units_1': 153}, {'weight': '0.1', 'hidden_units_2': 664, 'learning_rate': 4.2030416802172165e-05, 'batch_size': 236, 'beta': 0.8556712977317443, 'weight_decay': 9.448134464026927e-15, 'hidden_units_1': 190}, {'weight': 'balanced', 'hidden_units_2': 375, 'learning_rate': 3.003759223085158e-05, 'batch_size': 239, 'beta': 0.9068044561093932, 'weight_decay': 1.9451325325217145e-12, 'hidden_units_1': 215}, {'weight': '0.1', 'hidden_units_2': 685, 'learning_rate': 1.1777037507521709e-05, 'batch_size': 209, 'beta': 0.8868241539840548, 'weight_decay': 3.0319268602096686e-11, 'hidden_units_1': 247}, {'weight': '0.1', 'hidden_units_2': 497, 'learning_rate': 7.413309777968015e-05, 'batch_size': 282, 'beta': 0.9300910751979644, 'weight_decay': 5.446035309065944e-13, 'hidden_units_1': 79}, {'weight': 'None', 'hidden_units_2': 382, 'learning_rate': 4.773968291550972e-05, 'batch_size': 367, 'beta': 0.8643353287409046, 'weight_decay': 2.5073225886269265e-11, 'hidden_units_1': 177}, {'weight': 'balanced', 'hidden_units_2': 378, 'learning_rate': 3.3254339154005814e-05, 'batch_size': 351, 'beta': 0.9274233689434216, 'weight_decay': 4.421362475707932e-15, 'hidden_units_1': 233}, {'weight': '0.1', 'hidden_units_2': 594, 'learning_rate': 3.701978983307306e-05, 'batch_size': 328, 'beta': 0.8649674867183683, 'weight_decay': 1.7434523326842915e-14, 'hidden_units_1': 282}, {'weight': '0.1', 'hidden_units_2': 607, 'learning_rate': 2.4349440200234778e-05, 'batch_size': 332, 'beta': 0.8859507900573785, 'weight_decay': 6.579653011946035e-11, 'hidden_units_1': 170}, {'weight': 'None', 'hidden_units_2': 780, 'learning_rate': 4.984610132094826e-05, 'batch_size': 369, 'beta': 0.9153140035797938, 'weight_decay': 4.989886617167463e-08, 'hidden_units_1': 61}, {'weight': '0.1', 'hidden_units_2': 776, 'learning_rate': 2.281141192305522e-05, 'batch_size': 328, 'beta': 0.8863710770942622, 'weight_decay': 1.426538539991019e-14, 'hidden_units_1': 171}, {'weight': 'balanced', 'hidden_units_2': 481, 'learning_rate': 2.745373981494028e-05, 'batch_size': 268, 'beta': 0.915279031700549, 'weight_decay': 3.885909538167125e-08, 'hidden_units_1': 294}, {'weight': 'None', 'hidden_units_2': 792, 'learning_rate': 9.892352141083794e-05, 'batch_size': 380, 'beta': 0.8966310772856306, 'weight_decay': 3.4112694539306683e-15, 'hidden_units_1': 277}, {'weight': '0.1', 'hidden_units_2': 519, 'learning_rate': 1.4420143547184013e-05, 'batch_size': 341, 'beta': 0.8817201742069296, 'weight_decay': 5.5914342528287057e-11, 'hidden_units_1': 119}, {'weight': 'balanced', 'hidden_units_2': 642, 'learning_rate': 8.9037109669707e-05, 'batch_size': 295, 'beta': 0.8597101275793061, 'weight_decay': 1.6215572955007176e-09, 'hidden_units_1': 228}, {'weight': 'None', 'hidden_units_2': 755, 'learning_rate': 1.2476661940792187e-05, 'batch_size': 293, 'beta': 0.90096243767199, 'weight_decay': 6.052498820395658e-09, 'hidden_units_1': 162}, {'weight': 'None', 'hidden_units_2': 373, 'learning_rate': 5.486098228809556e-05, 'batch_size': 200, 'beta': 0.9479586728812728, 'weight_decay': 2.6128183199526794e-13, 'hidden_units_1': 177}, {'weight': 'None', 'hidden_units_2': 432, 'learning_rate': 3.0261714581821538e-05, 'batch_size': 321, 'beta': 0.891426299451467, 'weight_decay': 1.5129751464040645e-14, 'hidden_units_1': 173}, {'weight': 'None', 'hidden_units_2': 408, 'learning_rate': 3.674420594853437e-05, 'batch_size': 348, 'beta': 0.9023248053466699, 'weight_decay': 4.507076290850352e-15, 'hidden_units_1': 91}, {'weight': 'balanced', 'hidden_units_2': 657, 'learning_rate': 3.76657392486504e-05, 'batch_size': 210, 'beta': 0.8509356704856532, 'weight_decay': 5.986674044283314e-15, 'hidden_units_1': 52}, {'weight': 'None', 'hidden_units_2': 721, 'learning_rate': 4.437865545090026e-05, 'batch_size': 382, 'beta': 0.8683191362007117, 'weight_decay': 7.835395619686483e-15, 'hidden_units_1': 92}, {'weight': 'None', 'hidden_units_2': 444, 'learning_rate': 1.0473878850506294e-05, 'batch_size': 277, 'beta': 0.9177816536796229, 'weight_decay': 1.1141784696858775e-17, 'hidden_units_1': 53}, {'weight': 'None', 'hidden_units_2': 678, 'learning_rate': 5.4349308412859864e-05, 'batch_size': 213, 'beta': 0.925610669386504, 'weight_decay': 2.1521900529011082e-13, 'hidden_units_1': 145}, {'weight': '0.1', 'hidden_units_2': 610, 'learning_rate': 7.871153601454579e-05, 'batch_size': 380, 'beta': 0.9180055569463412, 'weight_decay': 8.232739472621617e-09, 'hidden_units_1': 146}, {'weight': 'balanced', 'hidden_units_2': 681, 'learning_rate': 9.518552912110089e-05, 'batch_size': 339, 'beta': 0.9313797819702476, 'weight_decay': 9.426913206427512e-15, 'hidden_units_1': 159}, {'weight': '0.1', 'hidden_units_2': 794, 'learning_rate': 7.605069672357748e-05, 'batch_size': 352, 'beta': 0.9465416220519927, 'weight_decay': 1.3557615702786872e-14, 'hidden_units_1': 258}, {'weight': '0.1', 'hidden_units_2': 578, 'learning_rate': 4.5673933166870946e-05, 'batch_size': 317, 'beta': 0.9143990199229637, 'weight_decay': 3.637779298876378e-08, 'hidden_units_1': 210}, {'weight': 'None', 'hidden_units_2': 534, 'learning_rate': 4.040110226321074e-05, 'batch_size': 321, 'beta': 0.9216074531228643, 'weight_decay': 5.027150563333661e-13, 'hidden_units_1': 121}, {'weight': 'None', 'hidden_units_2': 516, 'learning_rate': 2.4180330080995552e-05, 'batch_size': 241, 'beta': 0.863547406422245, 'weight_decay': 1.939487609596899e-13, 'hidden_units_1': 61}, {'weight': '0.1', 'hidden_units_2': 374, 'learning_rate': 3.715052117137243e-05, 'batch_size': 329, 'beta': 0.9153200819857134, 'weight_decay': 5.5367190735909694e-12, 'hidden_units_1': 175}, {'weight': 'None', 'hidden_units_2': 539, 'learning_rate': 2.7003399116040404e-05, 'batch_size': 203, 'beta': 0.8935864925265626, 'weight_decay': 4.7382579557492443e-14, 'hidden_units_1': 264}, {'weight': 'balanced', 'hidden_units_2': 532, 'learning_rate': 3.4436778831299664e-05, 'batch_size': 243, 'beta': 0.9419482613744673, 'weight_decay': 1.0052379437428291e-16, 'hidden_units_1': 188}, {'weight': 'None', 'hidden_units_2': 567, 'learning_rate': 9.97348655549582e-05, 'batch_size': 325, 'beta': 0.853755938382814, 'weight_decay': 1.1129545827750138e-08, 'hidden_units_1': 77}, {'weight': '0.1', 'hidden_units_2': 385, 'learning_rate': 1.329903052505582e-05, 'batch_size': 251, 'beta': 0.8911396723755454, 'weight_decay': 4.459980460340283e-09, 'hidden_units_1': 117}, {'weight': 'balanced', 'hidden_units_2': 784, 'learning_rate': 1.0794993502436188e-05, 'batch_size': 273, 'beta': 0.8953542682678068, 'weight_decay': 9.425138297617704e-11, 'hidden_units_1': 268}, {'weight': 'None', 'hidden_units_2': 773, 'learning_rate': 7.3516077332079e-05, 'batch_size': 231, 'beta': 0.8730742335406796, 'weight_decay': 1.4664921322891175e-17, 'hidden_units_1': 249}, {'weight': '0.1', 'hidden_units_2': 483, 'learning_rate': 8.792725587064772e-05, 'batch_size': 297, 'beta': 0.9021036606204129, 'weight_decay': 5.203514542168401e-16, 'hidden_units_1': 83}, {'weight': 'None', 'hidden_units_2': 787, 'learning_rate': 1.5848805106375277e-05, 'batch_size': 236, 'beta': 0.8963450977384483, 'weight_decay': 5.127031513200954e-16, 'hidden_units_1': 176}, {'weight': 'None', 'hidden_units_2': 641, 'learning_rate': 8.792649289986852e-05, 'batch_size': 390, 'beta': 0.8531838929531308, 'weight_decay': 1.1069908195870022e-10, 'hidden_units_1': 162}, {'weight': 'balanced', 'hidden_units_2': 709, 'learning_rate': 4.182908855309802e-05, 'batch_size': 368, 'beta': 0.9492396398888632, 'weight_decay': 9.794465643610749e-11, 'hidden_units_1': 117}, {'weight': '0.1', 'hidden_units_2': 623, 'learning_rate': 4.776289206280621e-05, 'batch_size': 314, 'beta': 0.923012202951677, 'weight_decay': 7.926834000844164e-12, 'hidden_units_1': 196}, {'weight': 'None', 'hidden_units_2': 603, 'learning_rate': 2.501618397868549e-05, 'batch_size': 202, 'beta': 0.9444372389983933, 'weight_decay': 7.276626082702215e-16, 'hidden_units_1': 167}, {'weight': 'balanced', 'hidden_units_2': 764, 'learning_rate': 3.093561887514263e-05, 'batch_size': 291, 'beta': 0.85852955658587, 'weight_decay': 6.389206310420224e-11, 'hidden_units_1': 200}, {'weight': 'balanced', 'hidden_units_2': 647, 'learning_rate': 3.0749474935899195e-05, 'batch_size': 399, 'beta': 0.887775183929248, 'weight_decay': 9.19252645000315e-11, 'hidden_units_1': 227}, {'weight': 'None', 'hidden_units_2': 679, 'learning_rate': 1.0584704264684174e-05, 'batch_size': 349, 'beta': 0.897859632561875, 'weight_decay': 2.743755783950583e-10, 'hidden_units_1': 269}, {'weight': 'balanced', 'hidden_units_2': 456, 'learning_rate': 4.04617871478307e-05, 'batch_size': 305, 'beta': 0.8956014623217867, 'weight_decay': 4.954180560940305e-13, 'hidden_units_1': 66}, {'weight': '0.1', 'hidden_units_2': 790, 'learning_rate': 1.3715777146485733e-05, 'batch_size': 224, 'beta': 0.8820017150822468, 'weight_decay': 3.835186945091952e-10, 'hidden_units_1': 186}, {'weight': 'balanced', 'hidden_units_2': 437, 'learning_rate': 3.8754051633440804e-05, 'batch_size': 323, 'beta': 0.8500055356865008, 'weight_decay': 1.5743886006150747e-12, 'hidden_units_1': 122}, {'weight': 'None', 'hidden_units_2': 570, 'learning_rate': 6.281260916940163e-05, 'batch_size': 281, 'beta': 0.8956129772122862, 'weight_decay': 6.292084756270517e-11, 'hidden_units_1': 253}, {'weight': 'None', 'hidden_units_2': 674, 'learning_rate': 6.148078832716363e-05, 'batch_size': 232, 'beta': 0.8753941642595026, 'weight_decay': 2.0349702650376682e-10, 'hidden_units_1': 70}, {'weight': '0.1', 'hidden_units_2': 355, 'learning_rate': 2.9631359044979743e-05, 'batch_size': 322, 'beta': 0.8924685468751505, 'weight_decay': 1.187683585795772e-15, 'hidden_units_1': 191}, {'weight': 'balanced', 'hidden_units_2': 632, 'learning_rate': 2.9078729288456297e-05, 'batch_size': 381, 'beta': 0.9247079470139233, 'weight_decay': 1.1907009183590643e-09, 'hidden_units_1': 223}, {'weight': 'balanced', 'hidden_units_2': 723, 'learning_rate': 1.8918807175969345e-05, 'batch_size': 377, 'beta': 0.8963150199990133, 'weight_decay': 1.1085175773094885e-10, 'hidden_units_1': 68}, {'weight': '0.1', 'hidden_units_2': 779, 'learning_rate': 1.602749913734112e-05, 'batch_size': 358, 'beta': 0.8855612737849955, 'weight_decay': 7.595612604139267e-13, 'hidden_units_1': 68}, {'weight': 'None', 'hidden_units_2': 686, 'learning_rate': 5.60613587983119e-05, 'batch_size': 270, 'beta': 0.924925441706324, 'weight_decay': 1.4945309976722127e-15, 'hidden_units_1': 109}, {'weight': 'balanced', 'hidden_units_2': 619, 'learning_rate': 1.1107456910670885e-05, 'batch_size': 363, 'beta': 0.8740828779915446, 'weight_decay': 8.345940187091068e-15, 'hidden_units_1': 94}, {'weight': '0.1', 'hidden_units_2': 558, 'learning_rate': 1.038555307669527e-05, 'batch_size': 238, 'beta': 0.9333038097509391, 'weight_decay': 1.2370884007525746e-17, 'hidden_units_1': 185}, {'weight': 'None', 'hidden_units_2': 410, 'learning_rate': 5.052365770257252e-05, 'batch_size': 279, 'beta': 0.908044713718813, 'weight_decay': 1.3302541821301015e-12, 'hidden_units_1': 259}, {'weight': '0.1', 'hidden_units_2': 655, 'learning_rate': 2.8521267060742122e-05, 'batch_size': 314, 'beta': 0.9441377704706498, 'weight_decay': 3.113065253908788e-08, 'hidden_units_1': 150}, {'weight': 'None', 'hidden_units_2': 757, 'learning_rate': 5.1114974682217015e-05, 'batch_size': 254, 'beta': 0.9348943555312917, 'weight_decay': 8.515356704419017e-15, 'hidden_units_1': 235}, {'weight': '0.1', 'hidden_units_2': 357, 'learning_rate': 1.0309482309945547e-05, 'batch_size': 361, 'beta': 0.9280426651104013, 'weight_decay': 4.19853495729283e-09, 'hidden_units_1': 136}, {'weight': 'None', 'hidden_units_2': 559, 'learning_rate': 3.1433400462554816e-05, 'batch_size': 337, 'beta': 0.8910492138094457, 'weight_decay': 4.9521562560606135e-15, 'hidden_units_1': 214}, {'weight': '0.1', 'hidden_units_2': 674, 'learning_rate': 1.9853716612014992e-05, 'batch_size': 203, 'beta': 0.9480699674024019, 'weight_decay': 1.609051378929534e-16, 'hidden_units_1': 86}, {'weight': '0.1', 'hidden_units_2': 393, 'learning_rate': 3.7086065421699305e-05, 'batch_size': 375, 'beta': 0.9406555499221179, 'weight_decay': 4.058322731487043e-08, 'hidden_units_1': 141}, {'weight': '0.1', 'hidden_units_2': 769, 'learning_rate': 2.153501368231738e-05, 'batch_size': 348, 'beta': 0.927521977746661, 'weight_decay': 1.0430587227194709e-14, 'hidden_units_1': 161}, {'weight': '0.1', 'hidden_units_2': 481, 'learning_rate': 2.114999675893636e-05, 'batch_size': 218, 'beta': 0.851142745862503, 'weight_decay': 1.8027837805067722e-10, 'hidden_units_1': 221}, {'weight': '0.1', 'hidden_units_2': 536, 'learning_rate': 1.4026413745663416e-05, 'batch_size': 258, 'beta': 0.9177411829471944, 'weight_decay': 3.2079471289007915e-15, 'hidden_units_1': 74}, {'weight': '0.1', 'hidden_units_2': 794, 'learning_rate': 2.7048802940231027e-05, 'batch_size': 269, 'beta': 0.9360551173828793, 'weight_decay': 3.747036410689507e-12, 'hidden_units_1': 218}, {'weight': 'balanced', 'hidden_units_2': 551, 'learning_rate': 1.8634935986756723e-05, 'batch_size': 268, 'beta': 0.8976477200549831, 'weight_decay': 8.305228428864996e-12, 'hidden_units_1': 229}, {'weight': 'balanced', 'hidden_units_2': 690, 'learning_rate': 2.874183899474957e-05, 'batch_size': 361, 'beta': 0.9195625445638856, 'weight_decay': 6.804768507798784e-11, 'hidden_units_1': 296}, {'weight': 'balanced', 'hidden_units_2': 573, 'learning_rate': 2.398429492625049e-05, 'batch_size': 308, 'beta': 0.9160039862222948, 'weight_decay': 7.307516484200448e-17, 'hidden_units_1': 133}, {'weight': '0.1', 'hidden_units_2': 499, 'learning_rate': 7.035181763538615e-05, 'batch_size': 291, 'beta': 0.8873813137932561, 'weight_decay': 3.928489922849432e-15, 'hidden_units_1': 57}, {'weight': '0.1', 'hidden_units_2': 388, 'learning_rate': 1.8742257589143044e-05, 'batch_size': 336, 'beta': 0.9279191974251388, 'weight_decay': 2.1609867091019177e-16, 'hidden_units_1': 229}, {'weight': '0.1', 'hidden_units_2': 382, 'learning_rate': 5.190136130358836e-05, 'batch_size': 323, 'beta': 0.8806810099545196, 'weight_decay': 1.5429487051398813e-12, 'hidden_units_1': 256}, {'weight': 'balanced', 'hidden_units_2': 583, 'learning_rate': 9.108216409771483e-05, 'batch_size': 297, 'beta': 0.9283647965121287, 'weight_decay': 5.661016478808241e-13, 'hidden_units_1': 223}, {'weight': 'None', 'hidden_units_2': 779, 'learning_rate': 1.4449974545611075e-05, 'batch_size': 293, 'beta': 0.8777596097731766, 'weight_decay': 6.515160306158151e-11, 'hidden_units_1': 198}, {'weight': 'None', 'hidden_units_2': 555, 'learning_rate': 2.4698789941044004e-05, 'batch_size': 319, 'beta': 0.8629412337422251, 'weight_decay': 3.969178876531774e-16, 'hidden_units_1': 291}, {'weight': 'None', 'hidden_units_2': 376, 'learning_rate': 2.863593854034777e-05, 'batch_size': 287, 'beta': 0.9300378462107238, 'weight_decay': 6.75643303282405e-17, 'hidden_units_1': 233}, {'weight': 'None', 'hidden_units_2': 411, 'learning_rate': 1.2401078494622024e-05, 'batch_size': 298, 'beta': 0.8827720401557119, 'weight_decay': 9.913352348541911e-11, 'hidden_units_1': 258}, {'weight': 'None', 'hidden_units_2': 444, 'learning_rate': 4.325746404230576e-05, 'batch_size': 304, 'beta': 0.9296391474517331, 'weight_decay': 4.0307719239454527e-16, 'hidden_units_1': 121}, {'weight': 'None', 'hidden_units_2': 432, 'learning_rate': 2.8716984069687382e-05, 'batch_size': 358, 'beta': 0.8957223453353856, 'weight_decay': 3.777691090871986e-09, 'hidden_units_1': 221}, {'weight': '0.1', 'hidden_units_2': 579, 'learning_rate': 3.764880206182018e-05, 'batch_size': 265, 'beta': 0.9492429523285655, 'weight_decay': 3.7698386037438866e-16, 'hidden_units_1': 128}, {'weight': '0.1', 'hidden_units_2': 443, 'learning_rate': 1.2429621792049128e-05, 'batch_size': 300, 'beta': 0.8562712952023346, 'weight_decay': 9.645896245569012e-14, 'hidden_units_1': 224}, {'weight': 'None', 'hidden_units_2': 475, 'learning_rate': 1.814195422808052e-05, 'batch_size': 212, 'beta': 0.8806986055962612, 'weight_decay': 2.6447204862153706e-10, 'hidden_units_1': 250}, {'weight': 'balanced', 'hidden_units_2': 694, 'learning_rate': 2.2735624632840367e-05, 'batch_size': 355, 'beta': 0.8901259500803609, 'weight_decay': 7.117430783578032e-16, 'hidden_units_1': 247}, {'weight': 'None', 'hidden_units_2': 694, 'learning_rate': 1.2578096728209088e-05, 'batch_size': 203, 'beta': 0.8592027585611957, 'weight_decay': 9.655617065520362e-17, 'hidden_units_1': 261}, {'weight': 'None', 'hidden_units_2': 452, 'learning_rate': 1.2204851235893353e-05, 'batch_size': 334, 'beta': 0.8533074591475506, 'weight_decay': 1.3952340445639798e-11, 'hidden_units_1': 142}, {'weight': 'None', 'hidden_units_2': 755, 'learning_rate': 4.465108058136476e-05, 'batch_size': 397, 'beta': 0.8721160915346083, 'weight_decay': 3.306814874390018e-15, 'hidden_units_1': 127}, {'weight': '0.1', 'hidden_units_2': 604, 'learning_rate': 1.352244523379308e-05, 'batch_size': 281, 'beta': 0.9481639680404068, 'weight_decay': 8.132937016111926e-13, 'hidden_units_1': 186}, {'weight': 'None', 'hidden_units_2': 469, 'learning_rate': 2.367681041267286e-05, 'batch_size': 373, 'beta': 0.8662954426046605, 'weight_decay': 1.231802138523402e-13, 'hidden_units_1': 288}, {'weight': '0.1', 'hidden_units_2': 788, 'learning_rate': 3.0924689412318416e-05, 'batch_size': 329, 'beta': 0.8812753295773524, 'weight_decay': 4.797599823091014e-14, 'hidden_units_1': 124}, {'weight': '0.1', 'hidden_units_2': 520, 'learning_rate': 1.7434866250106368e-05, 'batch_size': 272, 'beta': 0.9133930956317924, 'weight_decay': 6.416415066532574e-14, 'hidden_units_1': 237}, {'weight': '0.1', 'hidden_units_2': 688, 'learning_rate': 7.143978149251089e-05, 'batch_size': 257, 'beta': 0.9322559451360811, 'weight_decay': 1.8982079515512412e-13, 'hidden_units_1': 256}, {'weight': 'None', 'hidden_units_2': 481, 'learning_rate': 2.799908509727011e-05, 'batch_size': 332, 'beta': 0.9424966911953192, 'weight_decay': 1.934501832966756e-09, 'hidden_units_1': 202}, {'weight': 'None', 'hidden_units_2': 752, 'learning_rate': 1.708547246392578e-05, 'batch_size': 364, 'beta': 0.8757233485471393, 'weight_decay': 2.35962853349661e-08, 'hidden_units_1': 211}, {'weight': 'None', 'hidden_units_2': 477, 'learning_rate': 4.978994159410833e-05, 'batch_size': 335, 'beta': 0.8741545686493963, 'weight_decay': 1.6240477569517252e-16, 'hidden_units_1': 294}, {'weight': 'balanced', 'hidden_units_2': 640, 'learning_rate': 3.0868059509061e-05, 'batch_size': 357, 'beta': 0.918976826507775, 'weight_decay': 2.1587510145483283e-15, 'hidden_units_1': 82}, {'weight': '0.1', 'hidden_units_2': 460, 'learning_rate': 5.3790935721493174e-05, 'batch_size': 324, 'beta': 0.8632814998768693, 'weight_decay': 2.047648538375408e-11, 'hidden_units_1': 102}, {'weight': '0.1', 'hidden_units_2': 398, 'learning_rate': 5.607745009412586e-05, 'batch_size': 318, 'beta': 0.9003933611879436, 'weight_decay': 9.022927367103408e-10, 'hidden_units_1': 181}, {'weight': '0.1', 'hidden_units_2': 399, 'learning_rate': 7.690636220339148e-05, 'batch_size': 316, 'beta': 0.9002389457489262, 'weight_decay': 2.384614937824915e-15, 'hidden_units_1': 229}]\n"
     ]
    }
   ],
   "source": [
    "no_trials = 100\n",
    "                              \n",
    "if classifier_method == \"NN\":\n",
    "    lr_list = [[learning_rate_lb, learning_rate_ub],[sec_learning_rate_lb, sec_learning_rate_ub]]\n",
    "    \n",
    "    lr_list_weights = [lr_weight_1, lr_weight_2]\n",
    "    \n",
    "    batch_size_list = [[batch_size_lb, batch_size_ub],[sec_batch_size_lb, sec_batch_size_ub]]\n",
    "    \n",
    "    batch_size_list_weights = [batch_size_weight_1, batch_size_weight_2]\n",
    "    \n",
    "    weight_decay_list = [[weight_decay_lb, weight_decay_ub],[sec_weight_decay_lb, sec_weight_decay_ub]]\n",
    "    \n",
    "    weight_decay_list_weights = [weight_decay_weight_1, weight_decay_weight_2]\n",
    "    \n",
    "    beta_list = [[beta_lb, beta_ub], [sec_beta_lb, sec_beta_ub]]\n",
    "    \n",
    "    beta_list_weights = [beta_weight_1, beta_weight_2]\n",
    "    \n",
    "    hidden_units_1_list = [[hidden_units_1_lb, hidden_units_1_ub],[sec_hidden_units_1_lb, sec_hidden_units_1_ub]]\n",
    "    \n",
    "    hidden_units_1_list_weights = [hidden_units_1_weight_1, hidden_units_1_weight_2]\n",
    "    \n",
    "    hidden_units_2_list = [[hidden_units_2_lb, hidden_units_2_ub],[sec_hidden_units_2_lb, sec_hidden_units_2_ub]]\n",
    "    \n",
    "    hidden_units_2_list_weights = [hidden_units_2_weight_1, hidden_units_2_weight_2]\n",
    "                              \n",
    "    hyperparameter_trials = generate_trials_NN(no_trials, lr_list, lr_list_weights, batch_size_list, \n",
    "                                               batch_size_list_weights, weight_decay_list, weight_decay_list_weights, \n",
    "                                               beta_list, beta_list_weights, hidden_units_1_list, hidden_units_1_list_weights, \n",
    "                                               hidden_units_2_list, hidden_units_2_list_weights)\n",
    "elif classifier_method == \"XGB\":\n",
    "    max_depth_list = [[max_depth_lb, max_depth_ub], [sec_max_depth_lb, sec_max_depth_ub]]\n",
    "    \n",
    "    max_depth_list_weights = [max_depth_weight_1, max_depth_weight_2]\n",
    "    \n",
    "    min_child_weight_list = [[min_child_weight_lb, min_child_weight_ub],[sec_min_child_weight_lb, sec_min_child_weight_ub]]\n",
    "    \n",
    "    min_child_weight_list_weights = [min_child_weight_weight_1, min_child_weight_weight_2]\n",
    "    \n",
    "    colsample_bytree_list = [[colsample_bytree_lb, colsample_bytree_ub], [sec_colsample_bytree_lb, sec_colsample_bytree_ub]]\n",
    "    \n",
    "    colsample_bytree_list_weights = [colsample_bytree_weight_1, colsample_bytree_weight_2]\n",
    "    \n",
    "    gamma_list = [[gamma_lb, gamma_ub],[sec_gamma_lb, sec_gamma_ub]] \n",
    "    \n",
    "    gamma_list_weights = [gamma_weight_1, gamma_weight_2]\n",
    "    \n",
    "    lr_list = [[learning_rate_lb, learning_rate_ub],[sec_learning_rate_lb, sec_learning_rate_ub]]\n",
    "    \n",
    "    lr_list_weights = [lr_weight_1, lr_weight_2]\n",
    "\n",
    "    hyperparameter_trials = generate_trials_XGB(no_trials, max_depth_list, max_depth_list_weights, min_child_weight_list, \n",
    "                                                min_child_weight_list_weights, colsample_bytree_list, colsample_bytree_list_weights, \n",
    "                                                gamma_list, gamma_list_weights, lr_list, lr_list_weights)\n",
    "                                      \n",
    "elif classifier_method == \"RF\":\n",
    "    \n",
    "    n_estimators_list = [[n_estimators_lb, n_estimators_ub],[sec_n_estimators_lb, sec_n_estimators_ub]]\n",
    "    \n",
    "    n_estimators_list_weights = [n_estimators_weight_1, n_estimators_weight_2]\n",
    "\n",
    "    max_depth_list = [[max_depth_lb, max_depth_ub],[sec_max_depth_lb, sec_max_depth_ub]]\n",
    "    \n",
    "    max_depth_list_weights = [max_depth_weight_1, max_depth_weight_2]\n",
    "    \n",
    "    min_samples_leaf_list = [[min_samples_leaf_lb, min_samples_leaf_ub], [sec_min_samples_leaf_lb, sec_min_samples_leaf_ub]]\n",
    " \n",
    "    min_samples_leaf_list_weights = [min_samples_leaf_weight_1, min_samples_leaf_weight_2]\n",
    "    \n",
    "    min_samples_split_list = [[min_samples_split_lb, min_samples_split_ub],\n",
    "                              [sec_min_samples_split_lb, sec_min_samples_split_ub]]\n",
    "       \n",
    "    min_samples_split_list_weights = [min_samples_split_weight_1, min_samples_split_weight_2]\n",
    "    \n",
    "    hyperparameter_trials = generate_trials_RF(no_trials, n_estimators_list, n_estimators_list_weights, max_depth_list, \n",
    "                                               max_depth_list_weights, min_samples_leaf_list, min_samples_leaf_list_weights, \n",
    "                                               min_samples_split_list, min_samples_split_list_weights)\n",
    "    \n",
    "elif classifier_method == \"Logistic\":\n",
    "    C_list = [[C_lb, C_ub], [sec_C_lb, sec_C_ub]]\n",
    "    \n",
    "    C_list_weights = [C_weight_1, C_weight_2]\n",
    "    \n",
    "    hyperparameter_trials = generate_trials_Log(no_trials, C_list, C_list_weights)\n",
    "    \n",
    "elif classifier_method == \"KNN\":\n",
    "    \n",
    "    n_neighbors_list = [[n_neighbors_lb, n_neighbors_ub], [sec_n_neighbors_lb, sec_n_neighbors_ub]]\n",
    "    \n",
    "    n_neighbors_list_weights = [n_neighbors_weight_1, n_neighbors_weight_2]\n",
    "    \n",
    "    hyperparameter_trials = generate_trials_KNN(no_trials, n_neighbors_list, n_neighbors_list_weights)\n",
    "    \n",
    "elif classifier_method == \"ADA\":\n",
    "    \n",
    "    n_estimators_list = [[n_estimators_lb, n_estimators_ub], [sec_n_estimators_lb, sec_n_estimators_ub]]\n",
    "    \n",
    "    n_estimators_list_weights = [n_estimators_weight_1, n_estimators_weight_2]\n",
    "    \n",
    "    lr_list = [[learning_rate_lb, learning_rate_ub], [sec_learning_rate_lb, sec_learning_rate_ub]]\n",
    "    \n",
    "    lr_list_weights = [lr_weight_1, lr_weight_2]\n",
    "    \n",
    "    hyperparameter_trials = generate_trials_ADA(no_trials, n_estimators_list, n_estimators_list_weights, lr_list, \n",
    "                                                lr_list_weights)\n",
    "    \n",
    "elif classifier_method == \"SVM\":\n",
    "    C_list = [[C_lb, C_ub], [sec_C_lb, sec_C_ub]]\n",
    "    \n",
    "    C_list_weights = [C_weight_1, C_weight_2]\n",
    "                                      \n",
    "    gamma_list = [[gamma_lb, gamma_ub], [sec_gamma_lb, sec_gamma_ub]]\n",
    "    \n",
    "    gamma_list_weights = [gamma_weight_1, gamma_weight_2]\n",
    "                                          \n",
    "    hyperparameter_trials = generate_trials_SVM(no_trials, C_list, C_list_weights, gamma_list, gamma_list_weights)\n",
    "\n",
    "print hyperparameter_trials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'weight': 'None', 'hidden_units_2': 359, 'learning_rate': 3.53845359090655e-05, 'batch_size': 317, 'beta': 0.9357945617622756, 'weight_decay': 2.7711337379161135e-09, 'hidden_units_1': 153}\n"
     ]
    }
   ],
   "source": [
    "hyperparameters = hyperparameter_trials[trial_idx]\n",
    "print hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the Network\n",
    "\n",
    "Tutorial for Neural Net Architecture: https://pytorch.org/tutorials/beginner/blitz/neural_networks_tutorial.html\n",
    "\n",
    "Utilize batch normalization, as explained here: https://www.youtube.com/watch?v=fv1Luwd-LOI&index=69&list=PLBAGcD3siRDguyYYzhVwZ3tLvOyyG5k6K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##epochs = defaultdict(list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define the network with batch normalization\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, dropout_parameter = 0.5, hidden_units_1 = hyperparameters[\"hidden_units_1\"], \n",
    "                 hidden_units_2 = hyperparameters[\"hidden_units_2\"], batch_size = hyperparameters[\"batch_size\"], \n",
    "                 learning_rate = hyperparameters[\"learning_rate\"], beta = hyperparameters[\"beta\"], \n",
    "                 weight_decay = hyperparameters[\"weight_decay\"]):\n",
    "\n",
    "        super(Net, self).__init__()\n",
    "        self.input = nn.Linear(len(features_cols), hidden_units_1) # read input size from the .shape of data table\n",
    "        self.hidden1 = nn.Linear(hidden_units_1, hidden_units_2)\n",
    "        self.hidden1_bn = nn.BatchNorm1d(hidden_units_2)\n",
    "        self.hidden2 = nn.Linear(hidden_units_2, hidden_units_2)\n",
    "        self.hidden2_bn = nn.BatchNorm1d(hidden_units_2)\n",
    "        self.hidden3 = nn.Linear(hidden_units_2, hidden_units_1)\n",
    "        self.hidden3_bn = nn.BatchNorm1d(hidden_units_1)\n",
    "        self.dropout = nn.Dropout(p = dropout_parameter)\n",
    "        self.output = nn.Linear(hidden_units_1,2)\n",
    "        self.learning_rate = learning_rate\n",
    "        self.beta = beta\n",
    "        self.batch_size = batch_size\n",
    "        self.weight_decay = weight_decay\n",
    "  \n",
    "    def forward(self, x):\n",
    "        x = F.rrelu(self.input(x))\n",
    "        x = self.dropout(F.rrelu(self.hidden1_bn(self.hidden1(x))))\n",
    "        x = self.dropout(F.rrelu(self.hidden2_bn(self.hidden2(x))))\n",
    "        x = self.dropout(F.rrelu(self.hidden3_bn(self.hidden3(x))))\n",
    "        x = self.output(x)\n",
    "        return x\n",
    "    \n",
    "    def fit(self, X_train, y_train_label, X_valid, y_valid, weight):\n",
    "\n",
    "            # set random seed for weights and biases\n",
    "            torch.manual_seed(0)\n",
    "\n",
    "            # dataset\n",
    "            dataset = pd.concat([X_train,y_train_label],axis=1)\n",
    "            dataset = shuffle(dataset, random_state = 0)\n",
    "\n",
    "            X_train = dataset.iloc[:,:dataset.shape[1]-1]\n",
    "            y_train_label = dataset.iloc[:,dataset.shape[1]-1]\n",
    "\n",
    "\n",
    "            # create loss function\n",
    "            loss = nn.BCEWithLogitsLoss(weight = weight)\n",
    "            # mini-batching\n",
    "            batch_size = self.batch_size\n",
    "\n",
    "            BETA_2 = 0.999\n",
    "            TOTAL_EPOCHS_TRAINED = 10**4\n",
    "\n",
    "            # create adam optimizer for Phase 1\n",
    "            optimizer_1 = optim.Adam(self.parameters(), lr=self.learning_rate,betas=(self.beta,BETA_2), \n",
    "                                     weight_decay = self.weight_decay)\n",
    "\n",
    "            lambda1 = lambda epoch_count: 0.99 ** epoch_count \n",
    "            scheduler = LambdaLR(optimizer_1, lr_lambda=lambda1)\n",
    "            no_batch_minus_1 = X_train.shape[0] / batch_size \n",
    "\n",
    "            # Repeated Stratified K Fold to ensure positives are evenly distributed across batchSes\n",
    "            skf_1 = RepeatedStratifiedKFold(n_splits=no_batch_minus_1,n_repeats=TOTAL_EPOCHS_TRAINED,random_state=0)\n",
    "\n",
    "            INITIAL_PATIENCE = 10000\n",
    "            count = 0\n",
    "            epoch_count = 0\n",
    "            max_auprc = 0\n",
    "            ideal_epoch_count = 0 \n",
    "            patience = INITIAL_PATIENCE\n",
    "            patience_j = 0\n",
    "\n",
    "            for train,test in skf_1.split(X_train,y_train_label):\n",
    "                data = X_train.iloc[test,:]\n",
    "                data = torch.Tensor(data.values.astype(np.float32))\n",
    "                 # forward pass\n",
    "                output = self.forward(data)\n",
    "                output.data = output.data.view(data.shape[0],2)\n",
    "\n",
    "                labels = y_train_label[test]\n",
    "                labels = labels.astype(int)\n",
    "                labels = torch.Tensor(np.eye(2)[labels])\n",
    "                labels = torch.autograd.Variable(labels, requires_grad = False)\n",
    "\n",
    "                # zero the gradient buffers\n",
    "                optimizer_1.zero_grad()\n",
    "                # compute loss and gradients\n",
    "                loss_output = loss(output,labels)\n",
    "                loss_output.backward()\n",
    "                # Does the update\n",
    "                optimizer_1.step()\n",
    "\n",
    "                count = count + 1\n",
    "\n",
    "                # Early Stopping\n",
    "                if count == no_batch_minus_1 + 1:\n",
    "                    count = 0\n",
    "                    epoch_count = epoch_count + 1\n",
    "                    scheduler.step()\n",
    "                    probs_valid = self.predict_proba(X_valid)\n",
    "                    precision, recall, _ = precision_recall_curve(y_valid, probs_valid)\n",
    "                    auprc = auc(recall, precision)\n",
    "                    print auprc\n",
    "                    if auprc > max_auprc:\n",
    "                        max_auprc = auprc\n",
    "                        ideal_epoch_count = epoch_count\n",
    "                        patience = patience + epoch_count\n",
    "                        patience_j = 0\n",
    "                    else:\n",
    "                        patience_j = patience_j + 1 \n",
    "                        if patience_j == patience: break\n",
    "            \n",
    "                self.train()\n",
    "            return max_auprc, ideal_epoch_count\n",
    "\n",
    "        \n",
    "    #prediction probabilities array\n",
    "    def predict_proba(self, X_test):\n",
    "        self.eval()\n",
    "        #forward pass\n",
    "        test = torch.Tensor(X_test.values.astype(np.float32))\n",
    "        output = self.forward(test)\n",
    "        sf = nn.Softmax()\n",
    "        probs = sf(output.data)\n",
    "        return probs[:,1]\n",
    "    \n",
    "    def evaluate_loss(self,X_valid, y_valid, weight):\n",
    "        data = torch.Tensor(X_valid.values.astype(np.float32))\n",
    "        loss = nn.BCEWithLogitsLoss(weight = weight)\n",
    "         # forward pass\n",
    "        output = self.forward(data)\n",
    "        output.data = output.data.view(data.shape[0],2)\n",
    "\n",
    "        labels = y_valid\n",
    "        labels = labels.astype(int)\n",
    "        labels = torch.Tensor(np.eye(2)[labels])\n",
    "        labels = torch.autograd.Variable(labels, requires_grad = False)\n",
    "\n",
    "        # compute loss and gradients\n",
    "        loss_output = loss(output,labels)\n",
    "\n",
    "        return loss_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Models tested (and their hyper-parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_model(classifier_method, hyperparameters, no_pos=1, no_neg=1):\n",
    "    \n",
    "    xgb_trees_limit = 5000\n",
    "    \n",
    "    if (classifier_method == \"XGB\"):\n",
    "        if (hyperparameters[\"scale_pos_weight\"] == \"balanced\"):\n",
    "            scale_weight = no_neg/float(no_pos)\n",
    "        else:\n",
    "            scale_weight = hyperparameters[\"scale_pos_weight\"]\n",
    "        model = XGBClassifier(n_estimators=xgb_trees_limit, n_jobs=-1, random_state=0, max_depth=hyperparameters[\"max_depth\"], \n",
    "                              min_child_weight=hyperparameters[\"min_child_weight\"], colsample_bytree=hyperparameters[\"colsample_bytree\"], \n",
    "                              gamma=hyperparameters[\"gamma\"], learning_rate=hyperparameters[\"learning_rate\"], scale_pos_weight=scale_weight)\n",
    "        \n",
    "    elif (classifier_method == \"RF\"):\n",
    "        model = RandomForestClassifier(n_estimators=hyperparameters[\"n_estimators\"], n_jobs=-1, random_state=0,\n",
    "                                      max_depth=hyperparameters[\"max_depth\"], min_samples_leaf=hyperparameters[\"min_samples_leaf\"],\n",
    "                                      min_samples_split=hyperparameters[\"min_samples_split\"], class_weight=hyperparameters[\"class_weight\"])\n",
    "        \n",
    "    elif(classifier_method == \"Logistic\"):\n",
    "        model = LogisticRegression(C=hyperparameters[\"C\"], random_state=0, n_jobs=-1, class_weight=hyperparameters[\"class_weight\"])\n",
    "        \n",
    "    elif (classifier_method == \"KNN\"):\n",
    "        model = KNeighborsClassifier(n_neighbors=hyperparameters[\"n_neighbors\"], n_jobs=-1, weights=hyperparameters[\"weights\"])\n",
    "        \n",
    "    elif (classifier_method == \"ADA\"):\n",
    "        model = AdaBoostClassifier(n_estimators=hyperparameters[\"n_estimators\"], random_state=0, learning_rate=hyperparameters[\"learning_rate\"])\n",
    "        \n",
    "    elif (classifier_method == \"SVM\"):\n",
    "        model = SVC(C=hyperparameters[\"C\"], gamma = hyperparameters[\"gamma\"], kernel=hyperparameters[\"kernel\"], probability=True, random_state=0, cache_size=400,\n",
    "                    class_weight = hyperparameters[\"class_weight\"])\n",
    "        \n",
    "    elif (classifier_method ==\"NN\"):\n",
    "        torch.manual_seed(0)\n",
    "        model = Net(dropout_parameter = 0.5, hidden_units_1 = hyperparameters[\"hidden_units_1\"], \n",
    "                 hidden_units_2 = hyperparameters[\"hidden_units_2\"], batch_size = hyperparameters[\"batch_size\"], \n",
    "                 learning_rate = hyperparameters[\"learning_rate\"], beta = hyperparameters[\"beta\"], \n",
    "                 weight_decay = hyperparameters[\"weight_decay\"])\n",
    "        # sets model in training mode because batch normalization behavior in training and testing modes are different\n",
    "        model.train()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dealing with model imbalance\n",
    "Weight Vector: https://towardsdatascience.com/dealing-with-imbalanced-classes-in-machine-learning-d43d6fa19d2 (look at section on \"Cost-sensitive Learning\")\n",
    "\n",
    "Implementing Early Stopping for XGBoost: https://cambridgespark.com/content/tutorials/hyperparameter-tuning-in-xgboost/index.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test_model_iterative_fixed(hyperparameters_dict,ligand_bind_features, ligand_negatives_features, ligand_name, features=[]):\n",
    "    \n",
    "    \"\"\"\n",
    "    Test different models in k-folds cross-validation.\n",
    "    \"\"\"\n",
    "    \n",
    "    #Default: Exclude no features\n",
    "    if len(features) == 0:\n",
    "        features = np.ones([ligand_bind_features.shape[1],]).astype(bool)\n",
    "    \n",
    "    models_req_scaling = [\"SVM\", \"KNN\", \"Logistic\", \"NN\"]\n",
    "    classifier = classifier_method\n",
    "\n",
    "    #Create X and y with included features\n",
    "    X = pd.concat([ligand_bind_features.iloc[:,features], ligand_negatives_features.iloc[:,features]])\n",
    "    y = [1] * ligand_bind_features.shape[0]\n",
    "    y.extend([0] * ligand_negatives_features.shape[0])\n",
    "    y = np.array(y)\n",
    "    y_df = pd.DataFrame(y)\n",
    "    y_df.index = X.index\n",
    "    y_df.columns = [\"label\"]\n",
    "    \n",
    "    #Get the fold indices\n",
    "    cv_idx = calc_CV_idx_iterative(X, splits_dict)\n",
    "    k = (int(fold)-1)\n",
    "    \n",
    "    pred_idx = k+1\n",
    "    print \"fold #: \"+str(pred_idx)\n",
    "    #test_index = cv_idx[k][\"test\"]\n",
    "    full_train_index = cv_idx[k][\"train\"]\n",
    "        \n",
    "    # phase 1: testing on validation set, hyperparameter tuning\n",
    "    \n",
    "    trial_auprc_results = np.zeros(folds_num-1)\n",
    "    trial_auc_results = np.zeros(folds_num-1)\n",
    "    epoch_counts = np.zeros(folds_num-1, dtype = \"int\")\n",
    "    for i in range(folds_num-1):\n",
    "    #for i in range(1):\n",
    "        valid_k = (k + 1 + i) % folds_num\n",
    "        valid_index = cv_idx[valid_k][\"test\"]\n",
    "\n",
    "        train_index = [index for index in full_train_index if index not in valid_index]\n",
    "        X_train, X_valid = X.loc[train_index,:], X.loc[valid_index,:]\n",
    "        y_train, y_valid = y_df.loc[train_index,:], y_df.loc[valid_index,:]\n",
    "\n",
    "        if (classifier in models_req_scaling):\n",
    "            cols = X_train.columns\n",
    "\n",
    "            # phase 1 scaling with just training data\n",
    "            scaler_1 = StandardScaler() \n",
    "            scaler_1.fit(X_train) \n",
    "            X_train = pd.DataFrame(scaler_1.transform(X_train))\n",
    "            # apply same transformation to validation data\n",
    "            X_valid = pd.DataFrame(scaler_1.transform(X_valid))\n",
    "\n",
    "            #Restoring indices after scaling\n",
    "            X_train.index = train_index \n",
    "            X_valid.index = valid_index\n",
    "\n",
    "            #Restoring features names\n",
    "            X_train.columns = cols\n",
    "            X_valid.columns = cols\n",
    "\n",
    "        #No down-sampling\n",
    "        X_train_sampled = X_train\n",
    "        y_train_sampled = y_train\n",
    "        \n",
    "        #pos and neg numbers in the training\n",
    "        no_pos = np.count_nonzero(y_train_sampled[\"label\"] == 1)\n",
    "        no_neg = np.count_nonzero(y_train_sampled[\"label\"] == 0)  \n",
    "        \n",
    "        #fit to training data\n",
    "        if (classifier == \"NN\"):\n",
    "            if hyperparameters[\"weight\"] == \"balanced\":              \n",
    "                #weight vector\n",
    "                neg_weight = float(no_pos) / float(no_neg + no_pos) \n",
    "                pos_weight = 1 - neg_weight\n",
    "            elif hyperparameters[\"weight\"] == \"0.1\":\n",
    "                neg_weight = 10\n",
    "                pos_weight = 1\n",
    "            elif hyperparameters[\"weight\"] == \"None\":\n",
    "                neg_weight = 1\n",
    "                pos_weight = 1\n",
    "            \n",
    "            weight = torch.Tensor([neg_weight, pos_weight])\n",
    "            model = generate_model(classifier_method, hyperparameters)\n",
    "            auprc_score,epoch_count = model.fit(X_train_sampled, y_train_sampled[\"label\"],X_valid, y_valid[\"label\"], weight)\n",
    "\n",
    "        elif (classifier == \"XGB\"):\n",
    "            num_early_stopping_rounds = 750\n",
    "            model = generate_model(classifier_method, hyperparameters, no_pos = no_pos, no_neg = no_neg)\n",
    "            model.fit(X_train_sampled, y_train_sampled[\"label\"], eval_set = [(X_valid,y_valid[\"label\"])], eval_metric = \"map\", \n",
    "                      verbose=False, early_stopping_rounds = num_early_stopping_rounds)\n",
    "            probs_list = []\n",
    "            probs = model.predict_proba(X_valid, ntree_limit=model.best_ntree_limit)\n",
    "            for l in probs:\n",
    "                probs_list.append(l[1])\n",
    "            precision, recall, _ = precision_recall_curve(y_valid, probs_list)\n",
    "            auprc_score = auc(recall, precision)\n",
    "            auc_score = roc_auc_score(y_valid, probs_list)\n",
    "            print \"model.best_iteration = \"+str(model.best_iteration)\n",
    "            epoch_count = model.best_ntree_limit\n",
    "            \n",
    "\n",
    "        else:            \n",
    "            model = generate_model(classifier_method, hyperparameters)\n",
    "            model.fit(X_train_sampled, y_train_sampled[\"label\"])\n",
    "            probs_list = []\n",
    "            probs = model.predict_proba(X_valid)\n",
    "            for l in probs:\n",
    "                probs_list.append(l[1])\n",
    "            precision, recall, _ = precision_recall_curve(y_valid, probs_list)\n",
    "            auprc_score = auc(recall, precision)\n",
    "        \n",
    "        \n",
    "        print \"AUPRC = \"+str(auprc_score)\n",
    "        print \"AUC = \"+str(auc_score)\n",
    "        trial_auprc_results[i] = auprc_score \n",
    "        trial_auc_results[i] = auc_score \n",
    "        if classifier == \"NN\" or classifier == \"XGB\": epoch_counts[i] = epoch_count\n",
    "    \n",
    "    mean_auprc_result = np.mean(trial_auprc_results)\n",
    "    mean_auc_result = np.mean(trial_auc_results)\n",
    "    var_auprc_result = np.var(trial_auprc_results)\n",
    "    var_auc_result = np.var(trial_auc_results)\n",
    "    if classifier == \"NN\" or classifier == \"XGB\":\n",
    "        mean_epoch_count = int(np.mean(epoch_counts))\n",
    "        hyperparameters_dict[\"mean_epoch_count\"] = mean_epoch_count\n",
    "\n",
    "    hyperparameters_dict[\"mean_AUPRC\"] = mean_auprc_result\n",
    "    hyperparameters_dict[\"mean_AUC\"] = mean_auc_result\n",
    "    hyperparameters_dict[\"var_AUPRC\"] = var_auprc_result\n",
    "    hyperparameters_dict[\"var_AUC\"] = var_auc_result\n",
    "    hyperparameters_dict[\"trial_idx\"] = trial_idx\n",
    "\n",
    "    # Update dictionary with all hyperparameters\n",
    "    keys = hyperparameters.keys()\n",
    "    for key in keys:\n",
    "        hyperparameters_dict[key].append(hyperparameters[key])\n",
    "    pred_idx += 1\n",
    "\n",
    "    print \"Finished \"+ligand+\" \"+classifier+\" fold: \"+fold+\" trial: \"+str(trial_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predict for each ligand seperatelly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold #: 1\n",
      "0.0357437730442\n",
      "0.107610030126\n",
      "0.183625613828\n",
      "0.211979785469\n",
      "0.254252145429\n",
      "0.272404761141\n",
      "0.407298633932\n",
      "0.432589542091\n",
      "0.466299966288\n",
      "0.481557640008\n",
      "0.494097361174\n",
      "0.502923015478\n",
      "0.537071980794\n",
      "0.542660701797\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/madhumithashridharan/anaconda/lib/python2.7/site-packages/ipykernel/__main__.py:122: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-61-5d8c9a4119eb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_cell_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mu'time'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mu''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mu'\\n#Initialize dictionary\\nhyperparameters_dict = defaultdict(list)\\n\\ntest_model_iterative_fixed(hyperparameters_dict,ligands_positives_df[ligand], ligands_negatives_df[ligand], ligand)\\n\\nhyperparameters_df = pd.DataFrame.from_dict(hyperparameters_dict)\\n\\n#Save to file\\nhyperparameters_df.to_csv(curr_dir[0]+\"/hyperparam_tuning/phase1_initial_run/\"+datafile_date+\"_\"+prec_th_str+\"/per_trial/\"+ligand+\"_\"+classifier_method+\"_fold\"+fold+\"_trial\"+str(trial_idx)+\"_\"+str(folds_num)+\"w_hyperparameters.csv\", sep=\\'\\\\t\\')\\n\\nprint \"Finished ligand \"+ligand'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Users/madhumithashridharan/anaconda/lib/python2.7/site-packages/IPython/core/interactiveshell.pyc\u001b[0m in \u001b[0;36mrun_cell_magic\u001b[0;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[1;32m   2118\u001b[0m             \u001b[0mmagic_arg_s\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvar_expand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstack_depth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2119\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2120\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmagic_arg_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2121\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<decorator-gen-60>\u001b[0m in \u001b[0;36mtime\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n",
      "\u001b[0;32m/Users/madhumithashridharan/anaconda/lib/python2.7/site-packages/IPython/core/magic.pyc\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(f, *a, **k)\u001b[0m\n\u001b[1;32m    191\u001b[0m     \u001b[0;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 193\u001b[0;31m         \u001b[0mcall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    194\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/madhumithashridharan/anaconda/lib/python2.7/site-packages/IPython/core/magics/execution.pyc\u001b[0m in \u001b[0;36mtime\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n\u001b[1;32m   1175\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1176\u001b[0m             \u001b[0mst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1177\u001b[0;31m             \u001b[0;32mexec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocal_ns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1178\u001b[0m             \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m<ipython-input-60-3fda11ddbc5d>\u001b[0m in \u001b[0;36mtest_model_iterative_fixed\u001b[0;34m(hyperparameters_dict, ligand_bind_features, ligand_negatives_features, ligand_name, features)\u001b[0m\n\u001b[1;32m     85\u001b[0m             \u001b[0mweight\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mneg_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpos_weight\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m             \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclassifier_method\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhyperparameters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m             \u001b[0mauprc_score\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepoch_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_sampled\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train_sampled\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"label\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mX_valid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_valid\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"label\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mclassifier\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"XGB\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-57-49e9734b91d7>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X_train, y_train_label, X_valid, y_valid, weight)\u001b[0m\n\u001b[1;32m     73\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m                  \u001b[0;31m# forward pass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m                 \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m                 \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-57-49e9734b91d7>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden1_bn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden2_bn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden3_bn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/madhumithashridharan/anaconda/lib/python2.7/site-packages/torch/nn/modules/module.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    489\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    490\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 491\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    492\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    493\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/madhumithashridharan/anaconda/lib/python2.7/site-packages/torch/nn/modules/dropout.pyc\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minplace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/madhumithashridharan/anaconda/lib/python2.7/site-packages/torch/nn/functional.pyc\u001b[0m in \u001b[0;36mdropout\u001b[0;34m(input, p, training, inplace)\u001b[0m\n\u001b[1;32m    550\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    551\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 552\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_functions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDropout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    553\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    554\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/madhumithashridharan/anaconda/lib/python2.7/site-packages/torch/nn/_functions/dropout.pyc\u001b[0m in \u001b[0;36mforward\u001b[0;34m(cls, ctx, input, p, train, inplace)\u001b[0m\n\u001b[1;32m     38\u001b[0m             \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnoise\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfill_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m             \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnoise\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbernoulli_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiv_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m         \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnoise\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnoise\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_as\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnoise\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "#Initialize dictionary\n",
    "hyperparameters_dict = defaultdict(list)\n",
    "\n",
    "test_model_iterative_fixed(hyperparameters_dict,ligands_positives_df[ligand], ligands_negatives_df[ligand], ligand)\n",
    "\n",
    "hyperparameters_df = pd.DataFrame.from_dict(hyperparameters_dict)\n",
    "\n",
    "#Save to file\n",
    "hyperparameters_df.to_csv(curr_dir[0]+\"/hyperparam_tuning/phase1_initial_run/\"+datafile_date+\"_\"+prec_th_str+\"/per_trial/\"+ligand+\"_\"+classifier_method+\"_fold\"+fold+\"_trial\"+str(trial_idx)+\"_\"+str(folds_num)+\"w_hyperparameters.csv\", sep='\\t')\n",
    "\n",
    "print \"Finished ligand \"+ligand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##epochs_df = pd.DataFrame.from_dict(epochs)\n",
    "#Save to file\n",
    "##epochs_df.to_csv(curr_dir[0]+\"/epochs_met.csv\", sep=',')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
