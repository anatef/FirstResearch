{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Basic imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import pickle\n",
    "from collections import defaultdict\n",
    "from os import environ, getcwd\n",
    "import sys\n",
    "\n",
    "#Classifier imports\n",
    "from sklearn.neighbors import KNeighborsClassifier \n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "#ML framework imports\n",
    "from sklearn.metrics import auc, roc_auc_score, precision_recall_curve\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.preprocessing import scale\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "# Neural Net imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "#Import utils functions\n",
    "curr_dir = !pwd\n",
    "\n",
    "sys.path.append(curr_dir[0]+\"/utils\")\n",
    "from prop_threshold_funcs import create_negatives_datasets_combined, create_positives_datasets_combined\n",
    "from prediction_general_funcs import ligands, score_cols_suffix, get_features_cols, remove_unimportant_features\n",
    "from CV_funcs import add_domain_name_from_table_idx, calc_CV_idx_iterative\n",
    "from generate_hyperparameter_trials import *\n",
    "\n",
    "\n",
    "from IPython.core.display import HTML\n",
    "HTML(\"<style>.container { width:100% !important; }</style>\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading the input dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all samples positions #: 42535\n"
     ]
    }
   ],
   "source": [
    "curr_dir = !pwd\n",
    "pfam_version = \"31\"\n",
    "datafile_date = \"06.20.18\"\n",
    "input_path = curr_dir[0]+\"/domains_similarity/filtered_features_table/\"\n",
    "filename = \"windowed_positions_features_mediode_filter_\"+datafile_date+\".csv\"\n",
    "out_dir = \"mediode_NegLigand_NoFilter\"\n",
    "\n",
    "#flags for creating negatives\n",
    "zero_prop = True\n",
    "no_prop = True\n",
    "all_ligands = False\n",
    "prec_th_str = \"dna0.5_rna0.25_ion0.75\"\n",
    "folds_num = 5\n",
    "\n",
    "#Features table\n",
    "features_all = pd.read_csv(input_path+filename, sep='\\t', index_col=0)\n",
    "#Features columns names, without the labels (the binding scores)\n",
    "features_cols = get_features_cols(features_all)\n",
    "remove_unimportant_features(features_all, features_cols)\n",
    "\n",
    "print \"all samples positions #: \"+str(features_all.shape[0])\n",
    "\n",
    "#CV splits dictionary\n",
    "# with open(curr_dir[0]+\"/CV_splits/pfam-v\"+pfam_version+\"/domain_\"+str(folds_num)+\"_folds_\"+str(prec_th)+\"_prec_dict.pik\", 'rb') as handle:\n",
    "#         splits_dict = pickle.load(handle)\n",
    "with open(curr_dir[0]+\"/CV_splits/pfam-v\"+pfam_version+\"/domain_\"+str(folds_num)+\"_folds_combined_\"+prec_th_str+\"_prec_dict.pik\", 'rb') as handle:\n",
    "        splits_dict = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataset of negative examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dna non-binding #:41680\n",
      "dnabase non-binding #:42089\n",
      "dnabackbone non-binding #:41689\n",
      "dna combined non binding #: 41555\n",
      "rna non-binding #:41613\n",
      "rnabase non-binding #:41828\n",
      "rnabackbone non-binding #:41619\n",
      "rna combined non binding #: 41401\n",
      "peptide non-binding #:38794\n",
      "ion non-binding #:37525\n",
      "metabolite non-binding #:37463\n",
      "sm non-binding #:30978\n"
     ]
    }
   ],
   "source": [
    "ligands_negatives_df = create_negatives_datasets_combined(zero_prop, no_prop, features_all, features_cols, all_ligands)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Datasets of positive examples by ligand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dna #: 239\n",
      "dnabase #: 170\n",
      "dnabackbone #: 244\n",
      "dna combined #: 353\n",
      "rna #: 360\n",
      "rnabase #: 246\n",
      "rnabackbone #: 346\n",
      "rna combined #: 468\n",
      "peptide #: 462\n",
      "ion #: 350\n",
      "metabolite #: 504\n",
      "sm #: 708\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/madhumithashridharan/anaconda/lib/python2.7/site-packages/pandas/core/frame.py:1997: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  \"DataFrame index.\", UserWarning)\n"
     ]
    }
   ],
   "source": [
    "ligands_positives_df = create_positives_datasets_combined(features_all, features_cols, all_ligands)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading env input for downsampler technique, ligand and classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ligand = ion\n",
      "fold = 1\n",
      "classifier_method = XGB\n",
      "trial idx = 0\n"
     ]
    }
   ],
   "source": [
    "#Reading the ligand input\n",
    "try:\n",
    "    ligand = environ['ligand']\n",
    "except:\n",
    "    ligand = \"ion\"\n",
    "print \"ligand = \"+ligand\n",
    "    \n",
    "#Reading the downsampler input\n",
    "try: \n",
    "    fold = environ['fold']\n",
    "except:\n",
    "    fold = \"1\"\n",
    "print \"fold = \"+fold\n",
    "\n",
    "#Reading the classifier input\n",
    "try: \n",
    "    classifier_method = environ['classifier']\n",
    "except:\n",
    "    classifier_method = \"XGB\"\n",
    "print \"classifier_method = \"+classifier_method\n",
    "\n",
    "# Reading the index to generate model\n",
    "try:\n",
    "    trial_idx = int(environ[\"trial\"])\n",
    "except:\n",
    "    trial_idx = 0\n",
    "print \"trial idx = \"+ str(trial_idx)\n",
    "\n",
    "if classifier_method == \"NN\":\n",
    "    try:        \n",
    "        learning_rate_ub = int(environ['learning_rate_ub'])\n",
    "        learning_rate_lb = int(environ['learning_rate_lb'])\n",
    "        batch_size_ub = int(environ['batch_size_ub'])\n",
    "        batch_size_lb = int(environ['batch_size_lb'])\n",
    "        weight_decay_ub = int(environ['weight_decay_ub'])\n",
    "        weight_decay_lb = int(environ['weight_decay_lb'])\n",
    "        beta_ub = float(environ['beta_ub'])\n",
    "        beta_lb = float(environ['beta_lb'])\n",
    "        hidden_units_1_ub = int(environ['hidden_units_1_ub'])\n",
    "        hidden_units_1_lb = int(environ['hidden_units_1_lb'])\n",
    "        hidden_units_2_ub = int(environ['hidden_units_2_ub'])\n",
    "        hidden_units_2_lb = int(environ['hidden_units_2_lb'])\n",
    "        \n",
    "        try:\n",
    "            sec_learning_rate_ub = int(environ['sec_learning_rate_ub'])\n",
    "            sec_learning_rate_lb = int(environ['sec_learning_rate_lb'])\n",
    "        except:\n",
    "            sec_learning_rate_ub = learning_rate_ub \n",
    "            sec_learning_rate_lb = learning_rate_lb\n",
    "            \n",
    "        try:\n",
    "            sec_batch_size_ub = int(environ['sec_batch_size_ub'])\n",
    "            sec_batch_size_lb = int(environ['sec_batch_size_lb'])\n",
    "        except:\n",
    "            sec_batch_size_ub = batch_size_ub\n",
    "            sec_batch_size_lb = batch_size_lb\n",
    "        try:\n",
    "            sec_weight_decay_ub = int(environ['sec_weight_decay_ub'])\n",
    "            sec_weight_decay_lb = int(environ['sec_weight_decay_lb'])\n",
    "            \n",
    "        except:\n",
    "            sec_weight_decay_ub = weight_decay_ub\n",
    "            sec_weight_decay_lb = weight_decay_lb\n",
    "        try:\n",
    "            sec_beta_ub = float(environ['sec_beta_ub'])\n",
    "            sec_beta_lb = float(environ['sec_beta_lb'])\n",
    "        except:\n",
    "            sec_beta_ub = beta_ub\n",
    "            sec_beta_lb = beta_lb\n",
    "        try:\n",
    "            sec_hidden_units_1_ub = int(environ['sec_hidden_units_1_ub'])\n",
    "            sec_hidden_units_1_lb = int(environ['sec_hidden_units_1_lb'])\n",
    "        except:\n",
    "            sec_hidden_units_1_ub = hidden_units_1_ub\n",
    "            sec_hidden_units_1_lb = hidden_units_1_lb\n",
    "        try:\n",
    "            sec_hidden_units_2_ub = int(environ['sec_hidden_units_2_ub'])\n",
    "            sec_hidden_units_2_lb = int(environ['sec_hidden_units_2_lb'])\n",
    "        except: \n",
    "            sec_hidden_units_2_ub = hidden_units_2_ub\n",
    "            sec_hidden_units_2_lb = hidden_units_2_lb\n",
    "            \n",
    "            \n",
    "\n",
    "    except:        \n",
    "        learning_rate_ub = -2\n",
    "        learning_rate_lb = -3\n",
    "        batch_size_ub = 150\n",
    "        batch_size_lb = 30\n",
    "        weight_decay_ub = -7\n",
    "        weight_decay_lb = -17\n",
    "        beta_ub = 0.95\n",
    "        beta_lb = 0.85\n",
    "        hidden_units_1_ub = 300\n",
    "        hidden_units_1_lb = 50\n",
    "        hidden_units_2_ub = 800\n",
    "        hidden_units_2_lb = 350\n",
    "        \n",
    "        sec_learning_rate_ub = -2\n",
    "        sec_learning_rate_lb = -3\n",
    "        sec_batch_size_ub = 400\n",
    "        sec_batch_size_lb = 200\n",
    "        sec_weight_decay_ub = -7\n",
    "        sec_weight_decay_lb = -17\n",
    "        sec_beta_ub = 0.95\n",
    "        sec_beta_lb = 0.85\n",
    "        sec_hidden_units_1_ub = 300\n",
    "        sec_hidden_units_1_lb = 50\n",
    "        sec_hidden_units_2_ub = 800\n",
    "        sec_hidden_units_2_lb = 350\n",
    "    \n",
    "\n",
    "elif classifier_method == \"XGB\":\n",
    "    \n",
    "    try:\n",
    "        max_depth_ub = int(environ[\"max_depth_ub\"])\n",
    "        max_depth_lb = int(environ[\"max_depth_lb\"])\n",
    "        min_child_weight_ub = int(environ[\"min_child_weight_ub\"])\n",
    "        min_child_weight_lb = int(environ[\"min_child_weight_lb\"])\n",
    "        colsample_bytree_ub = float(environ[\"colsample_bytree_ub\"])\n",
    "        colsample_bytree_lb = float(environ[\"colsample_bytree_lb\"])\n",
    "        gamma_ub = int(environ[\"gamma_ub\"])\n",
    "        gamma_lb = int(environ[\"gamma_lb\"])\n",
    "        learning_rate_ub = float(environ[\"learning_rate_ub\"])\n",
    "        learning_rate_lb = int(environ[\"learning_rate_lb\"])\n",
    "\n",
    "        try:\n",
    "            sec_max_depth_ub = int(environ[\"sec_max_depth_ub\"])\n",
    "            sec_max_depth_lb = int(environ[\"sec_max_depth_lb\"])\n",
    "        except:\n",
    "            sec_max_depth_ub = max_depth_ub\n",
    "            sec_max_depth_lb = max_depth_lb\n",
    "        try:\n",
    "            sec_min_child_weight_ub = int(environ['sec_min_child_weight_ub'])\n",
    "            sec_min_child_weight_lb = int(environ['sec_min_child_weight_lb'])\n",
    "        except:\n",
    "            sec_min_child_weight_ub = min_child_weight_ub\n",
    "            sec_min_child_weight_lb = min_child_weight_lb\n",
    "        try:\n",
    "            sec_colsample_bytree_ub = float(environ['sec_colsample_bytree_ub'])\n",
    "            sec_colsample_bytree_lb = float(environ['sec_colsample_bytree_lb'])\n",
    "        except:\n",
    "            sec_colsample_bytree_ub = colsample_bytree_ub\n",
    "            sec_colsample_bytree_lb = colsample_bytree_lb\n",
    "        try:\n",
    "            sec_gamma_ub = int(environ['sec_gamma_ub'])\n",
    "            sec_gamma_lb = int(environ['sec_gamma_lb'])\n",
    "        except:\n",
    "            sec_gamma_ub = gamma_ub\n",
    "            sec_gamma_lb = gamma_lb            \n",
    "        try:\n",
    "            sec_learning_rate_ub = int(environ['sec_learning_rate_ub'])\n",
    "            sec_learning_rate_lb = int(environ['sec_learning_rate_lb'])\n",
    "        except:\n",
    "            sec_learning_rate_ub = learning_rate_ub\n",
    "            sec_learning_rate_lb = learning_rate_lb\n",
    "\n",
    "    except:    \n",
    "        \n",
    "        max_depth_ub = 1500\n",
    "        max_depth_lb = 100\n",
    "        min_child_weight_ub = 2\n",
    "        min_child_weight_lb = 0\n",
    "        colsample_bytree_ub = 1\n",
    "        colsample_bytree_lb = 0.25\n",
    "        gamma_ub = 0\n",
    "        gamma_lb = -3\n",
    "        learning_rate_ub = -0.5\n",
    "        learning_rate_lb = -3\n",
    "        \n",
    "        sec_max_depth_ub = 4000\n",
    "        sec_max_depth_lb = 2000\n",
    "        sec_min_child_weight_ub = 5\n",
    "        sec_min_child_weight_lb = 3\n",
    "        sec_colsample_bytree_ub = 0.25\n",
    "        sec_colsample_bytree_lb = 0\n",
    "        sec_gamma_ub = -4\n",
    "        sec_gamma_lb = -6\n",
    "        sec_learning_rate_ub = -1\n",
    "        sec_learning_rate_lb = -2\n",
    "        \n",
    "\n",
    "elif classifier_method == \"RF\":  \n",
    "    try:\n",
    "        n_estimators_ub = int(environ[\"n_estimators_ub\"])\n",
    "        n_estimators_lb = int(environ[\"n_estimators_lb\"])\n",
    "        max_depth_ub = int(environ[\"max_depth_ub\"])\n",
    "        max_depth_lb = int(environ[\"max_depth_lb\"])\n",
    "        min_samples_leaf_ub = int(environ[\"min_samples_leaf_ub\"])\n",
    "        min_samples_leaf_lb = int(environ[\"min_samples_leaf_lb\"])\n",
    "        min_samples_split_ub = int(environ[\"min_samples_split_ub\"])\n",
    "        min_samples_split_lb = int(environ[\"min_samples_split_lb\"])\n",
    "\n",
    "        try:\n",
    "            sec_n_estimators_ub = int(environ['sec_n_estimators_ub'])\n",
    "            sec_n_estimators_lb = int(environ['sec_n_estimators_lb'])\n",
    "        except:\n",
    "            sec_n_estimators_ub = n_estimators_ub\n",
    "            sec_n_estimators_lb = n_estimators_lb\n",
    "\n",
    "        try:\n",
    "            sec_max_depth_ub = int(environ['sec_max_depth_ub'])\n",
    "            sec_max_depth_lb = int(environ['sec_max_depth_lb'])\n",
    "        except:\n",
    "            sec_max_depth_ub = max_depth_ub\n",
    "            sec_max_depth_lb = max_depth_lb\n",
    "\n",
    "        try:\n",
    "            sec_min_samples_leaf_ub = int(environ['sec_min_samples_leaf_ub'])\n",
    "            sec_min_samples_leaf_lb = int(environ['sec_min_samples_leaf_lb'])\n",
    "        except:\n",
    "            sec_min_samples_leaf_ub = min_samples_leaf_ub\n",
    "            sec_min_samples_leaf_lb = min_samples_leaf_lb\n",
    "        try:\n",
    "            sec_min_samples_split_ub = int(environ['sec_min_samples_split_ub'])\n",
    "            sec_min_samples_split_lb = int(environ['sec_min_samples_split_lb'])\n",
    "        except:\n",
    "            sec_min_samples_split_ub = min_samples_split_ub\n",
    "            sec_min_samples_split_lb = min_samples_split_lb\n",
    "\n",
    "    except:\n",
    "        n_estimators_ub = 1500\n",
    "        n_estimators_lb = 100\n",
    "        max_depth_ub = 20\n",
    "        max_depth_lb = 2\n",
    "        min_samples_leaf_ub = 50\n",
    "        min_samples_leaf_lb = 1\n",
    "        min_samples_split_ub = 50\n",
    "        min_samples_split_lb = 2\n",
    "\n",
    "        sec_n_estimators_ub = 3000\n",
    "        sec_n_estimators_lb = 2000\n",
    "        sec_max_depth_ub = 100\n",
    "        sec_max_depth_lb = 50\n",
    "        sec_min_samples_leaf_ub = 100\n",
    "        sec_min_samples_leaf_lb = 60\n",
    "        sec_min_samples_split_ub = 100\n",
    "        sec_min_samples_split_lb = 60\n",
    "\n",
    "\n",
    "elif classifier_method == \"Logistic\":\n",
    "    try:        \n",
    "        C_ub = int(environ[\"C_ub\"])\n",
    "        C_lb = int(environ[\"C_lb\"])\n",
    "\n",
    "        try:\n",
    "            sec_C_ub = int(environ[\"sec_C_ub\"])\n",
    "            sec_C_lb = int(environ[\"sec_C_lb\"])\n",
    "        except:\n",
    "            sec_C_ub = C_ub\n",
    "            sec_C_lb = C_lb\n",
    "\n",
    "    except:       \n",
    "        C_ub = 3\n",
    "        C_lb = 1\n",
    "        \n",
    "        sec_C_ub = 7\n",
    "        sec_C_lb = 6\n",
    "\n",
    "elif classifier_method == \"KNN\":\n",
    "    try:\n",
    "        n_neighbors_ub = int(environ[\"n_neighbors_ub\"])\n",
    "        n_neighbors_lb = int(environ[\"n_neighbors_lb\"])\n",
    "        \n",
    "        try:\n",
    "            sec_n_neighbors_ub = int(environ[\"sec_n_neighbors_ub\"]) \n",
    "            sec_n_neighbors_lb = int(environ[\"sec_n_neighbors_lb\"]) \n",
    "        except:\n",
    "            sec_n_neighbors_ub = n_neighbors_ub\n",
    "            sec_n_neighbors_lb = n_neighbors_lb\n",
    "            \n",
    "\n",
    "    except:\n",
    "        n_neighbors_ub = 100\n",
    "        n_neighbors_lb = 5\n",
    "        \n",
    "        sec_n_neighbors_ub = 200\n",
    "        sec_n_neighbors_lb = 150\n",
    "        \n",
    "elif classifier_method == \"ADA\":\n",
    "    try:\n",
    "        n_estimators_ub = int(environ[\"n_estimators_ub\"])\n",
    "        n_estimators_lb = int(environ[\"n_estimators_lb\"])\n",
    "        learning_rate_ub = int(environ[\"learning_rate_ub\"])\n",
    "        learning_rate_lb = int(environ[\"learning_rate_lb\"])\n",
    "        \n",
    "        try:         \n",
    "            sec_n_estimators_ub = int(environ[\"sec_n_estimators_ub\"]) \n",
    "            sec_n_estimators_lb = int(environ[\"sec_n_estimators_lb\"]) \n",
    "        except:\n",
    "            sec_n_estimators_ub = n_estimators_ub\n",
    "            sec_n_estimators_lb = n_estimators_lb\n",
    "        try:\n",
    "            sec_learning_rate_ub = int(environ[\"sec_learning_rate_ub\"])\n",
    "            sec_learning_rate_lb = int(environ[\"sec_learning_rate_lb\"]) \n",
    "            \n",
    "        except:\n",
    "            sec_learning_rate_ub = learning_rate_ub\n",
    "            sec_learning_rate_lb = learning_rate_lb \n",
    "        \n",
    "            \n",
    "    except:\n",
    "        \n",
    "        n_estimators_ub = 6\n",
    "        n_estimators_lb = 3\n",
    "        learning_rate_ub = 0\n",
    "        learning_rate_lb = -4\n",
    "        \n",
    "        sec_n_estimators_ub = 12\n",
    "        sec_n_estimators_lb = 9\n",
    "        sec_learning_rate_ub = -14\n",
    "        sec_learning_rate_lb = -15\n",
    "        \n",
    "        \n",
    "        \n",
    "elif classifier_method == \"SVM\":\n",
    "    try:\n",
    "        C_ub = int(environ[\"C_ub\"])\n",
    "        C_lb = int(environ[\"C_lb\"])\n",
    "        gamma_ub = int(environ[\"gamma_ub\"])\n",
    "        gamma_lb = int(environ[\"gamma_lb\"])\n",
    "        \n",
    "        try:\n",
    "            sec_C_ub = int(environ[\"sec_C_ub\"]) \n",
    "            sec_C_lb = int(environ[\"sec_C_lb\"]) \n",
    "        except:\n",
    "            sec_C_ub = C_ub\n",
    "            sec_C_lb = C_lb \n",
    "        \n",
    "        try:\n",
    "            sec_gamma_ub = int(environ[\"sec_gamma_ub\"]) \n",
    "            sec_gamma_lb = int(environ[\"sec_gamma_lb\"])\n",
    "        except:\n",
    "            sec_gamma_ub = gamma_ub\n",
    "            sec_gamma_lb = gamma_lb\n",
    "            \n",
    "    except:\n",
    "        \n",
    "        C_ub = 4\n",
    "        C_lb = 2\n",
    "        gamma_ub = -4\n",
    "        gamma_lb = -6\n",
    "        \n",
    "        sec_C_ub = 10\n",
    "        sec_C_lb = 8\n",
    "        sec_gamma_ub = -13\n",
    "        sec_gamma_lb = -15"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate hyperparameter trials\n",
    "\n",
    "Choose hyperparameters and generate hyperparameters through random search in a grid, as explained by this video: https://www.youtube.com/watch?v=WrICwRrvuIc&index=66&list=PLBAGcD3siRDguyYYzhVwZ3tLvOyyG5k6K\n",
    "\n",
    "Use logarithmic scale for search for learnining rate and weight decay for NN, as explained by this video: https://www.youtube.com/watch?v=VUbrW8OK3uo&index=67&list=PLBAGcD3siRDguyYYzhVwZ3tLvOyyG5k6K\n",
    "\n",
    "Utilize nested cross validation to choose between models, as described here: https://stats.stackexchange.com/questions/266225/step-by-step-explanation-of-k-fold-cross-validation-with-grid-search-to-optimise/266229"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'colsample_bytree': 0.883199311435763, 'scale_pos_weight': 'balanced', 'learning_rate': 0.07034799738710229, 'min_child_weight': 4.185689236450036, 'max_depth': 2684, 'gamma': 0.37483216628479255}, {'colsample_bytree': 0.5781904084470194, 'scale_pos_weight': 0.1, 'learning_rate': 0.09197350992568115, 'min_child_weight': 4.2917882261333125, 'max_depth': 3747, 'gamma': 0.47349893044994734}, {'colsample_bytree': 0.8591265465816199, 'scale_pos_weight': 1, 'learning_rate': 0.02470499646601939, 'min_child_weight': 3.9553302346427, 'max_depth': 2314, 'gamma': 0.027537944297361432}, {'colsample_bytree': 0.31534697477615553, 'scale_pos_weight': 'balanced', 'learning_rate': 0.06801737161546002, 'min_child_weight': 3.1420721163957737, 'max_depth': 2537, 'gamma': 0.0011498870747119448}, {'colsample_bytree': 0.983963756674573, 'scale_pos_weight': 'balanced', 'learning_rate': 0.02893872296497689, 'min_child_weight': 4.7400242964936385, 'max_depth': 3871, 'gamma': 0.24973286104060585}, {'colsample_bytree': 0.7904744910444376, 'scale_pos_weight': 0.1, 'learning_rate': 0.03446459897310035, 'min_child_weight': 4.357759060237921, 'max_depth': 2659, 'gamma': 0.05572619319129036}, {'colsample_bytree': 0.5609964549928927, 'scale_pos_weight': 'balanced', 'learning_rate': 0.05946120272201706, 'min_child_weight': 4.043696643500144, 'max_depth': 2544, 'gamma': 0.006218230782016849}, {'colsample_bytree': 0.35141363005408904, 'scale_pos_weight': 1, 'learning_rate': 0.014114804498181489, 'min_child_weight': 3.4331007088487437, 'max_depth': 3207, 'gamma': 0.009384756816024659}, {'colsample_bytree': 0.9578110588859681, 'scale_pos_weight': 0.1, 'learning_rate': 0.022882733388778418, 'min_child_weight': 4.233867993749514, 'max_depth': 3512, 'gamma': 0.1110352557719758}, {'colsample_bytree': 0.3244602627692304, 'scale_pos_weight': 'balanced', 'learning_rate': 0.044992490724651, 'min_child_weight': 4.804697166347968, 'max_depth': 2888, 'gamma': 0.8117591698634132}, {'colsample_bytree': 0.34669472324114, 'scale_pos_weight': 1, 'learning_rate': 0.023105255265581107, 'min_child_weight': 3.4207651221476816, 'max_depth': 2779, 'gamma': 0.008836597074694356}, {'colsample_bytree': 0.7257055434680013, 'scale_pos_weight': 0.1, 'learning_rate': 0.04495627477756385, 'min_child_weight': 3.0768508529454692, 'max_depth': 2633, 'gamma': 0.7530916041188188}, {'colsample_bytree': 0.6863877470789007, 'scale_pos_weight': 0.1, 'learning_rate': 0.029833039441606802, 'min_child_weight': 4.990599135355775, 'max_depth': 3155, 'gamma': 0.01750272884486083}, {'colsample_bytree': 0.3327813558732289, 'scale_pos_weight': 0.1, 'learning_rate': 0.013746209273114532, 'min_child_weight': 3.3179391672910397, 'max_depth': 2954, 'gamma': 0.09310837957239837}, {'colsample_bytree': 0.7468951502125332, 'scale_pos_weight': 0.1, 'learning_rate': 0.041961025695416834, 'min_child_weight': 4.899142106901484, 'max_depth': 2169, 'gamma': 0.0010982846323993615}, {'colsample_bytree': 0.9823445987600468, 'scale_pos_weight': 1, 'learning_rate': 0.09478968677765556, 'min_child_weight': 3.192196815787926, 'max_depth': 3316, 'gamma': 0.025465581928924366}, {'colsample_bytree': 0.2793908441907405, 'scale_pos_weight': 0.1, 'learning_rate': 0.013188535151716804, 'min_child_weight': 4.478527158796603, 'max_depth': 2917, 'gamma': 0.007053763390632227}, {'colsample_bytree': 0.7664958871043278, 'scale_pos_weight': 0.1, 'learning_rate': 0.08283911802134532, 'min_child_weight': 3.9617870616723256, 'max_depth': 2023, 'gamma': 0.43795316349595603}, {'colsample_bytree': 0.6749510906549314, 'scale_pos_weight': 0.1, 'learning_rate': 0.03336169083954025, 'min_child_weight': 4.38494423874004, 'max_depth': 3987, 'gamma': 0.00625415258687639}, {'colsample_bytree': 0.3123343694729518, 'scale_pos_weight': 'balanced', 'learning_rate': 0.010217783699569046, 'min_child_weight': 4.8423152204744, 'max_depth': 3065, 'gamma': 0.006810134414375388}, {'colsample_bytree': 0.7872454030889242, 'scale_pos_weight': 1, 'learning_rate': 0.015247244410258017, 'min_child_weight': 3.263595724808784, 'max_depth': 2770, 'gamma': 0.007382751981499336}, {'colsample_bytree': 0.3737053451814131, 'scale_pos_weight': 'balanced', 'learning_rate': 0.014010117018971036, 'min_child_weight': 4.1056429597751425, 'max_depth': 3578, 'gamma': 0.012865429172738327}, {'colsample_bytree': 0.8013955165919462, 'scale_pos_weight': 'balanced', 'learning_rate': 0.01773181303860247, 'min_child_weight': 3.5400159463843295, 'max_depth': 2770, 'gamma': 0.7701328264939121}, {'colsample_bytree': 0.9220287906318078, 'scale_pos_weight': 0.1, 'learning_rate': 0.07790304570539067, 'min_child_weight': 3.7921965508467244, 'max_depth': 3043, 'gamma': 0.08255877279241351}, {'colsample_bytree': 0.8848065043533458, 'scale_pos_weight': 0.1, 'learning_rate': 0.019835216761483664, 'min_child_weight': 3.8942507572352545, 'max_depth': 3853, 'gamma': 0.12544051430643327}, {'colsample_bytree': 0.756579310900177, 'scale_pos_weight': 'balanced', 'learning_rate': 0.02056924409354973, 'min_child_weight': 3.823640277929689, 'max_depth': 3485, 'gamma': 0.0056155051123804745}, {'colsample_bytree': 0.7939407098647304, 'scale_pos_weight': 0.1, 'learning_rate': 0.09038235117410946, 'min_child_weight': 4.385063180155532, 'max_depth': 3134, 'gamma': 0.03191340515246819}, {'colsample_bytree': 0.7141064234435178, 'scale_pos_weight': 1, 'learning_rate': 0.029521076436278288, 'min_child_weight': 3.7897385869004583, 'max_depth': 2673, 'gamma': 0.026582911549201456}, {'colsample_bytree': 0.46755820540783305, 'scale_pos_weight': 0.1, 'learning_rate': 0.026839146502394867, 'min_child_weight': 4.32034707498537, 'max_depth': 2287, 'gamma': 0.0714572480972247}, {'colsample_bytree': 0.3192804170956737, 'scale_pos_weight': 'balanced', 'learning_rate': 0.035630035834116724, 'min_child_weight': 3.2057267173868764, 'max_depth': 3462, 'gamma': 0.011538251106311882}, {'colsample_bytree': 0.7390774525012667, 'scale_pos_weight': 1, 'learning_rate': 0.07880369761217115, 'min_child_weight': 4.3064016397142675, 'max_depth': 3910, 'gamma': 0.019690434802692797}, {'colsample_bytree': 0.9880316810231098, 'scale_pos_weight': 0.1, 'learning_rate': 0.034436778831299704, 'min_child_weight': 3.194519854126348, 'max_depth': 2234, 'gamma': 0.006039803128987408}, {'colsample_bytree': 0.9396119603085051, 'scale_pos_weight': 'balanced', 'learning_rate': 0.09973486555495815, 'min_child_weight': 3.2004537746246022, 'max_depth': 2555, 'gamma': 0.13890692540260635}, {'colsample_bytree': 0.2781695378711052, 'scale_pos_weight': 0.1, 'learning_rate': 0.014684106073312313, 'min_child_weight': 4.809295488419027, 'max_depth': 3661, 'gamma': 0.03349240589089625}, {'colsample_bytree': 0.855489219043758, 'scale_pos_weight': 0.1, 'learning_rate': 0.025537789166811525, 'min_child_weight': 4.696016458644468, 'max_depth': 2821, 'gamma': 0.05096854263999027}, {'colsample_bytree': 0.529968063087441, 'scale_pos_weight': 1, 'learning_rate': 0.011235389040668366, 'min_child_weight': 4.965149919650387, 'max_depth': 3417, 'gamma': 0.01820648507180312}, {'colsample_bytree': 0.8918525067944583, 'scale_pos_weight': 'balanced', 'learning_rate': 0.02290751947618342, 'min_child_weight': 4.951043010005772, 'max_depth': 3795, 'gamma': 0.0010842820109306225}, {'colsample_bytree': 0.5045528645871389, 'scale_pos_weight': 0.1, 'learning_rate': 0.021826551112303395, 'min_child_weight': 4.499998497540201, 'max_depth': 2341, 'gamma': 0.029420280608011204}, {'colsample_bytree': 0.8452732775180655, 'scale_pos_weight': 1, 'learning_rate': 0.022148875435748294, 'min_child_weight': 3.037043588921228, 'max_depth': 2767, 'gamma': 0.004696497161394395}, {'colsample_bytree': 0.6974915547212899, 'scale_pos_weight': 1, 'learning_rate': 0.031624691506044876, 'min_child_weight': 4.2165057317080255, 'max_depth': 3104, 'gamma': 0.22435344153433004}, {'colsample_bytree': 0.42841961603088147, 'scale_pos_weight': 0.1, 'learning_rate': 0.04111174926348787, 'min_child_weight': 4.154457177208335, 'max_depth': 3371, 'gamma': 0.6348074190853715}, {'colsample_bytree': 0.8131331842213236, 'scale_pos_weight': 1, 'learning_rate': 0.035829639215477285, 'min_child_weight': 4.728562885372489, 'max_depth': 2803, 'gamma': 0.7824728966705544}, {'colsample_bytree': 0.38964475441025215, 'scale_pos_weight': 'balanced', 'learning_rate': 0.05489727596641781, 'min_child_weight': 3.4196874979502443, 'max_depth': 2340, 'gamma': 0.6809530508136838}, {'colsample_bytree': 0.7604085183945846, 'scale_pos_weight': 1, 'learning_rate': 0.01138723629459351, 'min_child_weight': 4.940473366192201, 'max_depth': 3346, 'gamma': 0.0018025474156695609}, {'colsample_bytree': 0.7722576166115946, 'scale_pos_weight': 0.1, 'learning_rate': 0.015121806537230269, 'min_child_weight': 3.6235917639882054, 'max_depth': 3681, 'gamma': 0.013591128476936908}, {'colsample_bytree': 0.8078759089823409, 'scale_pos_weight': 0.1, 'learning_rate': 0.04754245170851443, 'min_child_weight': 4.709227160621429, 'max_depth': 2093, 'gamma': 0.027276520980284047}, {'colsample_bytree': 0.9927542105475282, 'scale_pos_weight': 'balanced', 'learning_rate': 0.04603394591192837, 'min_child_weight': 4.793342586080684, 'max_depth': 3564, 'gamma': 0.004473948217802781}, {'colsample_bytree': 0.4219144923149346, 'scale_pos_weight': 1, 'learning_rate': 0.08024629235408161, 'min_child_weight': 3.274440840194389, 'max_depth': 2106, 'gamma': 0.4413226300006937}, {'colsample_bytree': 0.8732863414271428, 'scale_pos_weight': 1, 'learning_rate': 0.07458485597745929, 'min_child_weight': 4.176634227107211, 'max_depth': 2952, 'gamma': 0.07708067883918877}, {'colsample_bytree': 0.9140032447071892, 'scale_pos_weight': 'balanced', 'learning_rate': 0.028584445514662313, 'min_child_weight': 3.850903075904352, 'max_depth': 2455, 'gamma': 0.1095565554516481}, {'colsample_bytree': 0.9605279428666932, 'scale_pos_weight': 0.1, 'learning_rate': 0.017944924794891418, 'min_child_weight': 3.431015354227117, 'max_depth': 2715, 'gamma': 0.15579998815213625}, {'colsample_bytree': 0.6038136781696009, 'scale_pos_weight': 1, 'learning_rate': 0.014246044775631812, 'min_child_weight': 4.914901711317907, 'max_depth': 3300, 'gamma': 0.1363802402537099}, {'colsample_bytree': 0.5976815682736081, 'scale_pos_weight': 'balanced', 'learning_rate': 0.0386175170050117, 'min_child_weight': 3.748339960668451, 'max_depth': 2573, 'gamma': 0.006805908701019693}, {'colsample_bytree': 0.7561355094465191, 'scale_pos_weight': 0.1, 'learning_rate': 0.014959217418964464, 'min_child_weight': 4.494158940278468, 'max_depth': 3028, 'gamma': 0.0067714433959858026}, {'colsample_bytree': 0.6740659838938817, 'scale_pos_weight': 'balanced', 'learning_rate': 0.01395878954008348, 'min_child_weight': 3.7921194056145877, 'max_depth': 3243, 'gamma': 0.003546822939411305}, {'colsample_bytree': 0.7915604968020614, 'scale_pos_weight': 0.1, 'learning_rate': 0.055846278811365536, 'min_child_weight': 3.249666114980172, 'max_depth': 2437, 'gamma': 0.0012341242748117281}, {'colsample_bytree': 0.3125668265815139, 'scale_pos_weight': 'balanced', 'learning_rate': 0.03841280918832586, 'min_child_weight': 4.807439479491867, 'max_depth': 2449, 'gamma': 0.04535001236057697}, {'colsample_bytree': 0.46518628247545346, 'scale_pos_weight': 0.1, 'learning_rate': 0.011562141755779225, 'min_child_weight': 3.4183140589616823, 'max_depth': 3660, 'gamma': 0.10759446143090666}, {'colsample_bytree': 0.7524374099431825, 'scale_pos_weight': 'balanced', 'learning_rate': 0.01913066671695943, 'min_child_weight': 4.859058633584381, 'max_depth': 3862, 'gamma': 0.22670376672491319}, {'colsample_bytree': 0.6342950974719278, 'scale_pos_weight': 'balanced', 'learning_rate': 0.06131153029376064, 'min_child_weight': 3.3632624009840972, 'max_depth': 2534, 'gamma': 0.055124087265135756}, {'colsample_bytree': 0.9711776159061238, 'scale_pos_weight': 'balanced', 'learning_rate': 0.088985413406148, 'min_child_weight': 3.6763179036736915, 'max_depth': 2468, 'gamma': 0.004955698533896726}, {'colsample_bytree': 0.6146196702333983, 'scale_pos_weight': 'balanced', 'learning_rate': 0.03148764826835877, 'min_child_weight': 4.431122314918079, 'max_depth': 2280, 'gamma': 0.1335501714993754}, {'colsample_bytree': 0.7134075189381428, 'scale_pos_weight': 'balanced', 'learning_rate': 0.02224505674516876, 'min_child_weight': 4.697887110625836, 'max_depth': 3812, 'gamma': 0.0010957477139878023}, {'colsample_bytree': 0.9887742091624269, 'scale_pos_weight': 'balanced', 'learning_rate': 0.010093646862083824, 'min_child_weight': 4.560853302208027, 'max_depth': 3845, 'gamma': 0.18226732508585594}, {'colsample_bytree': 0.3526752037641992, 'scale_pos_weight': 1, 'learning_rate': 0.015482743248200726, 'min_child_weight': 3.737169212259235, 'max_depth': 2694, 'gamma': 0.292653147666723}, {'colsample_bytree': 0.9855247555180144, 'scale_pos_weight': 1, 'learning_rate': 0.03708606542169931, 'min_child_weight': 3.2413139823722394, 'max_depth': 3283, 'gamma': 0.016533373579346547}, {'colsample_bytree': 0.9299166244158842, 'scale_pos_weight': 1, 'learning_rate': 0.02153501368231737, 'min_child_weight': 4.921669316126001, 'max_depth': 3967, 'gamma': 0.20996262708780825}, {'colsample_bytree': 0.8314148330999583, 'scale_pos_weight': 'balanced', 'learning_rate': 0.021149996758936338, 'min_child_weight': 3.603661751855033, 'max_depth': 3940, 'gamma': 0.6006739398150247}, {'colsample_bytree': 0.25857059396877324, 'scale_pos_weight': 'balanced', 'learning_rate': 0.01402641374566343, 'min_child_weight': 4.451188728421157, 'max_depth': 3298, 'gamma': 0.20499451718711995}, {'colsample_bytree': 0.7580588721039582, 'scale_pos_weight': 'balanced', 'learning_rate': 0.027048802940230997, 'min_child_weight': 3.5012454403877706, 'max_depth': 3850, 'gamma': 0.26994312660724756}, {'colsample_bytree': 0.8954133803715953, 'scale_pos_weight': 1, 'learning_rate': 0.01863493598675674, 'min_child_weight': 4.114737582647834, 'max_depth': 2837, 'gamma': 0.15175142855971285}, {'colsample_bytree': 0.607357900412373, 'scale_pos_weight': 0.1, 'learning_rate': 0.0287418389947496, 'min_child_weight': 4.1838703163768995, 'max_depth': 2836, 'gamma': 0.02909401825264376}, {'colsample_bytree': 0.7717190842291429, 'scale_pos_weight': 0.1, 'learning_rate': 0.023984294926250517, 'min_child_weight': 4.366562671095361, 'max_depth': 2929, 'gamma': 0.00708853584375251}, {'colsample_bytree': 0.31478273527798095, 'scale_pos_weight': 0.1, 'learning_rate': 0.01609142876748822, 'min_child_weight': 4.018684130736373, 'max_depth': 2213, 'gamma': 0.0955255587537484}, {'colsample_bytree': 0.4445669232590162, 'scale_pos_weight': 0.1, 'learning_rate': 0.03869008066616387, 'min_child_weight': 4.554815123697506, 'max_depth': 2256, 'gamma': 0.013226331813474766}, {'colsample_bytree': 0.39779071013922973, 'scale_pos_weight': 0.1, 'learning_rate': 0.011081850863667208, 'min_child_weight': 3.7417055984357774, 'max_depth': 3500, 'gamma': 0.023964460217819006}, {'colsample_bytree': 0.6021237271959063, 'scale_pos_weight': 0.1, 'learning_rate': 0.030790026305284764, 'min_child_weight': 3.981637225701756, 'max_depth': 3098, 'gamma': 0.4447621983542005}, {'colsample_bytree': 0.7341776833420028, 'scale_pos_weight': 'balanced', 'learning_rate': 0.026940300738592667, 'min_child_weight': 4.9188666816668505, 'max_depth': 2032, 'gamma': 0.0012766954401764138}, {'colsample_bytree': 0.357872024260792, 'scale_pos_weight': 'balanced', 'learning_rate': 0.03461175312888962, 'min_child_weight': 3.3197341641214115, 'max_depth': 3769, 'gamma': 0.08879511813180568}, {'colsample_bytree': 0.9673042920969617, 'scale_pos_weight': 0.1, 'learning_rate': 0.08016484458240573, 'min_child_weight': 3.785351353094189, 'max_depth': 2941, 'gamma': 0.0036424422532291448}, {'colsample_bytree': 0.9329085417950977, 'scale_pos_weight': 1, 'learning_rate': 0.06315074263568436, 'min_child_weight': 3.1845203408138665, 'max_depth': 3522, 'gamma': 0.0017738432633645775}, {'colsample_bytree': 0.9280332946757183, 'scale_pos_weight': 0.1, 'learning_rate': 0.050075126403944986, 'min_child_weight': 3.798050643406204, 'max_depth': 3975, 'gamma': 0.11751006339782041}, {'colsample_bytree': 0.2703017385828926, 'scale_pos_weight': 0.1, 'learning_rate': 0.05847409452429984, 'min_child_weight': 4.432488348436038, 'max_depth': 2998, 'gamma': 0.1563838929285332}, {'colsample_bytree': 0.9693749522764169, 'scale_pos_weight': 0.1, 'learning_rate': 0.03899277693819791, 'min_child_weight': 4.592782949034664, 'max_depth': 3418, 'gamma': 0.023681896666269086}, {'colsample_bytree': 0.3231015165441174, 'scale_pos_weight': 'balanced', 'learning_rate': 0.0729495499045637, 'min_child_weight': 4.115622601765743, 'max_depth': 3551, 'gamma': 0.0732884391164159}, {'colsample_bytree': 0.8616428640764267, 'scale_pos_weight': 0.1, 'learning_rate': 0.042549889743108825, 'min_child_weight': 4.8176874368254765, 'max_depth': 3705, 'gamma': 0.003007761461065643}, {'colsample_bytree': 0.9525598553420915, 'scale_pos_weight': 'balanced', 'learning_rate': 0.053419072489062754, 'min_child_weight': 4.898594946603669, 'max_depth': 2415, 'gamma': 0.021985425933638177}, {'colsample_bytree': 0.9692370413976051, 'scale_pos_weight': 'balanced', 'learning_rate': 0.022735624632840366, 'min_child_weight': 3.066609253093392, 'max_depth': 3760, 'gamma': 0.011644116593520668}, {'colsample_bytree': 0.8840182613651799, 'scale_pos_weight': 'balanced', 'learning_rate': 0.04789279147676467, 'min_child_weight': 4.063698354143385, 'max_depth': 2730, 'gamma': 0.040627830612619716}, {'colsample_bytree': 0.9021163979099742, 'scale_pos_weight': 0.1, 'learning_rate': 0.021217825910940423, 'min_child_weight': 4.890603066958159, 'max_depth': 3999, 'gamma': 0.023040250191004932}, {'colsample_bytree': 0.8796347251147474, 'scale_pos_weight': 0.1, 'learning_rate': 0.032589287319026605, 'min_child_weight': 3.474346587277423, 'max_depth': 3684, 'gamma': 0.0369072423664624}, {'colsample_bytree': 0.4389557411845697, 'scale_pos_weight': 0.1, 'learning_rate': 0.01791394323572313, 'min_child_weight': 3.1361481479494406, 'max_depth': 2869, 'gamma': 0.0046076846258765024}, {'colsample_bytree': 0.6182685556755454, 'scale_pos_weight': 1, 'learning_rate': 0.01756513185631723, 'min_child_weight': 3.752453148544374, 'max_depth': 3531, 'gamma': 0.8808846480321432}, {'colsample_bytree': 0.5567905715297962, 'scale_pos_weight': 'balanced', 'learning_rate': 0.043527302874074116, 'min_child_weight': 4.980690003121788, 'max_depth': 3305, 'gamma': 0.003082217473157844}, {'colsample_bytree': 0.5260768014861505, 'scale_pos_weight': 'balanced', 'learning_rate': 0.0633630610984365, 'min_child_weight': 4.280048310598877, 'max_depth': 3701, 'gamma': 0.008674808319887643}, {'colsample_bytree': 0.7468784286495069, 'scale_pos_weight': 'balanced', 'learning_rate': 0.046329660480708655, 'min_child_weight': 3.482837240153148, 'max_depth': 2950, 'gamma': 0.0054725476912372605}, {'colsample_bytree': 0.623258483901738, 'scale_pos_weight': 0.1, 'learning_rate': 0.06645986447971546, 'min_child_weight': 4.707880235077276, 'max_depth': 3194, 'gamma': 0.019208928344769675}, {'colsample_bytree': 0.5204091703644191, 'scale_pos_weight': 1, 'learning_rate': 0.08413310398376071, 'min_child_weight': 3.8297137386671283, 'max_depth': 2718, 'gamma': 0.3061757170872131}, {'colsample_bytree': 0.5868125707759673, 'scale_pos_weight': 0.1, 'learning_rate': 0.01808145960479789, 'min_child_weight': 4.477365318426892, 'max_depth': 2664, 'gamma': 0.6484150579024105}, {'colsample_bytree': 0.9287112591624451, 'scale_pos_weight': 1, 'learning_rate': 0.09817733597415344, 'min_child_weight': 4.937943409340703, 'max_depth': 2673, 'gamma': 0.007756553550975761}]\n"
     ]
    }
   ],
   "source": [
    "no_trials = 100\n",
    "                              \n",
    "if classifier_method == \"NN\":\n",
    "    lr_list = [[learning_rate_lb, learning_rate_ub],[sec_learning_rate_lb, sec_learning_rate_ub]]\n",
    "    \n",
    "    batch_size_list = [[batch_size_lb, batch_size_ub],[sec_batch_size_lb, sec_batch_size_ub]]\n",
    "    \n",
    "    weight_decay_list = [[weight_decay_lb, weight_decay_ub],[sec_weight_decay_lb, sec_weight_decay_ub]]\n",
    "    \n",
    "    beta_list = [[beta_lb, beta_ub], [sec_beta_lb, sec_beta_ub]]\n",
    "    \n",
    "    hidden_units_1_list = [[hidden_units_1_lb, hidden_units_1_ub],[sec_hidden_units_1_lb, sec_hidden_units_1_ub]]\n",
    "    \n",
    "    hidden_units_2_list = [[hidden_units_2_lb, hidden_units_2_ub],[sec_hidden_units_2_lb, sec_hidden_units_2_ub]]\n",
    "                              \n",
    "    hyperparameter_trials = generate_trials_NN(no_trials, lr_list, batch_size_list,weight_decay_list, beta_list, \n",
    "                                               hidden_units_1_list, hidden_units_2_list)\n",
    "elif classifier_method == \"XGB\":\n",
    "    max_depth_list = [[max_depth_lb, max_depth_ub], [sec_max_depth_lb, sec_max_depth_ub]]\n",
    "    \n",
    "    min_child_weight_list = [[min_child_weight_lb, min_child_weight_ub],[sec_min_child_weight_lb, sec_min_child_weight_ub]]\n",
    "    \n",
    "    colsample_bytree_list = [[colsample_bytree_lb, colsample_bytree_ub], [sec_colsample_bytree_lb, sec_colsample_bytree_ub]]\n",
    "    \n",
    "    gamma_list = [[gamma_lb, gamma_ub],[sec_gamma_lb, sec_gamma_ub]] \n",
    "    \n",
    "    lr_list = [[learning_rate_lb, learning_rate_ub],[sec_learning_rate_lb, sec_learning_rate_ub]]\n",
    "\n",
    "    hyperparameter_trials = generate_trials_XGB(no_trials, max_depth_list, min_child_weight_list, \n",
    "                                                colsample_bytree_list, gamma_list, lr_list)\n",
    "                                      \n",
    "elif classifier_method == \"RF\":\n",
    "    \n",
    "    n_estimators_list = [[n_estimators_lb, n_estimators_ub],[sec_n_estimators_lb, sec_n_estimators_ub]]\n",
    "\n",
    "    max_depth_list = [[max_depth_lb, max_depth_ub],[sec_max_depth_lb, sec_max_depth_ub]]\n",
    "    \n",
    "    min_samples_leaf_list = [[min_samples_leaf_lb, min_samples_leaf_ub], [sec_min_samples_leaf_lb, sec_min_samples_leaf_ub]]\n",
    " \n",
    "    min_samples_split_list = [[min_samples_split_lb, min_samples_split_ub],\n",
    "                              [sec_min_samples_split_lb, sec_min_samples_split_ub]]\n",
    "       \n",
    "    hyperparameter_trials = generate_trials_RF(no_trials, n_estimators_list,\n",
    "                                              max_depth_list, min_samples_leaf_list, \n",
    "                                              min_samples_split_list)\n",
    "elif classifier_method == \"Logistic\":\n",
    "    C_list = [[C_lb, C_ub], [sec_C_lb, sec_C_ub]]\n",
    "    \n",
    "    hyperparameter_trials = generate_trials_Log(no_trials, C_list)\n",
    "    \n",
    "elif classifier_method == \"KNN\":\n",
    "    n_neighbors_list = [[n_neighbors_lb, n_neighbors_ub], [sec_n_neighbors_lb, sec_n_neighbors_ub]]\n",
    "    \n",
    "    hyperparameter_trials = generate_trials_KNN(no_trials, n_neighbors_list)\n",
    "    \n",
    "elif classifier_method == \"ADA\":\n",
    "    n_estimators_list = [[n_estimators_lb, n_estimators_ub], [sec_n_estimators_lb, sec_n_estimators_ub]]\n",
    "    \n",
    "    lr_list = [[learning_rate_lb, learning_rate_ub], [sec_learning_rate_lb, sec_learning_rate_ub]]\n",
    "    \n",
    "    hyperparameter_trials = generate_trials_ADA(no_trials, n_estimators_list, lr_list)\n",
    "    \n",
    "elif classifier_method == \"SVM\":\n",
    "    C_list = [[C_lb, C_ub], [sec_C_lb, sec_C_ub]]\n",
    "                                      \n",
    "    gamma_list = [[gamma_lb, gamma_ub], [sec_gamma_lb, sec_gamma_ub]]\n",
    "                                          \n",
    "    hyperparameter_trials = generate_trials_SVM(no_trials, C_list, gamma_list)\n",
    "\n",
    "print hyperparameter_trials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'colsample_bytree': 0.883199311435763, 'scale_pos_weight': 'balanced', 'learning_rate': 0.07034799738710229, 'min_child_weight': 4.185689236450036, 'max_depth': 2684, 'gamma': 0.37483216628479255}\n"
     ]
    }
   ],
   "source": [
    "# Generate trial for model\n",
    "hyperparameters = hyperparameter_trials[trial_idx]\n",
    "print hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Define the Network\n",
    "\n",
    "Tutorial for Neural Net Architecture: https://pytorch.org/tutorials/beginner/blitz/neural_networks_tutorial.html\n",
    "\n",
    "Utilize batch normalization, as explained here: https://www.youtube.com/watch?v=fv1Luwd-LOI&index=69&list=PLBAGcD3siRDguyYYzhVwZ3tLvOyyG5k6K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define the network with batch normalization\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, hyperparameters):\n",
    "        hidden_units_1 = hyperparameters[\"hidden_units_1\"]\n",
    "        hidden_units_2 = hyperparameters[\"hidden_units_2\"]\n",
    "        super(Net, self).__init__()\n",
    "        self.input = nn.Linear(len(features_cols), hidden_units_1) # read input size from the .shape of data table\n",
    "        self.hidden1 = nn.Linear(hidden_units_1, hidden_units_2)\n",
    "        self.hidden1_bn = nn.BatchNorm1d(hidden_units_2)\n",
    "        self.hidden2 = nn.Linear(hidden_units_2, hidden_units_2)\n",
    "        self.hidden2_bn = nn.BatchNorm1d(hidden_units_2)\n",
    "        self.hidden3 = nn.Linear(hidden_units_2, hidden_units_1)\n",
    "        self.hidden3_bn = nn.BatchNorm1d(hidden_units_1)\n",
    "        self.output = nn.Linear(hidden_units_1,2)\n",
    "        self.batch_size = hyperparameters[\"batch_size\"]\n",
    "        self.learning_rate = hyperparameters[\"learning_rate\"]\n",
    "        self.beta = hyperparameters[\"beta\"]\n",
    "        self.weight_decay = hyperparameters[\"weight_decay\"]\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.input(x))\n",
    "        x = F.relu(self.hidden1_bn(self.hidden1(x)))\n",
    "        x = F.relu(self.hidden2_bn(self.hidden2(x)))\n",
    "        x = F.relu(self.hidden3_bn(self.hidden3(x)))\n",
    "        x = self.output(x)\n",
    "        return x\n",
    "    \n",
    "    def fit(self, X_train, y_train_label, X_valid, y_valid, weight):\n",
    "\n",
    "        # set random seed for weights and biases\n",
    "        torch.manual_seed(0)\n",
    "\n",
    "        # dataset\n",
    "        dataset = pd.concat([X_train,y_train_label],axis=1)\n",
    "        dataset = shuffle(dataset, random_state = 0)\n",
    "\n",
    "        X_train = dataset.iloc[:,:dataset.shape[1]-1]\n",
    "        y_train_label = dataset.iloc[:,dataset.shape[1]-1]\n",
    "\n",
    "\n",
    "        # create loss function\n",
    "        loss = nn.CrossEntropyLoss(weight = weight)\n",
    "        # mini-batching\n",
    "        batch_size = self.batch_size\n",
    "\n",
    "        # create adam optimizer for Phase 1\n",
    "        optimizer_1 = optim.Adam(self.parameters(), lr=self.learning_rate,betas=(self.beta,0.999), \n",
    "                                 weight_decay = self.weight_decay)\n",
    "        no_batch_minus_1 = X_train.shape[0] / batch_size \n",
    "\n",
    "        # Repeated Stratified K Fold to ensure positives are evenly distributed across batches\n",
    "        skf_1 = RepeatedStratifiedKFold(n_splits=no_batch_minus_1,n_repeats=10**4,random_state=0)\n",
    "        \n",
    "        count = 0\n",
    "        epoch_count = 0\n",
    "        max_auprc = 0\n",
    "        ideal_epoch_count = 0 \n",
    "        patience = 100\n",
    "        patience_j = 0\n",
    "\n",
    "        for train,test in skf_1.split(X_train,y_train_label):\n",
    "            data = X_train.iloc[test,:]\n",
    "            data = torch.Tensor(data.values.astype(np.float32))\n",
    "             # forward pass\n",
    "            output = self.forward(data)\n",
    "            output.data = output.data.view(data.shape[0],2)\n",
    "\n",
    "            labels = y_train_label[test]\n",
    "            labels = torch.Tensor(labels.astype(np.float32))\n",
    "            labels = torch.autograd.Variable(labels).long()\n",
    "\n",
    "            # zero the gradient buffers\n",
    "            optimizer_1.zero_grad()\n",
    "            # compute loss and gradients\n",
    "            loss_output = loss(output,labels)\n",
    "            loss_output.backward()\n",
    "            # Does the update\n",
    "            optimizer_1.step()\n",
    "\n",
    "            count = count + 1\n",
    "            \n",
    "            # Early Stopping\n",
    "            if count == no_batch_minus_1 + 1:\n",
    "                count = 0\n",
    "                epoch_count = epoch_count + 1\n",
    "                probs = self.predict_proba(X_valid)\n",
    "                precision, recall, _ = precision_recall_curve(y_valid, probs)\n",
    "                auprc = auc(recall, precision)\n",
    "                if auprc > max_auprc:\n",
    "                    max_auprc = auprc\n",
    "                    ideal_epoch_count = epoch_count\n",
    "                    patience = patience + epoch_count\n",
    "                    patience_j = 0\n",
    "                else:\n",
    "                    patience_j = patience_j + 1 \n",
    "                    if patience_j == patience: break\n",
    "\n",
    "                self.train()\n",
    "        return max_auprc, ideal_epoch_count\n",
    "\n",
    "        \n",
    "    #prediction probabilities array\n",
    "    def predict_proba(self, X_test):\n",
    "        self.eval()\n",
    "        #forward pass\n",
    "        test = torch.Tensor(X_test.values.astype(np.float32))\n",
    "        output = self.forward(test)\n",
    "        sf = nn.Softmax()\n",
    "        probs = sf(output.data)\n",
    "        return probs[:,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Models tested (and their hyper-parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_model(classifier_method, hyperparameters, no_pos=1, no_neg=1):\n",
    "    \n",
    "    xgb_trees_limit = 5000\n",
    "    \n",
    "    if (classifier_method == \"XGB\"):\n",
    "        if (hyperparameters[\"scale_pos_weight\"] == \"balanced\"):\n",
    "            scale_weight = no_neg/float(no_pos)\n",
    "        else:\n",
    "            scale_weight = hyperparameters[\"scale_pos_weight\"]\n",
    "        model = XGBClassifier(n_estimators=xgb_trees_limit, n_jobs=-1, random_state=0, max_depth=hyperparameters[\"max_depth\"], \n",
    "                              min_child_weight=hyperparameters[\"min_child_weight\"], colsample_bytree=hyperparameters[\"colsample_bytree\"], \n",
    "                              gamma=hyperparameters[\"gamma\"], learning_rate=hyperparameters[\"learning_rate\"], scale_pos_weight=scale_weight)\n",
    "        \n",
    "    elif (classifier_method == \"RF\"):\n",
    "        model = RandomForestClassifier(n_estimators=hyperparameters[\"n_estimators\"], n_jobs=-1, random_state=0,\n",
    "                                      max_depth=hyperparameters[\"max_depth\"], min_samples_leaf=hyperparameters[\"min_samples_leaf\"],\n",
    "                                      min_samples_split=hyperparameters[\"min_samples_split\"], class_weight=hyperparameters[\"class_weight\"])\n",
    "        \n",
    "    elif(classifier_method == \"Logistic\"):\n",
    "        model = LogisticRegression(C=hyperparameters[\"C\"], random_state=0, n_jobs=-1, class_weight=hyperparameters[\"class_weight\"])\n",
    "        \n",
    "    elif (classifier_method == \"KNN\"):\n",
    "        model = KNeighborsClassifier(n_neighbors=hyperparameters[\"n_neighbors\"], n_jobs=-1, weights=hyperparameters[\"weights\"])\n",
    "        \n",
    "    elif (classifier_method == \"ADA\"):\n",
    "        model = AdaBoostClassifier(n_estimators=hyperparameters[\"n_estimators\"], random_state=0, learning_rate=hyperparameters[\"learning_rate\"])\n",
    "        \n",
    "    elif (classifier_method == \"SVM\"):\n",
    "        model = SVC(C=hyperparameters[\"C\"], gamma = hyperparameters[\"gamma\"], kernel=hyperparameters[\"kernel\"], probability=True, random_state=0, cache_size=400,\n",
    "                    class_weight = hyperparameters[\"class_weight\"])\n",
    "        \n",
    "    elif (classifier_method ==\"NN\"):\n",
    "        torch.manual_seed(0)\n",
    "        model = Net(hyperparameters)\n",
    "        # sets model in training mode because batch normalization behavior in training and testing modes are different\n",
    "        model.train()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dealing with model imbalance\n",
    "Weight Vector: https://towardsdatascience.com/dealing-with-imbalanced-classes-in-machine-learning-d43d6fa19d2 (look at section on \"Cost-sensitive Learning\")\n",
    "\n",
    "Implementing Early Stopping for XGBoost: https://cambridgespark.com/content/tutorials/hyperparameter-tuning-in-xgboost/index.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test_model_iterative_fixed(hyperparameters_dict,ligand_bind_features, ligand_negatives_features, ligand_name, features=[]):\n",
    "    \n",
    "    \"\"\"\n",
    "    Test different models in k-folds cross-validation.\n",
    "    \"\"\"\n",
    "    \n",
    "    #Default: Exclude no features\n",
    "    if len(features) == 0:\n",
    "        features = np.ones([ligand_bind_features.shape[1],]).astype(bool)\n",
    "    \n",
    "    models_req_scaling = [\"SVM\", \"KNN\", \"Logistic\", \"NN\"]\n",
    "    classifier = classifier_method\n",
    "\n",
    "    #Create X and y with included features\n",
    "    X = pd.concat([ligand_bind_features.iloc[:,features], ligand_negatives_features.iloc[:,features]])\n",
    "    y = [1] * ligand_bind_features.shape[0]\n",
    "    y.extend([0] * ligand_negatives_features.shape[0])\n",
    "    y = np.array(y)\n",
    "    y_df = pd.DataFrame(y)\n",
    "    y_df.index = X.index\n",
    "    y_df.columns = [\"label\"]\n",
    "    \n",
    "    #Get the fold indices\n",
    "    cv_idx = calc_CV_idx_iterative(X, splits_dict)\n",
    "    k = (int(fold)-1)\n",
    "    \n",
    "    pred_idx = k+1\n",
    "    print \"fold #: \"+str(pred_idx)\n",
    "    #test_index = cv_idx[k][\"test\"]\n",
    "    full_train_index = cv_idx[k][\"train\"]\n",
    "        \n",
    "    # phase 1: testing on validation set, hyperparameter tuning\n",
    "    \n",
    "    trial_results = np.zeros(folds_num-1)\n",
    "    epoch_counts = np.zeros(folds_num-1, dtype = \"int\")\n",
    "    for i in range(folds_num-1):\n",
    "    #for i in range(1):\n",
    "        valid_k = (k + 1 + i) % folds_num\n",
    "        valid_index = cv_idx[valid_k][\"test\"]\n",
    "\n",
    "        train_index = [index for index in full_train_index if index not in valid_index]\n",
    "        X_train, X_valid = X.loc[train_index,:], X.loc[valid_index,:]\n",
    "        y_train, y_valid = y_df.loc[train_index,:], y_df.loc[valid_index,:]\n",
    "\n",
    "        if (classifier in models_req_scaling):\n",
    "            cols = X_train.columns\n",
    "\n",
    "            # phase 1 scaling with just training data\n",
    "            scaler_1 = StandardScaler() \n",
    "            scaler_1.fit(X_train) \n",
    "            X_train = pd.DataFrame(scaler_1.transform(X_train))\n",
    "            # apply same transformation to validation data\n",
    "            X_valid = pd.DataFrame(scaler_1.transform(X_valid))\n",
    "\n",
    "            #Restoring indices after scaling\n",
    "            X_train.index = train_index \n",
    "            X_valid.index = valid_index\n",
    "\n",
    "            #Restoring features names\n",
    "            X_train.columns = cols\n",
    "            X_valid.columns = cols\n",
    "\n",
    "        #No down-sampling\n",
    "        X_train_sampled = X_train\n",
    "        y_train_sampled = y_train\n",
    "        \n",
    "        #pos and neg numbers in the training\n",
    "        no_pos = np.count_nonzero(y_train_sampled[\"label\"] == 1)\n",
    "        no_neg = np.count_nonzero(y_train_sampled[\"label\"] == 0)  \n",
    "        \n",
    "        #fit to training data\n",
    "        if (classifier == \"NN\"):\n",
    "            if hyperparameters[\"weight\"] == \"balanced\":              \n",
    "                #weight vector\n",
    "                neg_weight = float(no_pos) / float(no_neg + no_pos) \n",
    "                pos_weight = 1 - neg_weight\n",
    "            elif hyperparameters[\"weight\"] == \"0.1\":\n",
    "                neg_weight = 10\n",
    "                pos_weight = 1\n",
    "            elif hyperparameters[\"weight\"] == \"None\":\n",
    "                neg_weight = 1\n",
    "                pos_weight = 1\n",
    "            \n",
    "            weight = torch.Tensor([neg_weight, pos_weight])\n",
    "            model = generate_model(classifier_method, hyperparameters)\n",
    "            auprc_score,epoch_count = model.fit(X_train_sampled, y_train_sampled[\"label\"],X_valid, y_valid[\"label\"], weight)\n",
    "\n",
    "        elif (classifier == \"XGB\"):\n",
    "            num_early_stopping_rounds = 500\n",
    "            model = generate_model(classifier_method, hyperparameters, no_pos = no_pos, no_neg = no_neg)\n",
    "            model.fit(X_train_sampled, y_train_sampled[\"label\"], eval_set = [(X_valid,y_valid[\"label\"])], eval_metric = \"map\", \n",
    "                      verbose=False, early_stopping_rounds = num_early_stopping_rounds)\n",
    "            probs_list = []\n",
    "            probs = model.predict_proba(X_valid, ntree_limit=model.best_ntree_limit)\n",
    "            for l in probs:\n",
    "                probs_list.append(l[1])\n",
    "            precision, recall, _ = precision_recall_curve(y_valid, probs_list)\n",
    "            auprc_score = auc(recall, precision)\n",
    "            epoch_count = model.best_iteration + 1\n",
    "            \n",
    "\n",
    "        else:            \n",
    "            model = generate_model(classifier_method, hyperparameters)\n",
    "            model.fit(X_train_sampled, y_train_sampled[\"label\"])\n",
    "            probs_list = []\n",
    "            probs = model.predict_proba(X_valid)\n",
    "            for l in probs:\n",
    "                probs_list.append(l[1])\n",
    "            precision, recall, _ = precision_recall_curve(y_valid, probs_list)\n",
    "            auprc_score = auc(recall, precision)\n",
    "        \n",
    "        trial_results[i] = auprc_score\n",
    "        print \"AUPRC: \" + str(auprc_score)\n",
    "        if classifier == \"NN\" or classifier == \"XGB\": epoch_counts[i] = epoch_count\n",
    "\n",
    "    mean_result = np.mean(trial_results)\n",
    "    if classifier == \"NN\" or classifier == \"XGB\":\n",
    "        mean_epoch_count = int(np.mean(epoch_counts))\n",
    "        hyperparameters_dict[\"mean_epoch_count\"] = mean_epoch_count\n",
    "\n",
    "    hyperparameters_dict[\"mean_AUPRC\"] = mean_result\n",
    "\n",
    "    # Update dictionary with all hyperparameters\n",
    "    keys = hyperparameters.keys()\n",
    "    for key in keys:\n",
    "        hyperparameters_dict[key].append(hyperparameters[key])\n",
    "    pred_idx += 1\n",
    "\n",
    "    print \"Finished \"+ligand+\" \"+classifier+\" fold: \"+fold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predict for each ligand seperatelly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "#Initialize dictionary\n",
    "hyperparameters_dict = defaultdict(list)\n",
    "\n",
    "test_model_iterative_fixed(hyperparameters_dict,ligands_positives_df[ligand], ligands_negatives_df[ligand], ligand)\n",
    "\n",
    "hyperparameters_df = pd.DataFrame.from_dict(hyperparameters_dict)\n",
    "\n",
    "#Save to file\n",
    "hyperparameters_df.to_csv(curr_dir[0]+\"/hyperparam_tuning/phase1_initial_run/\"+datafile_date+\"_\"+prec_th_str+\"/per_trial/\"+ligand+\"_\"+classifier_method+\"_fold\"+fold+\"_trial\"+str(trial_idx)+\"_\"+str(folds_num)+\"w_hyperparameters.csv\", sep='\\t')\n",
    "\n",
    "print \"Finished ligand \"+ligand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
