{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Basic imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from collections import defaultdict\n",
    "from os import environ, getcwd\n",
    "import sys\n",
    "\n",
    "#Classifier imports\n",
    "from sklearn.neighbors import KNeighborsClassifier \n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "#ML framework imports\n",
    "from sklearn.metrics import auc, roc_auc_score, precision_recall_curve, precision_score\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.preprocessing import scale\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "# Neural Net imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "#Import utils functions\n",
    "curr_dir = !pwd\n",
    "\n",
    "sys.path.append(curr_dir[0]+\"/utils\")\n",
    "from prop_threshold_funcs import create_negatives_datasets_combined, create_positives_datasets_combined\n",
    "from prediction_general_funcs import ligands, score_cols_suffix, get_features_cols, remove_unimportant_features\n",
    "from CV_funcs import add_domain_name_from_table_idx, calc_CV_idx_iterative\n",
    "from generate_hyperparameter_trials import *\n",
    "\n",
    "from IPython.core.display import HTML\n",
    "HTML(\"<style>.container { width:100% !important; }</style>\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading the input dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all samples positions #: 42535\n"
     ]
    }
   ],
   "source": [
    "curr_dir = !pwd\n",
    "pfam_version = \"31\"\n",
    "datafile_date = \"06.20.18\"\n",
    "input_path = curr_dir[0]+\"/domains_similarity/filtered_features_table/\"\n",
    "filename = \"windowed_positions_features_mediode_filter_\"+datafile_date+\".csv\"\n",
    "out_dir = \"mediode_NegLigand_NoFilter\"\n",
    "\n",
    "#flags for creating negatives\n",
    "zero_prop = True\n",
    "no_prop = True\n",
    "all_ligands = False\n",
    "prec_th = 0.25\n",
    "folds_num = 5\n",
    "\n",
    "#Features table\n",
    "features_all = pd.read_csv(input_path+filename, sep='\\t', index_col=0)\n",
    "#Features columns names, without the labels (the binding scores)\n",
    "features_cols = get_features_cols(features_all)\n",
    "remove_unimportant_features(features_all, features_cols)\n",
    "\n",
    "print \"all samples positions #: \"+str(features_all.shape[0])\n",
    "\n",
    "#CV splits dictionary\n",
    "with open(curr_dir[0]+\"/CV_splits/pfam-v\"+pfam_version+\"/domain_\"+str(folds_num)+\"_folds_\"+str(prec_th)+\"_prec_dict.pik\", 'rb') as handle:\n",
    "        splits_dict = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataset of negative examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dna non-binding #:41680\n",
      "dnabase non-binding #:42089\n",
      "dnabackbone non-binding #:41689\n",
      "dna combined non binding #: 41555\n",
      "rna non-binding #:41613\n",
      "rnabase non-binding #:41828\n",
      "rnabackbone non-binding #:41619\n",
      "rna combined non binding #: 41401\n",
      "peptide non-binding #:38794\n",
      "ion non-binding #:37525\n",
      "metabolite non-binding #:37463\n",
      "sm non-binding #:30978\n"
     ]
    }
   ],
   "source": [
    "ligands_negatives_df = create_negatives_datasets_combined(zero_prop, no_prop, features_all, features_cols, all_ligands)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Datasets of positive examples by ligand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dna #: 239\n",
      "dnabase #: 170\n",
      "dnabackbone #: 244\n",
      "dna combined #: 353\n",
      "rna #: 360\n",
      "rnabase #: 246\n",
      "rnabackbone #: 346\n",
      "rna combined #: 468\n",
      "peptide #: 462\n",
      "ion #: 350\n",
      "metabolite #: 504\n",
      "sm #: 708\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/madhumithashridharan/anaconda/lib/python2.7/site-packages/pandas/core/frame.py:1997: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  \"DataFrame index.\", UserWarning)\n"
     ]
    }
   ],
   "source": [
    "ligands_positives_df = create_positives_datasets_combined(features_all, features_cols, all_ligands)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading env input for downsampler technique, ligand and classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ligand = sm\n",
      "fold = 3\n",
      "classifier_method = SVM\n"
     ]
    }
   ],
   "source": [
    "#Reading the ligand input\n",
    "try:\n",
    "    ligand = environ['ligand']\n",
    "except:\n",
    "    ligand = \"sm\"\n",
    "print \"ligand = \"+ligand\n",
    "    \n",
    "#Reading the downsampler input\n",
    "try: \n",
    "    fold = environ['fold']\n",
    "except:\n",
    "    fold = \"3\"\n",
    "print \"fold = \"+fold\n",
    "\n",
    "#Reading the classifier input\n",
    "try: \n",
    "    classifier_method = environ['classifier']\n",
    "except:\n",
    "    classifier_method = \"SVM\"\n",
    "print \"classifier_method = \"+classifier_method\n",
    "\n",
    "if classifier_method == \"NN\":\n",
    "    try:\n",
    "        learning_rate_ub = int(environ['learning_rate_ub'])\n",
    "        learning_rate_lb = int(environ['learning_rate_lb'])\n",
    "        batch_size_ub = int(environ['batch_size_ub'])\n",
    "        batch_size_lb = int(environ['batch_size_lb'])\n",
    "        weight_decay_ub = int(environ['weight_decay_ub'])\n",
    "        weight_decay_lb = int(environ['weight_decay_lb'])\n",
    "        beta_ub = float(environ['beta_ub'])\n",
    "        beta_lb = float(environ['beta_lb'])\n",
    "        hidden_units_1_ub = int(environ['hidden_units_1_ub'])\n",
    "        hidden_units_1_lb = int(environ['hidden_units_1_lb'])\n",
    "        hidden_units_2_ub = int(environ['hidden_units_2_ub'])\n",
    "        hidden_units_2_lb = int(environ['hidden_units_2_lb'])\n",
    "\n",
    "    except:\n",
    "        learning_rate_ub = -2\n",
    "        learning_rate_lb = -3\n",
    "        batch_size_ub = 150\n",
    "        batch_size_lb = 30\n",
    "        weight_decay_ub = -7\n",
    "        weight_decay_lb = -17\n",
    "        beta_ub = 0.95\n",
    "        beta_lb = 0.85\n",
    "        hidden_units_1_ub = 300\n",
    "        hidden_units_1_lb = 50\n",
    "        hidden_units_2_ub = 800\n",
    "        hidden_units_2_lb = 350\n",
    "    \n",
    "\n",
    "if classifier_method == \"XGB\":\n",
    "    try:\n",
    "        n_estimators_ub =int(environ[\"xgb_n_estimators_ub\"])\n",
    "        n_estimators_lb =int(environ[\"xgb_n_estimators_lb\"])\n",
    "        max_depth_ub = int(environ[\"max_depth_ub\"])\n",
    "        max_depth_lb = int(environ[\"max_depth_lb\"])\n",
    "        min_child_weight_ub = int(environ[\"min_child_weight_ub\"])\n",
    "        min_child_weight_lb = int(environ[\"min_child_weight_lb\"])\n",
    "        colsample_bytree_ub = float(environ[\"colsample_bytree_ub\"])\n",
    "        colsample_bytree_lb = float(environ[\"colsample_bytree_lb\"])\n",
    "    \n",
    "    except:\n",
    "        n_estimators_ub = 3\n",
    "        n_estimators_lb =1\n",
    "        max_depth_ub = 5\n",
    "        max_depth_lb = 2\n",
    "        min_child_weight_ub = 6\n",
    "        min_child_weight_lb = 2\n",
    "        colsample_bytree_ub = 0.6\n",
    "        colsample_bytree_lb = 0.5\n",
    "\n",
    "        \n",
    "if classifier_method == \"RF\":\n",
    "    try:\n",
    "        n_estimators_ub = int(environ[\"rf_n_estimators_ub\"])\n",
    "        n_estimators_lb = int(environ[\"rf_n_estimators_lb\"])\n",
    "    except:\n",
    "        n_estimators_ub = 5\n",
    "        n_estimators_lb = 2\n",
    "\n",
    "if classifier_method == \"Logistic\":\n",
    "    try:\n",
    "        C_ub = int(environ[\"log_C_ub\"])\n",
    "        C_lb = int(environ[\"log_C_lb\"])\n",
    "    except:\n",
    "        C_ub = 3\n",
    "        C_lb = 1\n",
    "\n",
    "if classifier_method == \"KNN\":\n",
    "    try:\n",
    "        n_neighbors_ub = int(environ[\"n_neighbors_ub\"])\n",
    "        n_neighbors_lb = int(environ[\"n_neighbors_lb\"])\n",
    "\n",
    "    except:\n",
    "        n_neighbors_ub = 100\n",
    "        n_neighbors_lb = 5\n",
    "        \n",
    "if classifier_method == \"ADA\":\n",
    "    try:\n",
    "        n_estimators_ub = int(environ[\"ada_n_estimators_ub\"])\n",
    "        n_estimators_lb = int(environ[\"ada_n_estimators_lb\"])\n",
    "    except:\n",
    "        n_estimators_ub = 6\n",
    "        n_estimators_lb = 3\n",
    "        \n",
    "if classifier_method == \"SVM\":\n",
    "    try:\n",
    "        C_ub = int(environ[\"sv_C_ub\"])\n",
    "        C_lb = int(environ[\"sv_C_lb\"])\n",
    "        gamma_ub = int(environ[\"gamma_ub\"])\n",
    "        gamma_lb = int(environ[\"gamma_lb\"])\n",
    "    except:\n",
    "        C_ub = 4\n",
    "        C_lb = 2\n",
    "        gamma_ub = -4\n",
    "        gamma_lb = -6\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate hyperparameter trials\n",
    "\n",
    "Choose hyperparameters and generate hyperparameters through random search in a grid, as explained by this video: https://www.youtube.com/watch?v=WrICwRrvuIc&index=66&list=PLBAGcD3siRDguyYYzhVwZ3tLvOyyG5k6K\n",
    "\n",
    "Use logarithmic scale for search for learnining rate and weight decay for NN, as explained by this video: https://www.youtube.com/watch?v=VUbrW8OK3uo&index=67&list=PLBAGcD3siRDguyYYzhVwZ3tLvOyyG5k6K\n",
    "\n",
    "Utilize nested cross validation to choose between models, as described here: https://stats.stackexchange.com/questions/266225/step-by-step-explanation-of-k-fold-cross-validation-with-grid-search-to-optimise/266229"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'C': 1252.0653814999462, 'gamma': 2.693883019285411e-05}]\n"
     ]
    }
   ],
   "source": [
    "no_trials = 1\n",
    "\n",
    "if classifier_method == \"NN\":\n",
    "    hyperparameter_trials = generate_trials_NN(no_trials, learning_rate_ub, learning_rate_lb, batch_size_ub, batch_size_lb,weight_decay_ub, \n",
    "                                               weight_decay_lb, beta_ub, beta_lb, hidden_units_1_ub, hidden_units_1_lb, hidden_units_2_ub, \n",
    "                                               hidden_units_2_lb)\n",
    "if classifier_method == \"XGB\":\n",
    "    hyperparameter_trials = generate_trials_XGB(no_trials, n_estimators_ub, n_estimators_lb,\n",
    "                                                max_depth_ub, max_depth_lb, min_child_weight_ub,\n",
    "                                                min_child_weight_lb, colsample_bytree_ub, colsample_bytree_lb)\n",
    "if classifier_method == \"RF\":\n",
    "    hyperparameter_trials = generate_trials_RF(no_trials, n_estimators_ub, n_estimators_lb)\n",
    "if classifier_method == \"Logistic\":\n",
    "    hyperparameter_trials = generate_trials_Log(no_trials, C_ub, C_lb)\n",
    "if classifier_method == \"KNN\":\n",
    "    hyperparameter_trials = generate_trials_KNN(no_trials, n_neighbors_ub, n_neighbors_lb)\n",
    "if classifier_method == \"ADA\":\n",
    "    hyperparameter_trials = generate_trials_ADA(no_trials, n_estimators_ub, n_estimators_lb)\n",
    "if classifier_method == \"SVM\":\n",
    "    hyperparameter_trials = generate_trials_SVM(no_trials, C_ub, C_lb, gamma_ub, gamma_lb)\n",
    "    \n",
    "\n",
    "print hyperparameter_trials\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the Network\n",
    "\n",
    "Tutorial for Neural Net Architecture: https://pytorch.org/tutorials/beginner/blitz/neural_networks_tutorial.html\n",
    "\n",
    "Utilize batch normalization, as explained here: https://www.youtube.com/watch?v=fv1Luwd-LOI&index=69&list=PLBAGcD3siRDguyYYzhVwZ3tLvOyyG5k6K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define the network with batch normalization\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, hyperparameters):\n",
    "        hidden_units_1 = hyperparameters[\"hidden_units_1\"]\n",
    "        hidden_units_2 = hyperparameters[\"hidden_units_2\"]\n",
    "        super(Net, self).__init__()\n",
    "        self.input = nn.Linear(len(features_cols), hidden_units_1) # read input size from the .shape of data table\n",
    "        self.hidden1 = nn.Linear(hidden_units_1, hidden_units_2)\n",
    "        self.hidden1_bn = nn.BatchNorm1d(hidden_units_2)\n",
    "        self.hidden2 = nn.Linear(hidden_units_2, hidden_units_2)\n",
    "        self.hidden2_bn = nn.BatchNorm1d(hidden_units_2)\n",
    "        self.hidden3 = nn.Linear(hidden_units_2, hidden_units_1)\n",
    "        self.hidden3_bn = nn.BatchNorm1d(hidden_units_1)\n",
    "        self.output = nn.Linear(hidden_units_1,2)\n",
    "        self.batch_size = hyperparameters[\"batch_size\"]\n",
    "        self.learning_rate = hyperparameters[\"learning_rate\"]\n",
    "        self.beta = hyperparameters[\"beta\"]\n",
    "        self.weight_decay = hyperparameters[\"weight_decay\"]\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.input(x))\n",
    "        x = F.relu(self.hidden1_bn(self.hidden1(x)))\n",
    "        x = F.relu(self.hidden2_bn(self.hidden2(x)))\n",
    "        x = F.relu(self.hidden3_bn(self.hidden3(x)))\n",
    "        x = self.output(x)\n",
    "        return x\n",
    "    \n",
    "    def fit(self, X_train, y_train_label, X_valid, y_valid, weight):\n",
    "\n",
    "        # set random seed for weights and biases\n",
    "        torch.manual_seed(0)\n",
    "\n",
    "        # dataset\n",
    "        dataset = pd.concat([X_train,y_train_label],axis=1)\n",
    "        dataset = shuffle(dataset, random_state = 0)\n",
    "\n",
    "        X_train = dataset.iloc[:,:dataset.shape[1]-1]\n",
    "        y_train_label = dataset.iloc[:,dataset.shape[1]-1]\n",
    "\n",
    "\n",
    "        # create loss function\n",
    "        loss = nn.CrossEntropyLoss(weight = weight)\n",
    "        # mini-batching\n",
    "        batch_size = self.batch_size\n",
    "\n",
    "        # create adam optimizer for Phase 1\n",
    "        optimizer_1 = optim.Adam(self.parameters(), lr=self.learning_rate,betas=(self.beta,0.999), \n",
    "                                 weight_decay = self.weight_decay)\n",
    "        no_batch_minus_1 = X_train.shape[0] / batch_size \n",
    "\n",
    "        # Repeated Stratified K Fold to ensure positives are evenly distributed across batches\n",
    "        skf_1 = RepeatedStratifiedKFold(n_splits=no_batch_minus_1,n_repeats=5,random_state=0)\n",
    "        count = 0\n",
    "        epoch_count = 0\n",
    "        tol = 0.01\n",
    "        prev_score = 0\n",
    "        patience = 0 \n",
    "\n",
    "        for train,test in skf_1.split(X_train,y_train_label):\n",
    "            data = X_train.iloc[test,:]\n",
    "            data = torch.Tensor(data.values.astype(np.float32))\n",
    "             # forward pass\n",
    "            output = self.forward(data)\n",
    "            output.data = output.data.view(data.shape[0],2)\n",
    "\n",
    "            labels = y_train_label[test]\n",
    "            labels = torch.Tensor(labels.astype(np.float32))\n",
    "            labels = torch.autograd.Variable(labels).long()\n",
    "\n",
    "            # zero the gradient buffers\n",
    "            optimizer_1.zero_grad()\n",
    "            # compute loss and gradients\n",
    "            loss_output = loss(output,labels)\n",
    "            loss_output.backward()\n",
    "            # Does the update\n",
    "            optimizer_1.step()\n",
    "\n",
    "            count = count + 1\n",
    "\n",
    "            # Early Stopping\n",
    "            if count == no_batch_minus_1 + 1:\n",
    "                count = 0\n",
    "                epoch_count = epoch_count + 1\n",
    "                probs = self.predict_proba(X_valid)\n",
    "                precision, recall, _ = precision_recall_curve(y_valid, probs)\n",
    "                score = auc(recall, precision)\n",
    "                #score = roc_auc_score(y_valid, probs)\n",
    "                diff = score - prev_score\n",
    "\n",
    "                if diff < tol:\n",
    "                    patience = patience + 1\n",
    "                    prev_score = score\n",
    "                    if patience >= 4 :\n",
    "                        break\n",
    "\n",
    "                else:\n",
    "                    patience = 0\n",
    "                    prev_score = score\n",
    "                    self = self.train()\n",
    "        best_epoch_count = epoch_count - patience\n",
    "        return prev_score,best_epoch_count\n",
    "\n",
    "        \n",
    "    #prediction probabilities array\n",
    "    def predict_proba(self, X_test):\n",
    "        self = self.eval()\n",
    "        #forward pass\n",
    "        test = torch.Tensor(X_test.values.astype(np.float32))\n",
    "        output = self.forward(test)\n",
    "        sf = nn.Softmax()\n",
    "        probs = sf(output.data)\n",
    "        return probs[:,1]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Models tested (and their hyper-parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_model(classifier_method, hyperparameters):\n",
    "    if (classifier_method == \"XGB\"):\n",
    "        ligand_pos = ligands_positives_df[ligand].shape[0]\n",
    "        ligand_neg = ligands_negatives_df[ligand].shape[0]\n",
    "        scale_weight = ligand_neg/float(ligand_pos)\n",
    "        model = XGBClassifier(n_estimators=hyperparameters[\"n_estimators\"], n_jobs=-1, random_state=0, max_depth=hyperparameters[\"max_depth\"], min_child_weight=hyperparameters[\"min_child_weight\"], colsample_bytree=hyperparameters[\"colsample_bytree\"], \n",
    "                              scale_pos_weight=scale_weight)\n",
    "    elif (classifier_method == \"RF\"):\n",
    "        model = RandomForestClassifier(n_estimators=hyperparameters[\"n_estimators\"], n_jobs=-1, random_state=0)  \n",
    "    elif(classifier_method == \"Logistic\"):\n",
    "        model = LogisticRegression(C=hyperparameters[\"C\"], random_state=0, n_jobs=-1)\n",
    "    elif (classifier_method == \"KNN\"):\n",
    "        model = KNeighborsClassifier(n_neighbors=hyperparameters[\"n_neighbors\"], n_jobs=-1)\n",
    "    elif (classifier_method == \"ADA\"):\n",
    "        model = AdaBoostClassifier(n_estimators=hyperparameters[\"n_estimators\"], random_state=0)\n",
    "    elif (classifier_method == \"SVM\"):\n",
    "        model = SVC(C=hyperparameters[\"C\"], gamma = hyperparameters[\"gamma\"], kernel=\"rbf\", probability=True, random_state=0) \n",
    "    elif (classifier_method ==\"NN\"):\n",
    "        torch.manual_seed(0)\n",
    "        model = Net(hyperparameters)\n",
    "        # sets model in training mode because batch normalization behavior in training and testing modes are different\n",
    "        model = model.train()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_per_domain_auc(y_test, pred_probs, domain_pred_dict, pred_idx, classifier):\n",
    "    \"\"\"\n",
    "    Compute the average per_domain auc and auprc for the test set\n",
    "    \"\"\"\n",
    "    \n",
    "    y_test_copy = y_test.copy(deep=True)\n",
    "    y_test_copy[\"pred_probs\"] = pred_probs\n",
    "    \n",
    "    domain_auc_list = []\n",
    "    domain_auprc_list = []\n",
    "    domain_auprc_ratio_list = []\n",
    "    domain_name_list = []\n",
    "    \n",
    "    idx = y_test.index\n",
    "    y_test_copy[\"domain_name\"] = [x[:x.rfind(\"_\")] for x in idx]\n",
    "    domains_list = y_test_copy[\"domain_name\"].unique().tolist()\n",
    "        \n",
    "    for domain_name in domains_list:\n",
    "        \n",
    "        #Get only the domain positions\n",
    "        domain_df = y_test_copy[y_test_copy[\"domain_name\"] == domain_name]\n",
    "\n",
    "        #Find the binding and non-binding positions of this domain \n",
    "        bind_list = domain_df[domain_df[\"label\"] == 1].index\n",
    "        bind_idx = [int(x[len(domain_name)+1:]) for x in bind_list]\n",
    "        bind_num = len(bind_idx)\n",
    "        non_bind_list = domain_df[domain_df[\"label\"] == 0].index\n",
    "        non_bind_idx = [int(x[len(domain_name)+1:]) for x in non_bind_list]\n",
    "        non_bind_num = len(non_bind_idx)\n",
    "        if (bind_num == 0 or non_bind_num == 0):\n",
    "            #No positions of one of the classes \"binding/non-binding\" - skipping\"\n",
    "            continue\n",
    "        \n",
    "        domain_pred_dict[\"obs\"].extend(domain_df[\"label\"])\n",
    "        domain_pred_dict[\"prob\"].extend(domain_df[\"pred_probs\"])\n",
    "        fold_list = [pred_idx] * len(domain_df[\"pred_probs\"])\n",
    "        domain_pred_dict[\"fold\"].extend(fold_list)\n",
    "        model_list = [classifier] * len(domain_df[\"pred_probs\"])\n",
    "        domain_pred_dict[\"model\"].extend(model_list)\n",
    "        domain_str_list = [domain_name] * len(domain_df[\"pred_probs\"])\n",
    "        domain_pred_dict[\"domain\"].extend(domain_str_list)\n",
    "    \n",
    "        #Compute domain AUC\n",
    "        domain_auc = roc_auc_score(domain_df[\"label\"], domain_df[\"pred_probs\"])\n",
    "        domain_auc_list.append(domain_auc)\n",
    "        #Compute domain AUPRC\n",
    "        precision, recall, thresholds = precision_recall_curve(domain_df[\"label\"], domain_df[\"pred_probs\"])\n",
    "        domain_auprc = auc(recall, precision)\n",
    "        domain_auprc_list.append(domain_auprc)\n",
    "        #Add positives fraction to list\n",
    "        pos_frac_ratio = bind_num/float(domain_df.shape[0])\n",
    "        #Add ratio of AUPRC and positives fraction to list\n",
    "        domain_auprc_ratio_list.append(domain_auprc/float(pos_frac_ratio))\n",
    "        #Add domain name for AUC/AUPRC/Ratio tables\n",
    "        domain_name_list.append(domain_name)\n",
    "        \n",
    "    #Compute the means for the lists \n",
    "    domain_auc_mean = np.mean(domain_auc_list)\n",
    "    domain_auprc_mean = np.mean(domain_auprc_list)\n",
    "    domain_auprc_ratio_mean = np.mean(domain_auprc_ratio_list)\n",
    "    \n",
    "    return (domain_auc_mean, domain_auprc_mean, domain_auprc_ratio_mean, domain_auc_list, domain_auprc_list, domain_auprc_ratio_list, domain_name_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dealing with model imbalance\n",
    "Weight Vector: https://towardsdatascience.com/dealing-with-imbalanced-classes-in-machine-learning-d43d6fa19d2 (look at section on \"Cost-sensitive Learning\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test_model_iterative_fixed(hyperparameters_dict,ligand_bind_features, ligand_negatives_features, ligand_name, features=[]):\n",
    "    \n",
    "    \"\"\"\n",
    "    Test different models in 10-folds cross-validation.\n",
    "    \"\"\"\n",
    "    \n",
    "    #Default: Exclude no features\n",
    "    if len(features) == 0:\n",
    "        features = np.ones([ligand_bind_features.shape[1],]).astype(bool)\n",
    "        \n",
    "    #Arranging the features table by the CV order, for each model\n",
    "    #features_pred_dfs = dict.fromkeys(classifiers.keys())\n",
    "    features_pred_dfs = {}\n",
    "    \n",
    "    models_req_scaling = [\"SVM\", \"KNN\", \"Logistic\",\"NN\"]\n",
    "\n",
    "    classifier = classifier_method\n",
    "    \n",
    "    #model = classifiers[classifier]\n",
    "    features_pred_dfs[classifier] = pd.DataFrame()\n",
    "\n",
    "    #Create X and y with included features\n",
    "    X = pd.concat([ligand_bind_features.iloc[:,features], ligand_negatives_features.iloc[:,features]])\n",
    "    y = [1] * ligand_bind_features.shape[0]\n",
    "    y.extend([0] * ligand_negatives_features.shape[0])\n",
    "    y = np.array(y)\n",
    "    y_df = pd.DataFrame(y)\n",
    "    y_df.index = X.index\n",
    "    y_df.columns = [\"label\"]\n",
    "    \n",
    "    #Get the fold indices\n",
    "    cv_idx = calc_CV_idx_iterative(X, splits_dict)\n",
    "    k = (int(fold)-1)\n",
    "    \n",
    "    pred_idx = k+1\n",
    "    print \"fold #: \"+str(pred_idx)\n",
    "    #test_index = cv_idx[k][\"test\"]\n",
    "    full_train_index = cv_idx[k][\"train\"]\n",
    "        \n",
    "    # phase 1: testing on validation set, hyperparameter tuning\n",
    "    trials_mean_results_list = np.zeros(no_trials) \n",
    "    \n",
    "    for trial in range(no_trials):\n",
    "        trial_results = np.zeros(4)\n",
    "        epoch_counts = np.zeros(4, dtype = \"int\")\n",
    "        for i in range(4):\n",
    "            valid_k = (k + 1 + i) % 5\n",
    "            valid_index = cv_idx[valid_k][\"test\"]\n",
    "\n",
    "            train_index = [index for index in full_train_index if index not in valid_index]\n",
    "            #sanity checks to check if there are overlaps between training and validation\n",
    "            #valid_index_set = set(valid_index)\n",
    "            #train_index_set = set(train_index)\n",
    "            #print valid_index_set.intersection(train_index_set)\n",
    "            X_train, X_valid = X.loc[train_index,:], X.loc[valid_index,:]\n",
    "            y_train, y_valid = y_df.loc[train_index,:], y_df.loc[valid_index,:]\n",
    "            \n",
    "            if (classifier in models_req_scaling):\n",
    "                cols = X_train.columns\n",
    "\n",
    "                # phase 1 scaling with just training data\n",
    "                scaler_1 = StandardScaler() \n",
    "                scaler_1.fit(X_train) \n",
    "                X_train = pd.DataFrame(scaler_1.transform(X_train))\n",
    "                # apply same transformation to validation data\n",
    "                X_valid = pd.DataFrame(scaler_1.transform(X_valid))\n",
    "\n",
    "                #Restoring indices after scaling\n",
    "                X_train.index = train_index \n",
    "                X_valid.index = valid_index\n",
    "\n",
    "                #Restoring features names\n",
    "                X_train.columns = cols\n",
    "                X_valid.columns = cols\n",
    "\n",
    "            #No down-sampling\n",
    "            X_train_sampled = X_train\n",
    "            y_train_sampled = y_train\n",
    "\n",
    "            #fit to training data\n",
    "            if classifier == \"NN\":\n",
    "                #weight vector\n",
    "                no_pos = ligand_bind_features.shape[0]\n",
    "                no_neg = ligand_negatives_features.shape[0]\n",
    "                neg_weight = float(no_pos) / float(no_neg + no_pos)\n",
    "                pos_weight = 1 - neg_weight\n",
    "                weight = torch.Tensor([neg_weight, pos_weight])\n",
    "                model = generate_model(classifier, hyperparameter_trials[trial])\n",
    "                auprc_score,epoch_count = model.fit(X_train_sampled, y_train_sampled[\"label\"],X_valid, y_valid[\"label\"],weight)\n",
    "            else:\n",
    "                model = generate_model(classifier, hyperparameter_trials[trial])\n",
    "                model.fit(X_train_sampled, y_train_sampled[\"label\"])\n",
    "                probs_list = []\n",
    "                probs = model.predict_proba(X_valid)\n",
    "                for l in probs:\n",
    "                    probs_list.append(l[1])\n",
    "                precision, recall, _ = precision_recall_curve(y_valid, probs_list)\n",
    "                auprc_score = auc(recall, precision)\n",
    "                \n",
    "\n",
    "            trial_results[i] = auprc_score \n",
    "            if classifier == \"NN\": epoch_counts[i] = epoch_count\n",
    "        \n",
    "        mean_result = np.mean(trial_results)\n",
    "        trials_mean_results_list[trial] = mean_result\n",
    "        \n",
    "        if classifier == \"NN\":\n",
    "            majority_epoch_count = np.bincount(epoch_counts).argmax()\n",
    "            hyperparameter_trials[trial][\"epoch_count\"] = majority_epoch_count\n",
    "        \n",
    "        hyperparameter_trials[trial][\"mean_AUPRC\"] = mean_result\n",
    "    \n",
    "    # extract top performing hyperparameters\n",
    "    max_index = np.argmax(trials_mean_results_list)\n",
    "    best_hyperparameters = hyperparameter_trials[max_index]\n",
    "    print \"best_hyperparameters:\" + str(best_hyperparameters)\n",
    "\n",
    "    with open(curr_dir[0]+\"/best_hyperparameters/\"+ligand+\"_\"+classifier_method+\"_best_hyperparameters.pik\", 'wb') as handle:\n",
    "        pickle.dump(best_hyperparameters, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    handle.close()\n",
    "    \n",
    "    \n",
    "        \n",
    "    # Update dictionary with all hyperparameters\n",
    "    keys = hyperparameter_trials[0].keys()\n",
    "    for key in keys:\n",
    "        for trial in range(no_trials):\n",
    "            hyperparameters_dict[key].append(hyperparameter_trials[trial][key])\n",
    "\n",
    "    pred_idx += 1\n",
    "\n",
    "    print \"Finished \"+ligand+\" \"+classifier+\" fold: \"+fold\n",
    "    \n",
    "    return features_pred_dfs "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test model functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def combine_features_predictions(ligand, ordered_features, pred_df):\n",
    "    \n",
    "    pred_res = pred_df.copy(deep=True)\n",
    "    for classifier in classifiers.keys():\n",
    "        classifier = classifier_method\n",
    "        model_pred = pred_res[pred_res[\"model\"] == classifier]\n",
    "        model_pred.index = ordered_features[classifier].index\n",
    "        \n",
    "        #Creating the combined table\n",
    "        features_pred = pd.concat([ordered_features[classifier], model_pred], axis=1)\n",
    "        \n",
    "        #Saving\n",
    "        features_pred.to_csv(curr_dir[0]+\"/pred_AUC_AUPRC/\"+out_dir+\"/\"+downsample_method+\"/\"+datafile_date+\"_domain_CV/features_pred_tables/\"+ligand+\"_\"+classifier+\"_features_pred.csv\", sep='\\t')\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predict for each ligand seperatelly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold #: 3\n",
      "best_hyperparameters:{'C': 1252.0653814999462, 'mean_AUPRC': 0.173900007237034, 'gamma': 2.693883019285411e-05}\n",
      "Finished sm SVM fold: 3\n",
      "Finished ligand sm\n"
     ]
    }
   ],
   "source": [
    "#%%time\n",
    "\n",
    "#Initialize dictionary\n",
    "hyperparameters_dict = defaultdict(list)\n",
    "\n",
    "downsample_method = \"NoDown\"\n",
    "\n",
    "ordered_features = test_model_iterative_fixed(hyperparameters_dict,ligands_positives_df[ligand], ligands_negatives_df[ligand], ligand)\n",
    "\n",
    "hyperparameters_df = pd.DataFrame.from_dict(hyperparameters_dict)\n",
    "hyperparameters_df = hyperparameters_df.sort_values(by = \"mean_AUPRC\", ascending = False)\n",
    "\n",
    "#Save to file\n",
    "hyperparameters_df.to_csv(curr_dir[0]+\"/pred_AUC_AUPRC/\"+out_dir+\"/\"+downsample_method+\"/\"+datafile_date+\"_domain_CV/per_fold/\"+ligand+\"_\"+classifier_method+\"_fold\"+fold+\"_\"+str(folds_num)+\"w_hyperparameters.csv\", sep=',')\n",
    "\n",
    "#Combine features and pred results to a unified table\n",
    "#combine_features_predictions(ligand, ordered_features, pred_df)\n",
    "\n",
    "print \"Finished ligand \"+ligand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
