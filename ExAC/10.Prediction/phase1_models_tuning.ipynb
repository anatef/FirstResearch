{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n",
      "cpu\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Basic imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import pickle\n",
    "from collections import defaultdict\n",
    "from os import environ, getcwd\n",
    "import sys\n",
    "\n",
    "#Import utils functions\n",
    "curr_dir = getcwd()\n",
    "\n",
    "sys.path.append(curr_dir+\"/utils\")\n",
    "from prop_threshold_funcs import create_negatives_datasets_combined, create_positives_datasets_combined\n",
    "from prediction_general_funcs import ligands, get_features_cols, remove_unimportant_features\n",
    "from generate_hyperparameter_trials import *\n",
    "from tuning_helper_functions import test_model_on_validation, test_model_on_heldout\n",
    "\n",
    "from IPython.core.display import HTML\n",
    "HTML(\"<style>.container { width:100% !important; }</style>\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading the input dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datafile_date = \"08.06.18\"\n",
    "input_path = curr_dir+\"/domains_similarity/filtered_features_table/\"\n",
    "filename = \"windowed_positions_features_mediode_filter_\"+datafile_date+\".csv\"\n",
    "\n",
    "#flags for creating negatives\n",
    "zero_prop = True\n",
    "no_prop = True\n",
    "all_ligands = False\n",
    "prec_th_str = \"dna0.5_rna0.5_ion0.75\"\n",
    "folds_num = 5\n",
    "\n",
    "#Features table\n",
    "features_all = pd.read_csv(input_path+filename, sep='\\t', index_col=0)\n",
    "#Features columns names, without the labels (the binding scores)\n",
    "features_cols = get_features_cols(features_all)\n",
    "remove_unimportant_features(features_all, features_cols)\n",
    "\n",
    "print \"all samples positions #: \"+str(features_all.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataset of negative examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ligands_negatives_df = create_negatives_datasets_combined(zero_prop, no_prop, features_all, features_cols, all_ligands)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Datasets of positive examples by ligand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ligands_positives_df = create_positives_datasets_combined(features_all, features_cols, all_ligands)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading env input for downsampler technique, ligand and classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Reading the ligand input\n",
    "try:\n",
    "    ligand = environ['ligand']\n",
    "except:\n",
    "    ligand = \"ion\"\n",
    "print \"ligand = \"+ligand\n",
    "    \n",
    "#Reading the downsampler input\n",
    "try: \n",
    "    fold = environ['fold']\n",
    "except:\n",
    "    fold = \"4\"\n",
    "print \"fold = \"+fold\n",
    "\n",
    "#Reading the classifier input\n",
    "try: \n",
    "    classifier_method = environ['classifier']\n",
    "except:\n",
    "    classifier_method = \"NN\"\n",
    "print \"classifier_method = \"+classifier_method\n",
    "\n",
    "# Reading the index to generate model\n",
    "try:\n",
    "    trial_idx = int(environ[\"trial\"])\n",
    "except:\n",
    "    trial_idx = 12\n",
    "print \"trial idx = \"+ str(trial_idx)\n",
    "\n",
    "if classifier_method == \"NN\":\n",
    "    try:        \n",
    "        learning_rate_ub = int(environ['learning_rate_ub'])\n",
    "        learning_rate_lb = int(environ['learning_rate_lb'])\n",
    "        batch_size_ub = int(environ['batch_size_ub'])\n",
    "        batch_size_lb = int(environ['batch_size_lb'])\n",
    "        weight_decay_ub = int(environ['weight_decay_ub'])\n",
    "        weight_decay_lb = int(environ['weight_decay_lb'])\n",
    "        beta_ub = float(environ['beta_ub'])\n",
    "        beta_lb = float(environ['beta_lb'])\n",
    "        hidden_units_1_ub = int(environ['hidden_units_1_ub'])\n",
    "        hidden_units_1_lb = int(environ['hidden_units_1_lb'])\n",
    "        hidden_units_2_ub = int(environ['hidden_units_2_ub'])\n",
    "        hidden_units_2_lb = int(environ['hidden_units_2_lb'])\n",
    "        \n",
    "        try:\n",
    "            sec_learning_rate_ub = int(environ['sec_learning_rate_ub'])\n",
    "            sec_learning_rate_lb = int(environ['sec_learning_rate_lb'])\n",
    "            lr_weight_1 = float(environ['lr_weight_1'])\n",
    "            lr_weight_2 = float(environ['lr_weight_2'])\n",
    "        except:\n",
    "            sec_learning_rate_ub = learning_rate_ub \n",
    "            sec_learning_rate_lb = learning_rate_lb\n",
    "            lr_weight_1 = 1\n",
    "            lr_weight_2 = 1\n",
    "            \n",
    "        try:\n",
    "            sec_batch_size_ub = int(environ['sec_batch_size_ub'])\n",
    "            sec_batch_size_lb = int(environ['sec_batch_size_lb'])\n",
    "            batch_size_weight_1 = float(environ['batch_size_weight_1'])\n",
    "            batch_size_weight_2 = float(environ['batch_size_weight_2'])\n",
    "            \n",
    "        except:\n",
    "            sec_batch_size_ub = batch_size_ub\n",
    "            sec_batch_size_lb = batch_size_lb\n",
    "            batch_size_weight_1 = 1\n",
    "            batch_size_weight_2 = 1\n",
    "            \n",
    "        try:\n",
    "            sec_weight_decay_ub = int(environ['sec_weight_decay_ub'])\n",
    "            sec_weight_decay_lb = int(environ['sec_weight_decay_lb'])\n",
    "            weight_decay_weight_1 = float(environ['weight_decay_weight_1'])\n",
    "            weight_decay_weight_2 = float(environ['weight_decay_weight_2'])\n",
    "            \n",
    "        except:\n",
    "            sec_weight_decay_ub = weight_decay_ub\n",
    "            sec_weight_decay_lb = weight_decay_lb\n",
    "            weight_decay_weight_1 = 1\n",
    "            weight_decay_weight_2 = 1\n",
    "        try:\n",
    "            sec_beta_ub = float(environ['sec_beta_ub'])\n",
    "            sec_beta_lb = float(environ['sec_beta_lb'])\n",
    "            beta_weight_1 = float(environ['beta_weight_1'])\n",
    "            beta_weight_2 = float(environ['beta_weight_2'])\n",
    "        except:\n",
    "            sec_beta_ub = beta_ub\n",
    "            sec_beta_lb = beta_lb\n",
    "            beta_weight_1 = 1\n",
    "            beta_weight_2 = 1\n",
    "            \n",
    "        try:\n",
    "            sec_hidden_units_1_ub = int(environ['sec_hidden_units_1_ub'])\n",
    "            sec_hidden_units_1_lb = int(environ['sec_hidden_units_1_lb'])\n",
    "            hidden_units_1_weight_1 = float(environ['hidden_units_1_weight_1'])\n",
    "            hidden_units_1_weight_2 = float(environ['hidden_units_1_weight_2'])\n",
    "        except:\n",
    "            sec_hidden_units_1_ub = hidden_units_1_ub\n",
    "            sec_hidden_units_1_lb = hidden_units_1_lb\n",
    "            hidden_units_1_weight_1 = 1\n",
    "            hidden_units_1_weight_2 = 1\n",
    "            \n",
    "        try:\n",
    "            sec_hidden_units_2_ub = int(environ['sec_hidden_units_2_ub'])\n",
    "            sec_hidden_units_2_lb = int(environ['sec_hidden_units_2_lb'])\n",
    "            hidden_units_2_weight_1 = float(environ['hidden_units_2_weight_1'])\n",
    "            hidden_units_2_weight_2 = float(environ['hidden_units_2_weight_2'])\n",
    "            \n",
    "        except: \n",
    "            sec_hidden_units_2_ub = hidden_units_2_ub\n",
    "            sec_hidden_units_2_lb = hidden_units_2_lb\n",
    "            hidden_units_2_weight_1 = 1\n",
    "            hidden_units_2_weight_2 = 1\n",
    "            \n",
    "    except:        \n",
    "        print \"Error: goto NN exception\"\n",
    "        learning_rate_ub = -3\n",
    "        learning_rate_lb = -5\n",
    "        batch_size_ub = 300\n",
    "        batch_size_lb = 30\n",
    "        weight_decay_ub = -5\n",
    "        weight_decay_lb = -25\n",
    "        beta_ub = 0.95\n",
    "        beta_lb = 0.85\n",
    "        hidden_units_1_ub = 1000\n",
    "        hidden_units_1_lb = 200\n",
    "        hidden_units_2_ub = 1000\n",
    "        hidden_units_2_lb = 350\n",
    "        \n",
    "        sec_learning_rate_ub = -4\n",
    "        sec_learning_rate_lb = -5\n",
    "        sec_batch_size_ub = 300\n",
    "        sec_batch_size_lb = 30\n",
    "        sec_weight_decay_ub = -5\n",
    "        sec_weight_decay_lb = -25\n",
    "        sec_beta_ub = 0.95\n",
    "        sec_beta_lb = 0.85\n",
    "        sec_hidden_units_1_ub = 1000\n",
    "        sec_hidden_units_1_lb = 200\n",
    "        sec_hidden_units_2_ub = 1000\n",
    "        sec_hidden_units_2_lb = 350\n",
    "        \n",
    "        lr_weight_1 = 1\n",
    "        lr_weight_2 = 1\n",
    "        batch_size_weight_1 = 1\n",
    "        batch_size_weight_2 = 1\n",
    "        weight_decay_weight_1 = 1\n",
    "        weight_decay_weight_2 = 1\n",
    "        beta_weight_1 = 1\n",
    "        beta_weight_2 = 1\n",
    "        hidden_units_1_weight_1 = 1\n",
    "        hidden_units_1_weight_2 = 1\n",
    "        hidden_units_2_weight_1 = 1\n",
    "        hidden_units_2_weight_2 = 1\n",
    "        \n",
    "    \n",
    "\n",
    "elif classifier_method == \"XGB\":\n",
    "    \n",
    "    try:\n",
    "        max_depth_ub = int(environ[\"max_depth_ub\"])\n",
    "        max_depth_lb = int(environ[\"max_depth_lb\"])\n",
    "        min_child_weight_ub = float(environ[\"min_child_weight_ub\"])\n",
    "        min_child_weight_lb = float(environ[\"min_child_weight_lb\"])\n",
    "        colsample_bytree_ub = float(environ[\"colsample_bytree_ub\"])\n",
    "        colsample_bytree_lb = float(environ[\"colsample_bytree_lb\"])\n",
    "        gamma_ub = float(environ[\"gamma_ub\"])\n",
    "        gamma_lb = float(environ[\"gamma_lb\"])\n",
    "        learning_rate_ub = float(environ[\"learning_rate_ub\"])\n",
    "        learning_rate_lb = float(environ[\"learning_rate_lb\"])\n",
    "\n",
    "        try:\n",
    "            sec_max_depth_ub = int(environ[\"sec_max_depth_ub\"])\n",
    "            sec_max_depth_lb = int(environ[\"sec_max_depth_lb\"])\n",
    "            max_depth_weight_1 = float(environ[\"max_depth_weight_1\"])\n",
    "            max_depth_weight_2 = float(environ[\"max_depth_weight_2\"])\n",
    "        except:\n",
    "            sec_max_depth_ub = max_depth_ub\n",
    "            sec_max_depth_lb = max_depth_lb\n",
    "            max_depth_weight_1 = 1\n",
    "            max_depth_weight_2 = 1\n",
    "        try:\n",
    "            sec_min_child_weight_ub = float(environ['sec_min_child_weight_ub'])\n",
    "            sec_min_child_weight_lb = float(environ['sec_min_child_weight_lb'])\n",
    "            min_child_weight_weight_1 = float(environ[\"min_child_weight_weight_1\"])\n",
    "            min_child_weight_weight_2 = float(environ[\"min_child_weight_weight_2\"])\n",
    "            \n",
    "        except:\n",
    "            sec_min_child_weight_ub = min_child_weight_ub\n",
    "            sec_min_child_weight_lb = min_child_weight_lb\n",
    "            min_child_weight_weight_1 = 1\n",
    "            min_child_weight_weight_2 = 1\n",
    "        try:\n",
    "            sec_colsample_bytree_ub = float(environ['sec_colsample_bytree_ub'])\n",
    "            sec_colsample_bytree_lb = float(environ['sec_colsample_bytree_lb'])\n",
    "            colsample_bytree_weight_1 = float(environ['colsample_bytree_weight_1'])\n",
    "            colsample_bytree_weight_2 = float(environ['colsample_bytree_weight_2'])\n",
    "        except:\n",
    "            sec_colsample_bytree_ub = colsample_bytree_ub\n",
    "            sec_colsample_bytree_lb = colsample_bytree_lb\n",
    "            colsample_bytree_weight_1 = 1\n",
    "            colsample_bytree_weight_2 = 1\n",
    "            \n",
    "        try:\n",
    "            sec_gamma_ub = float(environ['sec_gamma_ub'])\n",
    "            sec_gamma_lb = float(environ['sec_gamma_lb'])\n",
    "            gamma_weight_1 = float(environ['gamma_weight_1'])\n",
    "            gamma_weight_2 = float(environ['gamma_weight_2'])\n",
    "        except:\n",
    "            sec_gamma_ub = gamma_ub\n",
    "            sec_gamma_lb = gamma_lb\n",
    "            gamma_weight_1 = 1\n",
    "            gamma_weight_2 = 1\n",
    "        try:\n",
    "            sec_learning_rate_ub = float(environ['sec_learning_rate_ub'])\n",
    "            sec_learning_rate_lb = float(environ['sec_learning_rate_lb'])\n",
    "            lr_weight_1 = float(environ['lr_weight_1'])\n",
    "            lr_weight_2 = float(environ['lr_weight_2'])\n",
    "        except:\n",
    "            sec_learning_rate_ub = learning_rate_ub\n",
    "            sec_learning_rate_lb = learning_rate_lb\n",
    "            lr_weight_1 = 1\n",
    "            lr_weight_2 = 1\n",
    "\n",
    "    except:    \n",
    "        print \"Error: goto XGB exception\"\n",
    "        max_depth_ub = 100\n",
    "        max_depth_lb = 1\n",
    "        min_child_weight_ub = 5\n",
    "        min_child_weight_lb = 0\n",
    "        colsample_bytree_ub = 1\n",
    "        colsample_bytree_lb = 0.25\n",
    "        gamma_ub = 0\n",
    "        gamma_lb = -3\n",
    "        learning_rate_ub = -0.5\n",
    "        learning_rate_lb = -4\n",
    "        \n",
    "        sec_max_depth_ub = 100\n",
    "        sec_max_depth_lb = 1\n",
    "        sec_min_child_weight_ub = 5\n",
    "        sec_min_child_weight_lb = 0\n",
    "        sec_colsample_bytree_ub = 1\n",
    "        sec_colsample_bytree_lb = 0.25\n",
    "        sec_gamma_ub = 0\n",
    "        sec_gamma_lb = -3\n",
    "        sec_learning_rate_ub = -0.5\n",
    "        sec_learning_rate_lb = -4\n",
    "        \n",
    "        max_depth_weight_1 = 0.5\n",
    "        max_depth_weight_2 = 0.5\n",
    "        min_child_weight_weight_1 = 0.5\n",
    "        min_child_weight_weight_2 = 0.5\n",
    "        colsample_bytree_weight_1 = 0.5\n",
    "        colsample_bytree_weight_2 = 0.5\n",
    "        gamma_weight_1 = 0.5\n",
    "        gamma_weight_2 = 0.5\n",
    "        lr_weight_1 = 0.5\n",
    "        lr_weight_2 = 0.5\n",
    "        \n",
    "\n",
    "elif classifier_method == \"RF\":  \n",
    "    try:\n",
    "        n_estimators_ub = int(environ[\"n_estimators_ub\"])\n",
    "        n_estimators_lb = int(environ[\"n_estimators_lb\"])\n",
    "        max_depth_ub = int(environ[\"max_depth_ub\"])\n",
    "        max_depth_lb = int(environ[\"max_depth_lb\"])\n",
    "        min_samples_leaf_ub = int(environ[\"min_samples_leaf_ub\"])\n",
    "        min_samples_leaf_lb = int(environ[\"min_samples_leaf_lb\"])\n",
    "        min_samples_split_ub = int(environ[\"min_samples_split_ub\"])\n",
    "        min_samples_split_lb = int(environ[\"min_samples_split_lb\"])\n",
    "\n",
    "        try:\n",
    "            sec_n_estimators_ub = int(environ['sec_n_estimators_ub'])\n",
    "            sec_n_estimators_lb = int(environ['sec_n_estimators_lb'])\n",
    "            n_estimators_weight_1 = float(environ['n_estimators_weight_1'])\n",
    "            n_estimators_weight_2 = float(environ['n_estimators_weight_2'])\n",
    "        except:\n",
    "            sec_n_estimators_ub = n_estimators_ub\n",
    "            sec_n_estimators_lb = n_estimators_lb\n",
    "            n_estimators_weight_1 = 1\n",
    "            n_estimators_weight_2 = 1\n",
    "            \n",
    "        try:\n",
    "            sec_max_depth_ub = int(environ['sec_max_depth_ub'])\n",
    "            sec_max_depth_lb = int(environ['sec_max_depth_lb'])\n",
    "            max_depth_weight_1 = float(environ['max_depth_weight_1'])\n",
    "            max_depth_weight_2 = float(environ['max_depth_weight_2'])\n",
    "        except:\n",
    "            sec_max_depth_ub = max_depth_ub\n",
    "            sec_max_depth_lb = max_depth_lb\n",
    "            max_depth_weight_1 = 1\n",
    "            max_depth_weight_2 = 1\n",
    "            \n",
    "        try:\n",
    "            sec_min_samples_leaf_ub = int(environ['sec_min_samples_leaf_ub'])\n",
    "            sec_min_samples_leaf_lb = int(environ['sec_min_samples_leaf_lb'])\n",
    "            min_samples_leaf_weight_1 = float(environ['min_samples_leaf_weight_1'])\n",
    "            min_samples_leaf_weight_2 = float(environ['min_samples_leaf_weight_2'])\n",
    "        except:\n",
    "            sec_min_samples_leaf_ub = min_samples_leaf_ub\n",
    "            sec_min_samples_leaf_lb = min_samples_leaf_lb\n",
    "            min_samples_leaf_weight_1 = 1\n",
    "            min_samples_leaf_weight_2 = 1\n",
    "        try:\n",
    "            sec_min_samples_split_ub = int(environ['sec_min_samples_split_ub'])\n",
    "            sec_min_samples_split_lb = int(environ['sec_min_samples_split_lb'])\n",
    "            min_samples_split_weight_1 = float(environ['min_samples_split_weight_1'])\n",
    "            min_samples_split_weight_2 = float(environ['min_samples_split_weight_2'])\n",
    "            \n",
    "        except:\n",
    "            sec_min_samples_split_ub = min_samples_split_ub\n",
    "            sec_min_samples_split_lb = min_samples_split_lb\n",
    "            min_samples_split_weight_1 = 1\n",
    "            min_samples_split_weight_2 = 1\n",
    "\n",
    "    except:\n",
    "        print \"Error: goto RF exception\"\n",
    "        n_estimators_ub = 1500\n",
    "        n_estimators_lb = 10\n",
    "        max_depth_ub = 100\n",
    "        max_depth_lb = 2\n",
    "        min_samples_leaf_ub = 50\n",
    "        min_samples_leaf_lb = 1\n",
    "        min_samples_split_ub = 50\n",
    "        min_samples_split_lb = 2\n",
    "\n",
    "        sec_n_estimators_ub = 1500\n",
    "        sec_n_estimators_lb = 10\n",
    "        sec_max_depth_ub = 100\n",
    "        sec_max_depth_lb = 2\n",
    "        sec_min_samples_leaf_ub = 50\n",
    "        sec_min_samples_leaf_lb = 1\n",
    "        sec_min_samples_split_ub = 50\n",
    "        sec_min_samples_split_lb = 2\n",
    "        \n",
    "        n_estimators_weight_1 = 1\n",
    "        n_estimators_weight_2 = 1\n",
    "        max_depth_weight_1 = 1\n",
    "        max_depth_weight_2 = 1\n",
    "        min_samples_leaf_weight_1 = 1\n",
    "        min_samples_leaf_weight_2 = 1\n",
    "        min_samples_split_weight_1 = 1\n",
    "        min_samples_split_weight_2 = 1\n",
    "\n",
    "\n",
    "elif classifier_method == \"Logistic\":\n",
    "    try:        \n",
    "        C_ub = int(environ[\"C_ub\"])\n",
    "        C_lb = int(environ[\"C_lb\"])\n",
    "\n",
    "        try:\n",
    "            sec_C_ub = int(environ[\"sec_C_ub\"])\n",
    "            sec_C_lb = int(environ[\"sec_C_lb\"])\n",
    "            C_weight_1 = float(environ[\"C_weight_1\"])\n",
    "            C_weight_2 = float(environ[\"C_weight_2\"])\n",
    "            \n",
    "        except:\n",
    "            sec_C_ub = C_ub\n",
    "            sec_C_lb = C_lb\n",
    "            C_weight_1 = 1\n",
    "            C_weight_2 = 1\n",
    "\n",
    "    except: \n",
    "        print \"Error: goto Logistic exception\"\n",
    "        C_ub = 1\n",
    "        C_lb = -3\n",
    "        \n",
    "        sec_C_ub = 1\n",
    "        sec_C_lb = -3\n",
    "        \n",
    "        C_weight_1 = 1\n",
    "        C_weight_2 = 1\n",
    "        \n",
    "\n",
    "elif classifier_method == \"KNN\":\n",
    "    try:\n",
    "        n_neighbors_ub = int(environ[\"n_neighbors_ub\"])\n",
    "        n_neighbors_lb = int(environ[\"n_neighbors_lb\"])\n",
    "        \n",
    "        try:\n",
    "            sec_n_neighbors_ub = int(environ[\"sec_n_neighbors_ub\"]) \n",
    "            sec_n_neighbors_lb = int(environ[\"sec_n_neighbors_lb\"]) \n",
    "            n_neighbors_weight_1 = float(environ[\"n_neighbors_weight_1\"])\n",
    "            n_neighbors_weight_2 = float(environ[\"n_neighbors_weight_2\"])\n",
    "        except:\n",
    "            sec_n_neighbors_ub = n_neighbors_ub\n",
    "            sec_n_neighbors_lb = n_neighbors_lb\n",
    "            n_neighbors_weight_1 = 1\n",
    "            n_neighbors_weight_2 = 1       \n",
    "\n",
    "    except:\n",
    "        print \"Error: goto KNN exception\"\n",
    "        n_neighbors_ub = 300\n",
    "        n_neighbors_lb = 150\n",
    "        \n",
    "        sec_n_neighbors_ub = 1000\n",
    "        sec_n_neighbors_lb = 450\n",
    "        \n",
    "        n_neighbors_weight_1 = 0.25\n",
    "        n_neighbors_weight_2 = 0.75\n",
    "          \n",
    "        \n",
    "elif classifier_method == \"ADA\":\n",
    "    try:\n",
    "        n_estimators_ub = int(environ[\"n_estimators_ub\"])\n",
    "        n_estimators_lb = int(environ[\"n_estimators_lb\"])\n",
    "        learning_rate_ub = float(environ[\"learning_rate_ub\"])\n",
    "        learning_rate_lb = float(environ[\"learning_rate_lb\"])\n",
    "        \n",
    "        try:         \n",
    "            sec_n_estimators_ub = int(environ[\"sec_n_estimators_ub\"]) \n",
    "            sec_n_estimators_lb = int(environ[\"sec_n_estimators_lb\"])\n",
    "            n_estimators_weight_1 = float(environ['n_estimators_weight_1'])\n",
    "            n_estimators_weight_2 = float(environ['n_estimators_weight_2'])\n",
    "            \n",
    "        except:\n",
    "            sec_n_estimators_ub = n_estimators_ub\n",
    "            sec_n_estimators_lb = n_estimators_lb\n",
    "            n_estimators_weight_1 = 1\n",
    "            n_estimators_weight_2 = 1\n",
    "        try:\n",
    "            sec_learning_rate_ub = float(environ[\"sec_learning_rate_ub\"])\n",
    "            sec_learning_rate_lb = float(environ[\"sec_learning_rate_lb\"]) \n",
    "            lr_weight_1 = float(environ['lr_weight_1'])\n",
    "            lr_weight_2 = float(environ['lr_weight_2'])\n",
    "        except:\n",
    "            sec_learning_rate_ub = learning_rate_ub\n",
    "            sec_learning_rate_lb = learning_rate_lb         \n",
    "            lr_weight_1 = 1\n",
    "            lr_weight_2 = 1\n",
    "            \n",
    "    except:\n",
    "        print \"Error: goto ADA exception\"\n",
    "        n_estimators_ub = 1500\n",
    "        n_estimators_lb = 100\n",
    "        learning_rate_ub = -0.5\n",
    "        learning_rate_lb = -4\n",
    "        \n",
    "        sec_n_estimators_ub = 1500\n",
    "        sec_n_estimators_lb = 100\n",
    "        sec_learning_rate_ub = -0.5\n",
    "        sec_learning_rate_lb = -4\n",
    "        \n",
    "        n_estimators_weight_1 = 1\n",
    "        n_estimators_weight_2 = 1\n",
    "        lr_weight_1 = 1\n",
    "        lr_weight_2 = 1\n",
    "        \n",
    "        \n",
    "        \n",
    "elif classifier_method == \"SVM\":\n",
    "    try:\n",
    "        C_ub = int(environ[\"C_ub\"])\n",
    "        C_lb = int(environ[\"C_lb\"])\n",
    "        gamma_ub = int(environ[\"gamma_ub\"])\n",
    "        gamma_lb = int(environ[\"gamma_lb\"])\n",
    "        \n",
    "        try:\n",
    "            sec_C_ub = int(environ[\"sec_C_ub\"]) \n",
    "            sec_C_lb = int(environ[\"sec_C_lb\"]) \n",
    "            C_weight_1 = float(environ[\"C_weight_1\"])\n",
    "            C_weight_2 = float(environ[\"C_weight_2\"])\n",
    "        \n",
    "        except:\n",
    "            sec_C_ub = C_ub\n",
    "            sec_C_lb = C_lb\n",
    "            C_weight_1 = 1\n",
    "            C_weight_2 = 1\n",
    "        \n",
    "        try:\n",
    "            sec_gamma_ub = int(environ[\"sec_gamma_ub\"]) \n",
    "            sec_gamma_lb = int(environ[\"sec_gamma_lb\"])\n",
    "            gamma_weight_1 = float(environ['gamma_weight_1'])\n",
    "            gamma_weight_2 = float(environ['gamma_weight_2'])\n",
    "        except:\n",
    "            sec_gamma_ub = gamma_ub\n",
    "            sec_gamma_lb = gamma_lb\n",
    "            gamma_weight_1 = 1\n",
    "            gamma_weight_2 = 1            \n",
    "            \n",
    "    except:\n",
    "        print \"Error: goto SVM exception\"\n",
    "        C_ub = 2\n",
    "        C_lb = -4\n",
    "        gamma_ub = -1\n",
    "        gamma_lb = -5\n",
    "        \n",
    "        sec_C_ub = 2\n",
    "        sec_C_lb = -4\n",
    "        sec_gamma_ub = -1\n",
    "        sec_gamma_lb = -5\n",
    "        C_weight_1 = 1\n",
    "        C_weight_2 = 1\n",
    "        gamma_weight_1 = 1\n",
    "        gamma_weight_2 = 1  \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate hyperparameter trials\n",
    "\n",
    "Choose hyperparameters and generate hyperparameters through random search in a grid, as explained by this video: https://www.youtube.com/watch?v=WrICwRrvuIc&index=66&list=PLBAGcD3siRDguyYYzhVwZ3tLvOyyG5k6K\n",
    "\n",
    "Use logarithmic scale for search for learnining rate and weight decay for NN, as explained by this video: https://www.youtube.com/watch?v=VUbrW8OK3uo&index=67&list=PLBAGcD3siRDguyYYzhVwZ3tLvOyyG5k6K\n",
    "\n",
    "Utilize nested cross validation to choose between models, as described here: https://stats.stackexchange.com/questions/266225/step-by-step-explanation-of-k-fold-cross-validation-with-grid-search-to-optimise/266229"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_trials = 100\n",
    "                              \n",
    "if classifier_method == \"NN\":\n",
    "    lr_list = [[learning_rate_lb, learning_rate_ub],[sec_learning_rate_lb, sec_learning_rate_ub]]\n",
    "    \n",
    "    lr_list_weights = [lr_weight_1, lr_weight_2]\n",
    "    \n",
    "    batch_size_list = [[batch_size_lb, batch_size_ub],[sec_batch_size_lb, sec_batch_size_ub]]\n",
    "    \n",
    "    batch_size_list_weights = [batch_size_weight_1, batch_size_weight_2]\n",
    "    \n",
    "    weight_decay_list = [[weight_decay_lb, weight_decay_ub],[sec_weight_decay_lb, sec_weight_decay_ub]]\n",
    "    \n",
    "    weight_decay_list_weights = [weight_decay_weight_1, weight_decay_weight_2]\n",
    "    \n",
    "    beta_list = [[beta_lb, beta_ub], [sec_beta_lb, sec_beta_ub]]\n",
    "    \n",
    "    beta_list_weights = [beta_weight_1, beta_weight_2]\n",
    "    \n",
    "    hidden_units_1_list = [[hidden_units_1_lb, hidden_units_1_ub],[sec_hidden_units_1_lb, sec_hidden_units_1_ub]]\n",
    "    \n",
    "    hidden_units_1_list_weights = [hidden_units_1_weight_1, hidden_units_1_weight_2]\n",
    "    \n",
    "    hidden_units_2_list = [[hidden_units_2_lb, hidden_units_2_ub],[sec_hidden_units_2_lb, sec_hidden_units_2_ub]]\n",
    "    \n",
    "    hidden_units_2_list_weights = [hidden_units_2_weight_1, hidden_units_2_weight_2]\n",
    "                              \n",
    "    hyperparameter_trials = generate_trials_NN(no_trials, lr_list, lr_list_weights, batch_size_list, \n",
    "                                               batch_size_list_weights, weight_decay_list, weight_decay_list_weights, \n",
    "                                               beta_list, beta_list_weights, hidden_units_1_list, hidden_units_1_list_weights, \n",
    "                                               hidden_units_2_list, hidden_units_2_list_weights)\n",
    "elif classifier_method == \"XGB\":\n",
    "    max_depth_list = [[max_depth_lb, max_depth_ub], [sec_max_depth_lb, sec_max_depth_ub]]\n",
    "    \n",
    "    max_depth_list_weights = [max_depth_weight_1, max_depth_weight_2]\n",
    "    \n",
    "    min_child_weight_list = [[min_child_weight_lb, min_child_weight_ub],[sec_min_child_weight_lb, sec_min_child_weight_ub]]\n",
    "    \n",
    "    min_child_weight_list_weights = [min_child_weight_weight_1, min_child_weight_weight_2]\n",
    "    \n",
    "    colsample_bytree_list = [[colsample_bytree_lb, colsample_bytree_ub], [sec_colsample_bytree_lb, sec_colsample_bytree_ub]]\n",
    "    \n",
    "    colsample_bytree_list_weights = [colsample_bytree_weight_1, colsample_bytree_weight_2]\n",
    "    \n",
    "    gamma_list = [[gamma_lb, gamma_ub],[sec_gamma_lb, sec_gamma_ub]] \n",
    "    \n",
    "    gamma_list_weights = [gamma_weight_1, gamma_weight_2]\n",
    "    \n",
    "    lr_list = [[learning_rate_lb, learning_rate_ub],[sec_learning_rate_lb, sec_learning_rate_ub]]\n",
    "    \n",
    "    lr_list_weights = [lr_weight_1, lr_weight_2]\n",
    "\n",
    "    hyperparameter_trials = generate_trials_XGB(no_trials, max_depth_list, max_depth_list_weights, min_child_weight_list, \n",
    "                                                min_child_weight_list_weights, colsample_bytree_list, colsample_bytree_list_weights, \n",
    "                                                gamma_list, gamma_list_weights, lr_list, lr_list_weights)\n",
    "                                      \n",
    "elif classifier_method == \"RF\":\n",
    "    \n",
    "    n_estimators_list = [[n_estimators_lb, n_estimators_ub],[sec_n_estimators_lb, sec_n_estimators_ub]]\n",
    "    \n",
    "    n_estimators_list_weights = [n_estimators_weight_1, n_estimators_weight_2]\n",
    "\n",
    "    max_depth_list = [[max_depth_lb, max_depth_ub],[sec_max_depth_lb, sec_max_depth_ub]]\n",
    "    \n",
    "    max_depth_list_weights = [max_depth_weight_1, max_depth_weight_2]\n",
    "    \n",
    "    min_samples_leaf_list = [[min_samples_leaf_lb, min_samples_leaf_ub], [sec_min_samples_leaf_lb, sec_min_samples_leaf_ub]]\n",
    " \n",
    "    min_samples_leaf_list_weights = [min_samples_leaf_weight_1, min_samples_leaf_weight_2]\n",
    "    \n",
    "    min_samples_split_list = [[min_samples_split_lb, min_samples_split_ub],\n",
    "                              [sec_min_samples_split_lb, sec_min_samples_split_ub]]\n",
    "       \n",
    "    min_samples_split_list_weights = [min_samples_split_weight_1, min_samples_split_weight_2]\n",
    "    \n",
    "    hyperparameter_trials = generate_trials_RF(no_trials, n_estimators_list, n_estimators_list_weights, max_depth_list, \n",
    "                                               max_depth_list_weights, min_samples_leaf_list, min_samples_leaf_list_weights, \n",
    "                                               min_samples_split_list, min_samples_split_list_weights)\n",
    "    \n",
    "elif classifier_method == \"Logistic\":\n",
    "    C_list = [[C_lb, C_ub], [sec_C_lb, sec_C_ub]]\n",
    "    \n",
    "    C_list_weights = [C_weight_1, C_weight_2]\n",
    "    \n",
    "    hyperparameter_trials = generate_trials_Log(no_trials, C_list, C_list_weights)\n",
    "    \n",
    "elif classifier_method == \"KNN\":\n",
    "    \n",
    "    n_neighbors_list = [[n_neighbors_lb, n_neighbors_ub], [sec_n_neighbors_lb, sec_n_neighbors_ub]]\n",
    "    \n",
    "    n_neighbors_list_weights = [n_neighbors_weight_1, n_neighbors_weight_2]\n",
    "    \n",
    "    hyperparameter_trials = generate_trials_KNN(no_trials, n_neighbors_list, n_neighbors_list_weights)\n",
    "    \n",
    "elif classifier_method == \"ADA\":\n",
    "    \n",
    "    n_estimators_list = [[n_estimators_lb, n_estimators_ub], [sec_n_estimators_lb, sec_n_estimators_ub]]\n",
    "    \n",
    "    n_estimators_list_weights = [n_estimators_weight_1, n_estimators_weight_2]\n",
    "    \n",
    "    lr_list = [[learning_rate_lb, learning_rate_ub], [sec_learning_rate_lb, sec_learning_rate_ub]]\n",
    "    \n",
    "    lr_list_weights = [lr_weight_1, lr_weight_2]\n",
    "    \n",
    "    hyperparameter_trials = generate_trials_ADA(no_trials, n_estimators_list, n_estimators_list_weights, lr_list, \n",
    "                                                lr_list_weights)\n",
    "    \n",
    "elif classifier_method == \"SVM\":\n",
    "    C_list = [[C_lb, C_ub], [sec_C_lb, sec_C_ub]]\n",
    "    \n",
    "    C_list_weights = [C_weight_1, C_weight_2]\n",
    "                                      \n",
    "    gamma_list = [[gamma_lb, gamma_ub], [sec_gamma_lb, sec_gamma_ub]]\n",
    "    \n",
    "    gamma_list_weights = [gamma_weight_1, gamma_weight_2]\n",
    "                                          \n",
    "    hyperparameter_trials = generate_trials_SVM(no_trials, C_list, C_list_weights, gamma_list, gamma_list_weights)\n",
    "\n",
    "#print hyperparameter_trials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters = hyperparameter_trials[trial_idx]\n",
    "print hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Models tested (and their hyper-parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predict for each ligand seperatelly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "hyperparameters = hyperparameter_trials[trial_idx]\n",
    "hyperparameters_output_dict = defaultdict(list)\n",
    "\n",
    "#Get validation params\n",
    "test_model_on_validation(hyperparameters, hyperparameters_output_dict,ligands_positives_df[ligand], ligands_negatives_df[ligand], ligand, classifier_method, fold, trial_idx,\n",
    "                        xgb_early_stopping_rounds=500, xgb_increase_rounds_limit=500)\n",
    "\n",
    "#Get test fold performance\n",
    "if (classifier_method == \"NN\" or classifier_method == \"XGB\"):\n",
    "    hyperparameters[\"mean_epoch_count\"] = hyperparameters_output_dict[\"mean_epoch_count\"]\n",
    "test_model_on_heldout(hyperparameters, hyperparameters_output_dict, hyperparameters, ligands_positives_df[ligand], ligands_negatives_df[ligand], ligand, classifier_method, fold)\n",
    "hyperparameters_df = pd.DataFrame.from_dict(hyperparameters_output_dict)\n",
    "\n",
    "#Save to file\n",
    "hyperparameters_df.to_csv(curr_dir+\"/hyperparam_tuning/phase1_initial_run/\"+datafile_date+\"_\"+prec_th_str+\"/per_trial/\"+ligand+\"_\"+classifier_method+\"_fold\"+fold+\"_trial\"+str(trial_idx)+\"_\"+str(folds_num)+\"w_hyperparameters.csv\", sep='\\t')\n",
    "\n",
    "print \"Finished ligand \"+ligand\n",
    "print \"time elapsed = \"+str(time.time()-start_time)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
