{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Basic imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from collections import defaultdict\n",
    "from os import environ, getcwd\n",
    "import sys\n",
    "\n",
    "#Classifier imports\n",
    "from sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, RandomForestRegressor, BaggingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "#ML framework imports\n",
    "from sklearn.metrics import auc, roc_auc_score, precision_recall_curve\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import scale\n",
    "\n",
    "#Downsamplers imports - prototype generation\n",
    "from imblearn.under_sampling import ClusterCentroids\n",
    "\n",
    "#Downsamplers imports - prototype selection - controlled\n",
    "from imblearn.under_sampling import RandomUnderSampler, NearMiss\n",
    "\n",
    "#Downsamplers imports - prototype selection - Cleaning techniques\n",
    "from imblearn.under_sampling import TomekLinks, EditedNearestNeighbours, RepeatedEditedNearestNeighbours\n",
    "\n",
    "#Downsamplers imports - prototype selection - Cleaning techniques - Condensed nearest neighbors and derived algorithms\n",
    "from imblearn.under_sampling import CondensedNearestNeighbour, OneSidedSelection, NeighbourhoodCleaningRule\n",
    "\n",
    "#Downsamplers imports - prototype selection - Cleaning techniques\n",
    "from imblearn.under_sampling import InstanceHardnessThreshold\n",
    "\n",
    "#Import utils functions\n",
    "curr_dir = !pwd\n",
    "sys.path.append(curr_dir[0]+\"/utils\")\n",
    "from neg_pos_funcs import create_negatives_datasets, create_positives_datasets\n",
    "from CV_funcs import add_domain_name_from_table_idx, calc_CV_idx_iterative\n",
    "\n",
    "from IPython.core.display import HTML\n",
    "HTML(\"<style>.container { width:100% !important; }</style>\")\n",
    "\n",
    "ABSOLUTE_NEGATIVES = False\n",
    "FILTER_DOMAIN = False\n",
    "FILTER_MAX_SCORE_ZERO = False\n",
    "out_dir = \"mediode_NegLigand_NoFilter\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading the input dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all samples positions #: 38944\n"
     ]
    }
   ],
   "source": [
    "curr_dir = !pwd\n",
    "input_path = curr_dir[0]+\"/domains_similarity/filtered_features_table/\"\n",
    "filename = \"windowed_positions_features_mediode_filter_01.25.18.csv\"\n",
    "\n",
    "#input_path = curr_dir[0]+\"/../9.Features_exploration/binding_df/10/\"\n",
    "#filename = \"positions_features_01.25.18.csv\"\n",
    "\n",
    "bind_scores_num = 10\n",
    "ligands = [\"dna\", \"dnabase\", \"dnabackbone\", \"rna\", \"rnabase\", \"rnabackbone\", \"peptide\", \"ion\", \"metabolite\"]\n",
    "\n",
    "#Features table\n",
    "features_all = pd.read_csv(input_path+filename, sep='\\t', index_col=0)\n",
    "#Features columns names, without the labels (the binding scores)\n",
    "features_cols = features_all.columns.tolist()\n",
    "#removing binding scores and domain name\n",
    "for ligand in ligands:\n",
    "    score_str = ligand+\"_binding_score\"\n",
    "    features_cols.remove(score_str)\n",
    "features_cols.remove(\"max_binding_score\")\n",
    "features_cols.remove(\"domain_name\")\n",
    "\n",
    "\n",
    "print \"all samples positions #: \"+str(features_all.shape[0])\n",
    "\n",
    "#lignd binding domains dictionary\n",
    "with open(curr_dir[0]+\"/ligands_negatives_domains_dict.pik\", 'rb') as handle:\n",
    "        negatives_dict = pickle.load(handle)\n",
    "\n",
    "#CV splits dictionary\n",
    "with open(curr_dir[0]+\"/CV_splits/domain_10_splits_dict.pik\", 'rb') as handle:\n",
    "        splits_dict = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataset of negative examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dna non-binding #:38095\n",
      "dnabase non-binding #:38577\n",
      "dnabackbone non-binding #:38203\n",
      "rna non-binding #:38047\n",
      "rnabase non-binding #:38407\n",
      "rnabackbone non-binding #:38223\n",
      "peptide non-binding #:35437\n",
      "ion non-binding #:34488\n",
      "metabolite non-binding #:33971\n",
      "all_ligands non-binding #:27191\n"
     ]
    }
   ],
   "source": [
    "ligands_negatives_df = create_negatives_datasets(FILTER_DOMAIN, ABSOLUTE_NEGATIVES, FILTER_MAX_SCORE_ZERO, features_all, features_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Datasets of positive examples by ligand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dna #: 501\n",
      "dnabase #: 193\n",
      "dnabackbone #: 408\n",
      "rna #: 433\n",
      "rnabase #: 224\n",
      "rnabackbone #: 308\n",
      "peptide #: 1496\n",
      "ion #: 1093\n",
      "metabolite #: 1525\n"
     ]
    }
   ],
   "source": [
    "bind_th = 0.1\n",
    "ligands_features_df = create_positives_datasets(bind_th, features_all, features_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataset of positive examples - all ligands combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all_ligands #: 4518\n"
     ]
    }
   ],
   "source": [
    "all_ligands_features_df = pd.concat([ligands_features_df[\"dna\"], ligands_features_df[\"dnabase\"], ligands_features_df[\"dnabackbone\"], ligands_features_df[\"rna\"], ligands_features_df[\"rnabase\"], \n",
    "                                     ligands_features_df[\"rnabackbone\"], ligands_features_df[\"ion\"], ligands_features_df[\"peptide\"], ligands_features_df[\"metabolite\"]])\n",
    "all_ligands_features_df = all_ligands_features_df.drop_duplicates()\n",
    "print \"all_ligands #: \"+str(all_ligands_features_df.shape[0])\n",
    "ligands_features_df[\"all_ligands\"] = all_ligands_features_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading env input for downsampler technique, ligand and classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ligand = dna\n",
      "downsample_method = NoDown\n",
      "classifier_method = XGB\n"
     ]
    }
   ],
   "source": [
    "#Reading the ligand input\n",
    "try:\n",
    "    ligand = environ['ligand']\n",
    "except:\n",
    "    ligand = \"dna\"\n",
    "print \"ligand = \"+ligand\n",
    "    \n",
    "#Reading the downsampler input\n",
    "try: \n",
    "    downsample_method = environ['down']\n",
    "except:\n",
    "    downsample_method = \"NoDown\"\n",
    "print \"downsample_method = \"+downsample_method\n",
    "\n",
    "#Reading the classifier input\n",
    "try: \n",
    "    classifier_method = environ['classifier']\n",
    "except:\n",
    "    classifier_method = \"XGB\"\n",
    "print \"classifier_method = \"+classifier_method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Models tested (and their hyper-parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifiers = {}\n",
    "classifiers[\"Logistic\"] = LogisticRegression(C=0.001, random_state=0)\n",
    "classifiers[\"RF\"] = RandomForestClassifier(n_estimators=1000, n_jobs=-1, random_state=0)  \n",
    "#classifiers[\"RF\"] = RandomForestRegressor(n_estimators=1000)  \n",
    "classifiers[\"KNN\"] = KNeighborsClassifier(n_neighbors=100, n_jobs=-1)\n",
    "#classifiers[\"KNN\"] = KNeighborsRegressor(n_neighbors=100)\n",
    "classifiers[\"SVM\"] = SVC(kernel='rbf', probability=True, random_state=0)\n",
    "classifiers[\"ADA\"] = AdaBoostClassifier(n_estimators=1000, random_state=0)\n",
    "# classifiers[\"ADA-Log\"] = AdaBoostClassifier(base_estimator=classifiers[\"Logistic\"], n_estimators=1000, random_state=0)\n",
    "# classifiers[\"Bag-Log\"] = BaggingClassifier(base_estimator=classifiers[\"Logistic\"], n_estimators=1000, n_jobs=-1, random_state=0)\n",
    "#Init the XGB according to the ligands neg/pos ratio\n",
    "ligand_pos = ligands_features_df[ligand].shape[0]\n",
    "ligand_neg = ligands_negatives_df[ligand].shape[0]\n",
    "scale_weight = ligand_neg/float(ligand_pos)\n",
    "classifiers[\"XGB\"] = XGBClassifier(n_estimators=1000, n_jobs=-1, random_state=0, max_depth=6, min_child_weight=0.05, colsample_bytree=0.5, scale_pos_weight=scale_weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Downsamplers tested"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#documentation on techniques: http://contrib.scikit-learn.org/imbalanced-learn/stable/under_sampling.html#cleaning-under-sampling-techniques\n",
    "downsamplers = defaultdict(dict)\n",
    "\n",
    "##Prototype generation##\n",
    "downsamplers[\"ClusterCentroids\"] = ClusterCentroids(random_state=0)\n",
    "\n",
    "##Prototype selection##\n",
    "#Contolled#\n",
    "downsamplers[\"RandomUnderSampler\"] = RandomUnderSampler(random_state=0)\n",
    "downsamplers[\"NearMiss3\"] = NearMiss(random_state=0, version=3)\n",
    "downsamplers[\"NearMiss2\"] = NearMiss(random_state=0, version=2)\n",
    "downsamplers[\"NearMiss1\"] = NearMiss(random_state=0, version=1)\n",
    "\n",
    "#Cleaning#\n",
    "downsamplers[\"TomekLinks\"] = TomekLinks(random_state=0)\n",
    "downsamplers[\"EditedNearestNeighbours\"] = EditedNearestNeighbours(random_state=0)\n",
    "downsamplers[\"RepeatedEditedNearestNeighbours\"] = RepeatedEditedNearestNeighbours(random_state=0)\n",
    "downsamplers[\"NeighbourhoodCleaningRule\"] = NeighbourhoodCleaningRule(random_state=0)\n",
    "\n",
    "# Instance hardness threshold#\n",
    "downsamplers[\"InstanceHardnessThreshold\"][\"KNN\"] = InstanceHardnessThreshold(random_state=0, estimator=classifiers[\"KNN\"])\n",
    "#downsamplers[\"InstanceHardnessThreshold\"][\"KNN\"] = InstanceHardnessThreshold(random_state=0, estimator= KNeighborsClassifier(n_neighbors=100))\n",
    "downsamplers[\"InstanceHardnessThreshold\"][\"SVM\"] = InstanceHardnessThreshold(random_state=0, estimator=classifiers[\"SVM\"])\n",
    "downsamplers[\"InstanceHardnessThreshold\"][\"RF\"] = InstanceHardnessThreshold(random_state=0, estimator=classifiers[\"RF\"])\n",
    "#downsamplers[\"InstanceHardnessThreshold\"][\"RF\"] = InstanceHardnessThreshold(random_state=0, estimator=RandomForestClassifier(n_estimators=1000))\n",
    "downsamplers[\"InstanceHardnessThreshold\"][\"Logistic\"] = InstanceHardnessThreshold(random_state=0, estimator=classifiers[\"Logistic\"])\n",
    "downsamplers[\"InstanceHardnessThreshold\"][\"ADA\"] = InstanceHardnessThreshold(random_state=0, estimator=classifiers[\"ADA\"])\n",
    "# downsamplers[\"InstanceHardnessThreshold\"][\"ADA-Log\"] = InstanceHardnessThreshold(random_state=0, estimator=classifiers[\"ADA-Log\"])\n",
    "# downsamplers[\"InstanceHardnessThreshold\"][\"Bag-Log\"] = InstanceHardnessThreshold(random_state=0, estimator=classifiers[\"Bag-Log\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test model functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model_iterative_fixed(pred_dict, auc_dict, auprc_dict, ligand_bind_features, ligand_negatives_features, ligand_name, downsample_method, features=[]):\n",
    "    \"\"\"\n",
    "    Test different models in 10-folds cross-validation.\n",
    "    \"\"\"\n",
    "    \n",
    "    #Default: Exclude no features\n",
    "    if len(features) == 0:\n",
    "        features = np.ones([ligand_bind_features.shape[1],]).astype(bool)\n",
    "        \n",
    "    #Arranging the features table by the CV order, for each model\n",
    "    features_pred_dfs = dict.fromkeys(classifiers.keys())\n",
    "    \n",
    "    models_req_scaling = [\"SVM\", \"KNN\"]\n",
    "    \n",
    "    for classifier in classifiers.keys():\n",
    "        classifier = classifier_method\n",
    "        model = classifiers[classifier]\n",
    "        features_pred_dfs[classifier] = pd.DataFrame()\n",
    "        \n",
    "        #Create X and y with included features\n",
    "        X = pd.concat([ligand_bind_features.iloc[:,features], ligand_negatives_features.iloc[:,features]])\n",
    "        \n",
    "        if (classifier in models_req_scaling):\n",
    "            idx = X.index\n",
    "            cols = X.columns\n",
    "            X = pd.DataFrame(scale(X)) #Is z-scoring the data needed?\n",
    "            X.index = idx #Restoring indices after scaling\n",
    "            X.columns = cols\n",
    "        \n",
    "        y = [1] * ligand_bind_features.shape[0]\n",
    "        y.extend([0] * ligand_negatives_features.shape[0])\n",
    "        y = np.array(y)\n",
    "        y_df = pd.DataFrame(y)\n",
    "        y_df.index = X.index\n",
    "        y_df.columns = [\"label\"]\n",
    "\n",
    "        cv_idx = calc_CV_idx_iterative(X, splits_dict)\n",
    "\n",
    "        for k in range(len(cv_idx)):\n",
    "            pred_idx = k+1\n",
    "            print \"fold #: \"+str(pred_idx)\n",
    "            test_index = cv_idx[k][\"test\"]\n",
    "            train_index = cv_idx[k][\"train\"]\n",
    "            X_train, X_test = X.loc[train_index,:], X.loc[test_index,:]\n",
    "            y_train, y_test = y_df.loc[train_index,:], y_df.loc[test_index,:]\n",
    "\n",
    "            #Down-sample negative examples to have balanced classes\n",
    "            if (downsample_method == \"NoDown\"):\n",
    "                X_train_sampled = X_train\n",
    "                y_train_sampled = y_train\n",
    "            else:\n",
    "                if (downsample_method == \"InstanceHardnessThreshold\"):\n",
    "                    downsampler = downsamplers[downsample_method][classifier]\n",
    "                else:\n",
    "                    downsampler = downsamplers[downsample_method]\n",
    "\n",
    "                X_train_sampled, y_train_sampled = downsampler.fit_sample(X_train, y_train[\"label\"])\n",
    "            \n",
    "            #fit to training data\n",
    "            model = classifiers[classifier]\n",
    "            model.fit(X_train_sampled, y_train_sampled[\"label\"])\n",
    "            \n",
    "            #probs = model.predict(X_test)\n",
    "            #probs_list = probs\n",
    "            probs_list = []\n",
    "            probs = model.predict_proba(X_test)\n",
    "            for l in probs:\n",
    "                probs_list.append(l[1])\n",
    "                \n",
    "            pred_dict[\"obs\"].extend(y_test[\"label\"])\n",
    "            pred_dict[\"prob\"].extend(probs_list)\n",
    "            fold_list = [pred_idx] * len(probs_list)\n",
    "            pred_dict[\"fold\"].extend(fold_list)\n",
    "\n",
    "            model_list = [classifier] * len(probs_list)\n",
    "            pred_dict[\"model\"].extend(model_list)\n",
    "\n",
    "            #Update auc auprc dictionaries\n",
    "            auc_dict[classifier].append(roc_auc_score(y_test[\"label\"], probs[:, 1]))\n",
    "            precision, recall, _ = precision_recall_curve(y_test[\"label\"], probs[:, 1])\n",
    "            \n",
    "            #auc_dict[classifier].append(roc_auc_score(y_test, probs))\n",
    "            #precision, recall, _ = precision_recall_curve(y_test, probs)\n",
    "            \n",
    "            auprc_dict[classifier].append(auc(recall, precision))\n",
    "            \n",
    "            #Update features table\n",
    "            features_pred_dfs[classifier] = features_pred_dfs[classifier].append(X_test)\n",
    "            pred_idx += 1\n",
    "            \n",
    "            print \"AUC = \"+str(auc_dict[classifier][-1])\n",
    "            print \"AUPRC = \"+str(auprc_dict[classifier][-1])\n",
    "\n",
    "        avg_auc = np.sum(auc_dict[classifier])/len(cv_idx)\n",
    "        print \"avg auc = \"+str(avg_auc)\n",
    "        print \"auc var = \"+str(np.var(auc_dict[classifier]))\n",
    "        \n",
    "        avg_auprc = np.sum(auprc_dict[classifier])/len(cv_idx)\n",
    "        print \"avg auprc = \"+str(avg_auprc)\n",
    "        print \"auprc var = \"+str(np.var(auprc_dict[classifier]))\n",
    "            \n",
    "        print \"Finished \"+ligand+\" \"+classifier\n",
    "        break\n",
    "    \n",
    "    return features_pred_dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model_stratified_per_ligand(pred_dict, auc_dict, auprc_dict, ligand_bind_features, ligand_negatives_features, ligand_name, downsample_method, features=[]):\n",
    "    \"\"\"\n",
    "    Test different models in 10-folds cross-validation.\n",
    "    \"\"\"\n",
    "    \n",
    "    #Default: Exclude no features\n",
    "    if len(features) == 0:\n",
    "        features = np.ones([ligand_bind_features.shape[1],]).astype(bool)\n",
    "        \n",
    "    #Arranging the features table by the CV order, for each model\n",
    "    features_pred_dfs = dict.fromkeys(classifiers.keys())\n",
    "    \n",
    "    models_req_scaling = [\"SVM\", \"KNN\"]\n",
    "    \n",
    "    for classifier in classifiers.keys():\n",
    "        classifier = classifier_method\n",
    "        model = classifiers[classifier]\n",
    "        features_pred_dfs[classifier] = pd.DataFrame()\n",
    "        \n",
    "        #Create X and y with included features\n",
    "        X = pd.concat([ligand_bind_features.iloc[:,features], ligand_negatives_features.iloc[:,features]])\n",
    "        \n",
    "        if (classifier in models_req_scaling):\n",
    "            idx = X.index\n",
    "            cols = X.columns\n",
    "            X = pd.DataFrame(scale(X)) #Is z-scoring the data needed?\n",
    "            X.index = idx #Restoring indices after scaling\n",
    "            X.columns = cols\n",
    "\n",
    "        y = [1] * ligand_bind_features.shape[0]\n",
    "        y.extend([0] * ligand_negatives_features.shape[0])\n",
    "        y = np.array(y)\n",
    "\n",
    "        binding_skf = StratifiedKFold(n_splits=10, shuffle=True, random_state=0)\n",
    "        pred_idx = 1\n",
    "\n",
    "        for train_index, test_index in binding_skf.split(X, y):\n",
    "            print \"fold #: \"+str(pred_idx)\n",
    "            X_train, X_test = X.iloc[train_index,:], X.iloc[test_index,:]\n",
    "            y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "            #Down-sample negative examples to have balanced classes\n",
    "            if (downsample_method == \"NoDown\"):\n",
    "                X_train_sampled = X_train\n",
    "                y_train_sampled = y_train\n",
    "            else:\n",
    "                if (downsample_method == \"InstanceHardnessThreshold\"):\n",
    "                    downsampler = downsamplers[downsample_method][classifier]\n",
    "                else:\n",
    "                    downsampler = downsamplers[downsample_method]\n",
    "\n",
    "                X_train_sampled, y_train_sampled = downsampler.fit_sample(X_train, y_train)\n",
    "            \n",
    "            #fit to training data\n",
    "            model = classifiers[classifier]\n",
    "            model.fit(X_train_sampled, y_train_sampled)\n",
    "    \n",
    "\n",
    "            #probs = model.predict(X_test)\n",
    "            #probs_list = probs\n",
    "            probs_list = []\n",
    "            probs = model.predict_proba(X_test)\n",
    "            for l in probs:\n",
    "                probs_list.append(l[1])\n",
    "                \n",
    "            pred_dict[\"obs\"].extend(y_test)\n",
    "            pred_dict[\"prob\"].extend(probs_list)\n",
    "            fold_list = [pred_idx] * len(probs_list)\n",
    "            pred_dict[\"fold\"].extend(fold_list)\n",
    "\n",
    "            model_list = [classifier] * len(probs_list)\n",
    "            pred_dict[\"model\"].extend(model_list)\n",
    "\n",
    "            #Update auc auprc dictionaries\n",
    "            auc_dict[classifier].append(roc_auc_score(y_test, probs[:, 1]))\n",
    "            precision, recall, _ = precision_recall_curve(y_test, probs[:, 1])\n",
    "            \n",
    "            #auc_dict[classifier].append(roc_auc_score(y_test, probs))\n",
    "            #precision, recall, _ = precision_recall_curve(y_test, probs)\n",
    "            \n",
    "            auprc_dict[classifier].append(auc(recall, precision))\n",
    "            \n",
    "            #Update features table\n",
    "            features_pred_dfs[classifier] = features_pred_dfs[classifier].append(X_test)\n",
    "            pred_idx += 1\n",
    "            \n",
    "            print \"AUC = \"+str(auc_dict[classifier][-1])\n",
    "            print \"AUPRC = \"+str(auprc_dict[classifier][-1])\n",
    "\n",
    "        avg_auc = np.sum(auc_dict[classifier])/10.0\n",
    "        print \"avg auc = \"+str(avg_auc)\n",
    "        \n",
    "        avg_auprc = np.sum(auprc_dict[classifier])/10.0\n",
    "        print \"avg auprc = \"+str(avg_auprc)\n",
    "            \n",
    "        print \"Finished \"+ligand+\" \"+classifier\n",
    "        break\n",
    "    \n",
    "    return features_pred_dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_features_predictions(ligand, ordered_features, pred_df):\n",
    "    \n",
    "    pred_res = pred_df.copy(deep=True)\n",
    "    for classifier in classifiers.keys():\n",
    "        classifier = classifier_method\n",
    "        model_pred = pred_res[pred_res[\"model\"] == classifier]\n",
    "        model_pred.index = ordered_features[classifier].index\n",
    "        \n",
    "        #Creating the combined table\n",
    "        features_pred = pd.concat([ordered_features[classifier], model_pred], axis=1)\n",
    "        \n",
    "        #Saving\n",
    "        features_pred.to_csv(curr_dir[0]+\"/pred_AUC_AUPRC/\"+out_dir+\"/\"+downsample_method+\"/01.25.2018_domain_CV/features_pred_tables/\"+ligand+\"_\"+classifier+\"_features_pred.csv\", sep='\\t')\n",
    "        \n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predict for each ligand seperatelly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold #: 1\n",
      "AUC = 0.850064837905\n",
      "AUPRC = 0.560776630586\n",
      "fold #: 2\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-a6341e2d6627>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mget_ipython\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_cell_magic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mu'time'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34mu''\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34mu'    \\nligand_pos = ligands_features_df[ligand].shape[0]\\nligand_neg = ligands_negatives_df[ligand].shape[0]\\nscale_weight = ligand_neg/float(ligand_pos)\\nclassifiers[\"XGB\"] = XGBClassifier(n_estimators=1000, n_jobs=-1, random_state=0, max_depth=6, min_child_weight=0.05, colsample_bytree=0.5, scale_pos_weight=scale_weight)\\n\\n#Initialize dictionary\\npred_dict = defaultdict(list)\\nauc_dict = defaultdict(list)\\nauprc_dict = defaultdict(list)\\n\\nordered_features = test_model_iterative_fixed(pred_dict, auc_dict, auprc_dict, ligands_features_df[ligand], ligands_negatives_df[ligand], ligand, downsample_method)\\n\\npred_df = pd.DataFrame.from_dict(pred_dict)\\nauc_df = pd.DataFrame.from_dict(auc_dict)\\nauprc_df = pd.DataFrame.from_dict(auprc_dict)\\n\\n#Save to file\\npred_df.to_csv(curr_dir[0]+\"/pred_AUC_AUPRC/\"+out_dir+\"/\"+downsample_method+\"/01.25.2018_domain_CV/\"+ligand+\"_\"+classifier_method+\"_10w.csv\", sep=\\'\\\\t\\')\\nauc_df.to_csv(curr_dir[0]+\"/pred_AUC_AUPRC/\"+out_dir+\"/\"+downsample_method+\"/01.25.2018_domain_CV/\"+ligand+\"_\"+classifier_method+\"_10w_auc.csv\", sep=\\'\\\\t\\')\\nauprc_df.to_csv(curr_dir[0]+\"/pred_AUC_AUPRC/\"+out_dir+\"/\"+downsample_method+\"/01.25.2018_domain_CV/\"+ligand+\"_\"+classifier_method+\"_10w_auprc.csv\", sep=\\'\\\\t\\')\\n\\n#Combine features and pred results to a unified table\\n#combine_features_predictions(ligand, ordered_features, pred_df)\\n\\nprint \"Finished ligand \"+ligand'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m/home/anat/anaconda2/lib/python2.7/site-packages/IPython/core/interactiveshell.pyc\u001b[0m in \u001b[0;36mrun_cell_magic\u001b[1;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[0;32m   2118\u001b[0m             \u001b[0mmagic_arg_s\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvar_expand\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mline\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstack_depth\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2119\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2120\u001b[1;33m                 \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmagic_arg_s\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2121\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2122\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<decorator-gen-60>\u001b[0m in \u001b[0;36mtime\u001b[1;34m(self, line, cell, local_ns)\u001b[0m\n",
      "\u001b[1;32m/home/anat/anaconda2/lib/python2.7/site-packages/IPython/core/magic.pyc\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(f, *a, **k)\u001b[0m\n\u001b[0;32m    191\u001b[0m     \u001b[1;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    192\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 193\u001b[1;33m         \u001b[0mcall\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    194\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    195\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/anat/anaconda2/lib/python2.7/site-packages/IPython/core/magics/execution.pyc\u001b[0m in \u001b[0;36mtime\u001b[1;34m(self, line, cell, local_ns)\u001b[0m\n\u001b[0;32m   1175\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1176\u001b[0m             \u001b[0mst\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1177\u001b[1;33m             \u001b[1;32mexec\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mglob\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlocal_ns\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1178\u001b[0m             \u001b[0mend\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1179\u001b[0m             \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m<ipython-input-9-75e7c8546b49>\u001b[0m in \u001b[0;36mtest_model_iterative_fixed\u001b[1;34m(pred_dict, auc_dict, auprc_dict, ligand_bind_features, ligand_negatives_features, ligand_name, downsample_method, features)\u001b[0m\n\u001b[0;32m     59\u001b[0m             \u001b[1;31m#fit to training data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     60\u001b[0m             \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclassifiers\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mclassifier\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 61\u001b[1;33m             \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train_sampled\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train_sampled\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"label\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     62\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m             \u001b[1;31m#probs = model.predict(X_test)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/anat/anaconda2/lib/python2.7/site-packages/xgboost/sklearn.pyc\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight, eval_set, eval_metric, early_stopping_rounds, verbose, xgb_model)\u001b[0m\n\u001b[0;32m    504\u001b[0m                               \u001b[0mearly_stopping_rounds\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mearly_stopping_rounds\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    505\u001b[0m                               \u001b[0mevals_result\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mevals_result\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeval\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfeval\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 506\u001b[1;33m                               verbose_eval=verbose, xgb_model=None)\n\u001b[0m\u001b[0;32m    507\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    508\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mobjective\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mxgb_options\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"objective\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/anat/anaconda2/lib/python2.7/site-packages/xgboost/training.pyc\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(params, dtrain, num_boost_round, evals, obj, feval, maximize, early_stopping_rounds, evals_result, verbose_eval, xgb_model, callbacks, learning_rates)\u001b[0m\n\u001b[0;32m    202\u001b[0m                            \u001b[0mevals\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mevals\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    203\u001b[0m                            \u001b[0mobj\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeval\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfeval\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 204\u001b[1;33m                            xgb_model=xgb_model, callbacks=callbacks)\n\u001b[0m\u001b[0;32m    205\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    206\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/anat/anaconda2/lib/python2.7/site-packages/xgboost/training.pyc\u001b[0m in \u001b[0;36m_train_internal\u001b[1;34m(params, dtrain, num_boost_round, evals, obj, feval, xgb_model, callbacks)\u001b[0m\n\u001b[0;32m     72\u001b[0m         \u001b[1;31m# Skip the first update if it is a recovery step.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     73\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mversion\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;36m2\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 74\u001b[1;33m             \u001b[0mbst\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     75\u001b[0m             \u001b[0mbst\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave_rabit_checkpoint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     76\u001b[0m             \u001b[0mversion\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/anat/anaconda2/lib/python2.7/site-packages/xgboost/core.pyc\u001b[0m in \u001b[0;36mupdate\u001b[1;34m(self, dtrain, iteration, fobj)\u001b[0m\n\u001b[0;32m    896\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mfobj\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    897\u001b[0m             _check_call(_LIB.XGBoosterUpdateOneIter(self.handle, ctypes.c_int(iteration),\n\u001b[1;32m--> 898\u001b[1;33m                                                     dtrain.handle))\n\u001b[0m\u001b[0;32m    899\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    900\u001b[0m             \u001b[0mpred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdtrain\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "    \n",
    "ligand_pos = ligands_features_df[ligand].shape[0]\n",
    "ligand_neg = ligands_negatives_df[ligand].shape[0]\n",
    "scale_weight = ligand_neg/float(ligand_pos)\n",
    "classifiers[\"XGB\"] = XGBClassifier(n_estimators=1000, n_jobs=-1, random_state=0, max_depth=6, min_child_weight=0.05, colsample_bytree=0.5, scale_pos_weight=scale_weight)\n",
    "\n",
    "#Initialize dictionary\n",
    "pred_dict = defaultdict(list)\n",
    "auc_dict = defaultdict(list)\n",
    "auprc_dict = defaultdict(list)\n",
    "\n",
    "ordered_features = test_model_iterative_fixed(pred_dict, auc_dict, auprc_dict, ligands_features_df[ligand], ligands_negatives_df[ligand], ligand, downsample_method)\n",
    "\n",
    "pred_df = pd.DataFrame.from_dict(pred_dict)\n",
    "auc_df = pd.DataFrame.from_dict(auc_dict)\n",
    "auprc_df = pd.DataFrame.from_dict(auprc_dict)\n",
    "\n",
    "#Save to file\n",
    "pred_df.to_csv(curr_dir[0]+\"/pred_AUC_AUPRC/\"+out_dir+\"/\"+downsample_method+\"/01.25.2018_domain_CV/\"+ligand+\"_\"+classifier_method+\"_10w.csv\", sep='\\t')\n",
    "auc_df.to_csv(curr_dir[0]+\"/pred_AUC_AUPRC/\"+out_dir+\"/\"+downsample_method+\"/01.25.2018_domain_CV/\"+ligand+\"_\"+classifier_method+\"_10w_auc.csv\", sep='\\t')\n",
    "auprc_df.to_csv(curr_dir[0]+\"/pred_AUC_AUPRC/\"+out_dir+\"/\"+downsample_method+\"/01.25.2018_domain_CV/\"+ligand+\"_\"+classifier_method+\"_10w_auprc.csv\", sep='\\t')\n",
    "\n",
    "#Combine features and pred results to a unified table\n",
    "#combine_features_predictions(ligand, ordered_features, pred_df)\n",
    "\n",
    "print \"Finished ligand \"+ligand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
