{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Basic imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from collections import defaultdict\n",
    "from os import environ\n",
    "\n",
    "#Classifier imports\n",
    "from sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, RandomForestRegressor, BaggingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "#ML framework imports\n",
    "from sklearn.metrics import auc, roc_auc_score, precision_recall_curve\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import scale\n",
    "\n",
    "#Downsamplers imports - prototype generation\n",
    "from imblearn.under_sampling import ClusterCentroids\n",
    "\n",
    "#Downsamplers imports - prototype selection - controlled\n",
    "from imblearn.under_sampling import RandomUnderSampler, NearMiss\n",
    "\n",
    "#Downsamplers imports - prototype selection - Cleaning techniques\n",
    "from imblearn.under_sampling import TomekLinks, EditedNearestNeighbours, RepeatedEditedNearestNeighbours\n",
    "\n",
    "#Downsamplers imports - prototype selection - Cleaning techniques - Condensed nearest neighbors and derived algorithms\n",
    "from imblearn.under_sampling import CondensedNearestNeighbour, OneSidedSelection, NeighbourhoodCleaningRule\n",
    "\n",
    "#Downsamplers imports - prototype selection - Cleaning techniques\n",
    "from imblearn.under_sampling import InstanceHardnessThreshold\n",
    "\n",
    "from IPython.core.display import HTML\n",
    "HTML(\"<style>.container { width:100% !important; }</style>\")\n",
    "\n",
    "ABSOLUTE_NEGATIVES = False\n",
    "FILTER_DOMAIN = False\n",
    "FILTER_MAX_SCORE_ZERO = False\n",
    "K=10\n",
    "out_dir = \"mediode_stacking\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading the input dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all samples positions #: 38944\n"
     ]
    }
   ],
   "source": [
    "curr_dir = !pwd\n",
    "input_path = curr_dir[0]+\"/../domains_similarity/filtered_features_table/\"\n",
    "filename = \"positions_features_mediode_filter_01.25.18.csv\"\n",
    "\n",
    "bind_scores_num = 10\n",
    "\n",
    "#Features table\n",
    "features_all = pd.read_csv(input_path+filename, sep='\\t', index_col=0)\n",
    "features_cols = features_all.columns[1:-bind_scores_num] #removing binding scores and domain name\n",
    "print \"all samples positions #: \"+str(features_all.shape[0])\n",
    "\n",
    "#Constants\n",
    "ligands = [\"dna\", \"dnabase\", \"dnabackbone\", \"rna\", \"rnabase\", \"rnabackbone\", \"peptide\", \"ion\", \"metabolite\"]\n",
    "models = [\"XGB\", \"RF\", \"ADA\", \"KNN\", \"SVM\", \"Logistic\"]\n",
    "\n",
    "#lignd binding domains dictionary\n",
    "with open(curr_dir[0]+\"/../ligands_negatives_domains_dict.pik\", 'rb') as handle:\n",
    "        negatives_dict = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataset of negative examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_to_ligand_binding_domains(use_max_binding_score):\n",
    "    \n",
    "    ligands_negatives_df = {}\n",
    "    for ligand in ligands:\n",
    "        \n",
    "        ligands_negatives_df[ligand] = pd.DataFrame()\n",
    "        for domain in negatives_dict[ligand].keys():\n",
    "            if domain == 'negatives' or domain == 'domains':\n",
    "                continue\n",
    "            domain_all = features_all.loc[features_all.loc[:,\"domain_name\"] == domain,:]\n",
    "            \n",
    "            #In case this domain was previously filtered\n",
    "            if len(domain_all) == 0:\n",
    "                continue\n",
    "            \n",
    "            if (use_max_binding_score):\n",
    "                ligands_negatives_df[ligand] = pd.concat([ligands_negatives_df[ligand],domain_all.loc[domain_all.loc[:,\"max_binding_score\"] == 0,:]])\n",
    "            else:\n",
    "                ligand_bind_str = ligand+\"_binding_score\"\n",
    "                ligands_negatives_df[ligand] = pd.concat([ligands_negatives_df[ligand],domain_all.loc[domain_all.loc[:,ligand_bind_str] == 0,:]])\n",
    "        \n",
    "    #Handeling the ligand \"all_ligands\"\n",
    "    all_ligands_negatives_df = pd.concat([ligands_negatives_df[\"dna\"], ligands_negatives_df[\"dnabase\"], ligands_negatives_df[\"dnabackbone\"], ligands_negatives_df[\"rna\"], ligands_negatives_df[\"rnabase\"], \n",
    "                                 ligands_negatives_df[\"rnabackbone\"], ligands_negatives_df[\"ion\"], ligands_negatives_df[\"peptide\"], ligands_negatives_df[\"metabolite\"]])\n",
    "    all_ligands_negatives_df = all_ligands_negatives_df.drop_duplicates()\n",
    "    #Filter to just positions with max. binding score = 0\n",
    "    all_ligands_negatives_df = all_ligands_negatives_df[all_ligands_negatives_df[\"max_binding_score\"] == 0]\n",
    "    ligands_negatives_df[\"all_ligands\"] = all_ligands_negatives_df\n",
    "    \n",
    "    #Leaving just the features columns\n",
    "    for ligand in ligands_negatives_df.keys():   \n",
    "        ligands_negatives_df[ligand] = ligands_negatives_df[ligand][features_cols]\n",
    "        print(ligand+\" non-binding #:\"+str(len(ligands_negatives_df[ligand])))\n",
    "    \n",
    "    return ligands_negatives_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def negatives_by_binding_score(use_max_binding_score, filter_max_score_zero):\n",
    "    \n",
    "    ligands_negatives_df = {}\n",
    "    for ligand in ligands:\n",
    "        \n",
    "        if use_max_binding_score:\n",
    "            ligand_bind_str = \"max_binding_score\"\n",
    "        else:\n",
    "            ligand_bind_str = ligand+\"_binding_score\"\n",
    "        \n",
    "        if (filter_max_score_zero):\n",
    "            ligands_negatives_df[ligand] = features_all[features_all[ligand_bind_str] == 0][features_all[\"max_binding_score\"] != 0]\n",
    "        else:\n",
    "            ligands_negatives_df[ligand] = features_all[features_all[ligand_bind_str] == 0]\n",
    "        ligands_negatives_df[ligand] = ligands_negatives_df[ligand].loc[:,features_cols]\n",
    "        print(ligand+\" non-binding #:\"+str(len(ligands_negatives_df[ligand])))\n",
    "        \n",
    "    #Handeling the ligand \"all_ligands\"\n",
    "    ligands_negatives_df[\"all_ligands\"] = features_all[features_all[\"max_binding_score\"] == 0]\n",
    "    ligands_negatives_df[\"all_ligands\"] = ligands_negatives_df[\"all_ligands\"].loc[:,features_cols]\n",
    "    print(\"all_ligands non-binding #:\"+str(len(ligands_negatives_df[\"all_ligands\"])))\n",
    "    \n",
    "    return ligands_negatives_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dna non-binding #:38095\n",
      "dnabase non-binding #:38577\n",
      "dnabackbone non-binding #:38203\n",
      "rna non-binding #:38047\n",
      "rnabase non-binding #:38407\n",
      "rnabackbone non-binding #:38223\n",
      "peptide non-binding #:35437\n",
      "ion non-binding #:34488\n",
      "metabolite non-binding #:33971\n",
      "all_ligands non-binding #:27191\n"
     ]
    }
   ],
   "source": [
    "#Create negatives datasets\n",
    "if FILTER_DOMAIN:\n",
    "    if ABSOLUTE_NEGATIVES:\n",
    "        ligands_negatives_df = filter_to_ligand_binding_domains(True)\n",
    "    else:\n",
    "        ligands_negatives_df = filter_to_ligand_binding_domains(False)\n",
    "else:\n",
    "    if ABSOLUTE_NEGATIVES:\n",
    "        ligands_negatives_df = negatives_by_binding_score(True, FILTER_MAX_SCORE_ZERO)\n",
    "    else:\n",
    "        ligands_negatives_df = negatives_by_binding_score(False, FILTER_MAX_SCORE_ZERO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Datasets of positive examples by ligand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dna #: 501\n",
      "dnabase #: 193\n",
      "dnabackbone #: 408\n",
      "rna #: 433\n",
      "rnabase #: 224\n",
      "rnabackbone #: 308\n",
      "peptide #: 1496\n",
      "ion #: 1093\n",
      "metabolite #: 1525\n"
     ]
    }
   ],
   "source": [
    "bind_th = 0.1\n",
    "ligands_features_df = {}\n",
    "\n",
    "for ligand in ligands:\n",
    "    score_col_str = ligand+\"_binding_score\"\n",
    "    ligand_binding_df = features_all[features_all[score_col_str] >= bind_th]\n",
    "    print ligand+\" #: \"+str(ligand_binding_df.shape[0])\n",
    "    ligands_features_df[ligand] = ligand_binding_df.loc[:,features_cols]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataset of positive examples - all ligands combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all_ligands #: 4518\n"
     ]
    }
   ],
   "source": [
    "all_ligands_features_df = pd.concat([ligands_features_df[\"dna\"], ligands_features_df[\"dnabase\"], ligands_features_df[\"dnabackbone\"], ligands_features_df[\"rna\"], ligands_features_df[\"rnabase\"], \n",
    "                                     ligands_features_df[\"rnabackbone\"], ligands_features_df[\"ion\"], ligands_features_df[\"peptide\"], ligands_features_df[\"metabolite\"]])\n",
    "all_ligands_features_df = all_ligands_features_df.drop_duplicates()\n",
    "print \"all_ligands #: \"+str(all_ligands_features_df.shape[0])\n",
    "ligands_features_df[\"all_ligands\"] = all_ligands_features_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Models tested (and their hyper-parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifiers = {}\n",
    "classifiers[\"Logistic\"] = LogisticRegression(C=0.001, random_state=0)\n",
    "classifiers[\"RF\"] = RandomForestClassifier(n_estimators=1000, n_jobs=-1, random_state=0)  \n",
    "#classifiers[\"RF\"] = RandomForestRegressor(n_estimators=1000)  \n",
    "classifiers[\"KNN\"] = KNeighborsClassifier(n_neighbors=100, n_jobs=-1)\n",
    "#classifiers[\"KNN\"] = KNeighborsRegressor(n_neighbors=100)\n",
    "classifiers[\"SVM\"] = SVC(kernel='rbf', probability=True, random_state=0)\n",
    "classifiers[\"ADA\"] = AdaBoostClassifier(n_estimators=1000, random_state=0)\n",
    "classifiers[\"ADA-Log\"] = AdaBoostClassifier(base_estimator=classifiers[\"Logistic\"], n_estimators=1000, random_state=0)\n",
    "classifiers[\"Bag-Log\"] = BaggingClassifier(base_estimator=classifiers[\"Logistic\"], n_estimators=1000, n_jobs=-1, random_state=0)\n",
    "classifiers[\"XGB\"] = XGBClassifier(n_estimators=1000, n_jobs=-1, random_state=0, max_depth=6, min_child_weight=0.05, colsample_bytree=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Downsamplers tested"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#documentation on techniques: http://contrib.scikit-learn.org/imbalanced-learn/stable/under_sampling.html#cleaning-under-sampling-techniques\n",
    "downsamplers = defaultdict(dict)\n",
    "\n",
    "##Prototype generation##\n",
    "downsamplers[\"ClusterCentroids\"] = ClusterCentroids(random_state=0)\n",
    "\n",
    "##Prototype selection##\n",
    "#Contolled#\n",
    "downsamplers[\"RandomUnderSampler\"] = RandomUnderSampler(random_state=0)\n",
    "downsamplers[\"NearMiss3\"] = NearMiss(random_state=0, version=3)\n",
    "downsamplers[\"NearMiss2\"] = NearMiss(random_state=0, version=2)\n",
    "downsamplers[\"NearMiss1\"] = NearMiss(random_state=0, version=1)\n",
    "\n",
    "#Cleaning#\n",
    "downsamplers[\"TomekLinks\"] = TomekLinks(random_state=0)\n",
    "downsamplers[\"EditedNearestNeighbours\"] = EditedNearestNeighbours(random_state=0)\n",
    "downsamplers[\"RepeatedEditedNearestNeighbours\"] = RepeatedEditedNearestNeighbours(random_state=0)\n",
    "downsamplers[\"NeighbourhoodCleaningRule\"] = NeighbourhoodCleaningRule(random_state=0)\n",
    "\n",
    "# Instance hardness threshold#\n",
    "downsamplers[\"InstanceHardnessThreshold\"][\"KNN\"] = InstanceHardnessThreshold(random_state=0, estimator=classifiers[\"KNN\"])\n",
    "#downsamplers[\"InstanceHardnessThreshold\"][\"KNN\"] = InstanceHardnessThreshold(random_state=0, estimator= KNeighborsClassifier(n_neighbors=100))\n",
    "downsamplers[\"InstanceHardnessThreshold\"][\"SVM\"] = InstanceHardnessThreshold(random_state=0, estimator=classifiers[\"SVM\"])\n",
    "downsamplers[\"InstanceHardnessThreshold\"][\"RF\"] = InstanceHardnessThreshold(random_state=0, estimator=classifiers[\"RF\"])\n",
    "#downsamplers[\"InstanceHardnessThreshold\"][\"RF\"] = InstanceHardnessThreshold(random_state=0, estimator=RandomForestClassifier(n_estimators=1000))\n",
    "downsamplers[\"InstanceHardnessThreshold\"][\"Logistic\"] = InstanceHardnessThreshold(random_state=0, estimator=classifiers[\"Logistic\"])\n",
    "downsamplers[\"InstanceHardnessThreshold\"][\"ADA\"] = InstanceHardnessThreshold(random_state=0, estimator=classifiers[\"ADA\"])\n",
    "downsamplers[\"InstanceHardnessThreshold\"][\"ADA-Log\"] = InstanceHardnessThreshold(random_state=0, estimator=classifiers[\"ADA-Log\"])\n",
    "downsamplers[\"InstanceHardnessThreshold\"][\"Bag-Log\"] = InstanceHardnessThreshold(random_state=0, estimator=classifiers[\"Bag-Log\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading env input for downsampler technique, ligand and classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ligand = dnabase\n",
      "downsample_method = RandomUnderSampler\n",
      "classifier_method = Logistic\n"
     ]
    }
   ],
   "source": [
    "#Reading the ligand input\n",
    "try:\n",
    "    ligand = environ['ligand']\n",
    "except:\n",
    "    ligand = \"dnabase\"\n",
    "print \"ligand = \"+ligand\n",
    "    \n",
    "#Reading the downsampler input\n",
    "try: \n",
    "    downsample_method = environ['down']\n",
    "except:\n",
    "    downsample_method = \"RandomUnderSampler\"\n",
    "print \"downsample_method = \"+downsample_method\n",
    "\n",
    "#Reading the classifier input\n",
    "try: \n",
    "    classifier_method = environ['classifier']\n",
    "except:\n",
    "    classifier_method = \"Logistic\"\n",
    "print \"classifier_method = \"+classifier_method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create out-of-fold 1st level predictions: one for each test-train split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_predict_auc(X_train, y_train, X_test, y_test, downsample_method, pred_dict, pred_idx, auc_dict, auprc_dict, features_pred_dfs):\n",
    "    \n",
    "    #Downsampling according to \"downsample_method\"\n",
    "    if (downsample_method == \"NoDown\"):\n",
    "        X_train_sampled = X_train\n",
    "        y_train_sampled = y_train\n",
    "    else:\n",
    "        if (downsample_method == \"InstanceHardnessThreshold\"):\n",
    "            downsampler = downsamplers[downsample_method][classifier_method]\n",
    "        else:\n",
    "            downsampler = downsamplers[downsample_method]\n",
    "\n",
    "        X_train_sampled, y_train_sampled = downsampler.fit_sample(X_train, y_train)\n",
    "        \n",
    "    #fit to training data\n",
    "    model = classifiers[classifier_method]\n",
    "    model.fit(X_train_sampled, y_train_sampled)\n",
    "    \n",
    "    #predict for the test data\n",
    "    probs = model.predict_proba(X_test)\n",
    "    probs_list = []\n",
    "    for l in probs:\n",
    "        probs_list.append(l[1])\n",
    "        \n",
    "    #Save predcition results for plotting\n",
    "    pred_dict[\"obs\"].extend(y_test)\n",
    "    pred_dict[\"prob\"].extend(probs_list)\n",
    "    fold_list = [pred_idx] * len(probs_list)\n",
    "    pred_dict[\"fold\"].extend(fold_list)\n",
    "    model_list = [classifier_method] * len(probs_list)\n",
    "    pred_dict[\"model\"].extend(model_list)\n",
    "\n",
    "    #Update auc auprc dictionaries\n",
    "    auc_dict[classifier_method].append(roc_auc_score(y_test, probs[:, 1]))\n",
    "    precision, recall, _ = precision_recall_curve(y_test, probs[:, 1])\n",
    "    auprc_dict[classifier_method].append(auc(recall, precision))\n",
    "    print \"AUC = \"+str(auc_dict[classifier_method][-1])\n",
    "    print \"AUPRC = \"+str(auprc_dict[classifier_method][-1])\n",
    "\n",
    "    #Update features table\n",
    "    features_pred_dfs[classifier_method] = features_pred_dfs[classifier_method].append(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_features_predictions(ligand, ordered_features, pred_df, i):\n",
    "    \n",
    "    pred_res = pred_df.copy(deep=True)\n",
    "    \n",
    "    model_pred = pred_res[pred_res[\"model\"] == classifier_method]\n",
    "    model_pred.index = ordered_features[classifier_method].index\n",
    "\n",
    "    #Creating the combined table\n",
    "    features_pred = pd.concat([ordered_features[classifier_method], model_pred], axis=1)\n",
    "\n",
    "    #Saving\n",
    "    features_pred.to_csv(curr_dir[0]+\"/1st_level_pred/\"+str(i+1)+\"/\"+ligand+\"_\"+classifier_method+\"_fold\"+str(i+1)+\"_features_pred.csv\", sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(ligand_bind_features, ligand_negatives_features, ligand_name, downsample_method, features=[]):\n",
    "    \"\"\"\n",
    "    compute 1st level stacking predictions from \"out-of-fold\" data, 10-folds CV\n",
    "    \"\"\"\n",
    "    \n",
    "    #Default: Exclude no features\n",
    "    if len(features) == 0:\n",
    "        features = np.ones([ligand_bind_features.shape[1],]).astype(bool)\n",
    "    features_pred_dfs = dict.fromkeys(classifiers.keys())\n",
    "    models_req_scaling = [\"SVM\", \"KNN\"]\n",
    "    \n",
    "    #Create X and y with included features\n",
    "    X = pd.concat([ligand_bind_features.iloc[:,features], ligand_negatives_features.iloc[:,features]])\n",
    "\n",
    "    if (classifier_method in models_req_scaling):\n",
    "        idx = X.index\n",
    "        cols = X.columns\n",
    "        X = pd.DataFrame(scale(X)) #Is z-scoring the data needed?\n",
    "        X.index = idx #Restoring indices after scaling\n",
    "        X.columns = cols\n",
    "\n",
    "    y = [1] * ligand_bind_features.shape[0]\n",
    "    y.extend([0] * ligand_negatives_features.shape[0])\n",
    "    y = np.array(y)\n",
    "\n",
    "    #Create the external CV indices that defines the held-out set\n",
    "    ext_cv_idx = []\n",
    "    external_skf = StratifiedKFold(n_splits=K, shuffle=True, random_state=0)\n",
    "    for ext_train_index, heldout_index in external_skf.split(X, y):\n",
    "        ext_cv_idx.append({\"train\": ext_train_index, \"test\": heldout_index})\n",
    "    \n",
    "    #For each heldout set, create out-of-fold 1st prediction results and save them\n",
    "    for i in range(K):\n",
    "        print \"ext fold #\"+str(i+1)\n",
    "        ext_train_index = ext_cv_idx[i][\"train\"]\n",
    "        heldout_index = ext_cv_idx[i][\"test\"]\n",
    "        X_ext_train, X_heldout = X.iloc[ext_train_index,:], X.iloc[heldout_index,:]\n",
    "        y_ext_train, y_heldout = y[ext_train_index], y[heldout_index]\n",
    "        \n",
    "        #Initialize dictionaries and features df\n",
    "        pred_dict = defaultdict(list)\n",
    "        auc_dict = defaultdict(list)\n",
    "        auprc_dict = defaultdict(list)\n",
    "        features_pred_dfs[classifier_method] = pd.DataFrame()\n",
    "\n",
    "        #internal CV on all folds except fold #i\n",
    "        pred_idx = 1\n",
    "        for j in range(K):\n",
    "            print \"int fold #\"+str(j+1)\n",
    "            if (j == i):\n",
    "                continue\n",
    "            int_test_index = ext_cv_idx[j][\"test\"]\n",
    "            int_train_index = np.array([x for x in ext_train_index if x not in int_test_index]) #all indices in ext train that aren't in current test\n",
    "            X_int_train, X_int_test = X.iloc[int_train_index,:], X.iloc[int_test_index,:]\n",
    "            y_int_train, y_int_test = y[int_train_index], y[int_test_index]\n",
    "\n",
    "            #fit to training data: (K-2) folds, and predict for the remaining fold\n",
    "            train_predict_auc(X_int_train, y_int_train, X_int_test, y_int_test, downsample_method, pred_dict, pred_idx, auc_dict, auprc_dict, features_pred_dfs)\n",
    "            pred_idx += 1\n",
    "\n",
    "        #print averages across internal CV\n",
    "        avg_auc = np.sum(auc_dict[classifier_method])/(K-1)\n",
    "        print \"avg int auc = \"+str(avg_auc)\n",
    "\n",
    "        avg_auprc = np.sum(auprc_dict[classifier_method])/(K-1)\n",
    "        print \"avg int auprc = \"+str(avg_auprc)\n",
    "\n",
    "        #Fit to all K-1 folds to predict 1st level on the heldout\n",
    "        print \"heldout fold #\"+str(i+1)\n",
    "        train_predict_auc(X_ext_train, y_ext_train, X_heldout, y_heldout, downsample_method, pred_dict, pred_idx, auc_dict, auprc_dict, features_pred_dfs)\n",
    "        \n",
    "        #Save auc,auprc,prob to file\n",
    "        pred_df = pd.DataFrame.from_dict(pred_dict)\n",
    "        auc_df = pd.DataFrame.from_dict(auc_dict)\n",
    "        auprc_df = pd.DataFrame.from_dict(auprc_dict)\n",
    "        pred_df.to_csv(curr_dir[0]+\"/1st_level_pred/\"+str(i+1)+\"/\"+ligand+\"_\"+classifier_method+\"_fold\"+str(i+1)+\"_0.1.csv\", sep='\\t')\n",
    "        auc_df.to_csv(curr_dir[0]+\"/1st_level_pred/\"+str(i+1)+\"/\"+ligand+\"_\"+classifier_method+\"_fold\"+str(i+1)+\"_0.1_auc.csv\", sep='\\t')\n",
    "        auprc_df.to_csv(curr_dir[0]+\"/1st_level_pred/\"+str(i+1)+\"/\"+ligand+\"_\"+classifier_method+\"_fold\"+str(i+1)+\"_0.1_auprc.csv\", sep='\\t')\n",
    "\n",
    "        #Save 1st level predictions to file, one file for each heldout\n",
    "        combine_features_predictions(ligand, features_pred_dfs, pred_df, i)\n",
    "        \n",
    "        print \"Finished ext fold #\"+str(i+1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test model functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ext fold #1\n",
      "int fold #1\n",
      "int fold #2\n",
      "AUC = 0.671632545932\n",
      "AUPRC = 0.0826057737086\n",
      "int fold #3\n",
      "AUC = 0.65811023622\n",
      "AUPRC = 0.0672875065179\n",
      "int fold #4\n",
      "AUC = 0.598572178478\n",
      "AUPRC = 0.042870876984\n",
      "int fold #5\n",
      "AUC = 0.545742782152\n",
      "AUPRC = 0.0169680692455\n",
      "int fold #6\n",
      "AUC = 0.625954318719\n",
      "AUPRC = 0.0437931227421\n",
      "int fold #7\n",
      "AUC = 0.502614859543\n",
      "AUPRC = 0.0195409413828\n",
      "int fold #8\n",
      "AUC = 0.538241008139\n",
      "AUPRC = 0.0139406273237\n",
      "int fold #9\n",
      "AUC = 0.652118666317\n",
      "AUPRC = 0.080789884169\n",
      "int fold #10\n",
      "AUC = 0.641008138619\n",
      "AUPRC = 0.0421641357006\n",
      "avg int auc = 0.60377719268\n",
      "avg int auprc = 0.0455512153083\n",
      "heldout fold #1\n",
      "AUC = 0.486542123411\n",
      "AUPRC = 0.0365969839255\n",
      "Finished ext fold #1\n",
      "ext fold #2\n",
      "int fold #1\n",
      "AUC = 0.632051875868\n",
      "AUPRC = 0.0656197529583\n",
      "int fold #2\n",
      "int fold #3\n",
      "AUC = 0.767805774278\n",
      "AUPRC = 0.101551066903\n",
      "int fold #4\n",
      "AUC = 0.603044619423\n",
      "AUPRC = 0.0435280392152\n",
      "int fold #5\n",
      "AUC = 0.484078740157\n",
      "AUPRC = 0.0157521266676\n",
      "int fold #6\n",
      "AUC = 0.612354948805\n",
      "AUPRC = 0.0408745120283\n",
      "int fold #7\n",
      "AUC = 0.467492780257\n",
      "AUPRC = 0.0157836592598\n",
      "int fold #8\n",
      "AUC = 0.723071672355\n",
      "AUPRC = 0.0402076524808\n",
      "int fold #9\n",
      "AUC = 0.553851404568\n",
      "AUPRC = 0.0825728957808\n",
      "int fold #10\n",
      "AUC = 0.499270149646\n",
      "AUPRC = 0.0374813763334\n",
      "avg int auc = 0.593669107262\n",
      "avg int auprc = 0.0492634535141\n",
      "heldout fold #2\n",
      "AUC = 0.6056167979\n",
      "AUPRC = 0.0887679009495\n",
      "Finished ext fold #2\n",
      "ext fold #3\n",
      "int fold #1\n",
      "AUC = 0.492517111832\n",
      "AUPRC = 0.037596214818\n",
      "int fold #2\n",
      "AUC = 0.696356955381\n",
      "AUPRC = 0.0965369311076\n",
      "int fold #3\n",
      "int fold #4\n",
      "AUC = 0.654430446194\n",
      "AUPRC = 0.0396439833069\n",
      "int fold #5\n",
      "AUC = 0.567816272966\n",
      "AUPRC = 0.0181957250999\n",
      "int fold #6\n",
      "AUC = 0.630753478603\n",
      "AUPRC = 0.0428693063616\n",
      "int fold #7\n",
      "AUC = 0.457505907062\n",
      "AUPRC = 0.015207765385\n",
      "int fold #8\n",
      "AUC = 0.538135993699\n",
      "AUPRC = 0.0138555702586\n",
      "int fold #9\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-19-0dfe538cb624>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mligand\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"dna\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mtest_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mligands_features_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mligand\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mligands_negatives_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mligand\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mligand\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdownsample_method\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;32mprint\u001b[0m \u001b[1;34m\"Finished ligand \"\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mligand\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-18-17e0f885cd4e>\u001b[0m in \u001b[0;36mtest_model\u001b[1;34m(ligand_bind_features, ligand_negatives_features, ligand_name, downsample_method, features)\u001b[0m\n\u001b[0;32m     56\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m             \u001b[1;31m#fit to training data: (K-2) folds, and predict for the remaining fold\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 58\u001b[1;33m             \u001b[0mtrain_predict_auc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_int_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_int_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_int_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_int_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdownsample_method\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpred_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpred_idx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mauc_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mauprc_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeatures_pred_dfs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     59\u001b[0m             \u001b[0mpred_idx\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     60\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-11-ac490a911fc2>\u001b[0m in \u001b[0;36mtrain_predict_auc\u001b[1;34m(X_train, y_train, X_test, y_test, downsample_method, pred_dict, pred_idx, auc_dict, auprc_dict, features_pred_dfs)\u001b[0m\n\u001b[0;32m     15\u001b[0m     \u001b[1;31m#fit to training data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclassifiers\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mclassifier_method\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m     \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train_sampled\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train_sampled\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m     \u001b[1;31m#predict for the test data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/anat/anaconda2/lib/python2.7/site-packages/sklearn/linear_model/logistic.pyc\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m   1231\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpenalty\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdual\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1232\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax_iter\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtol\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1233\u001b[1;33m                 sample_weight=sample_weight)\n\u001b[0m\u001b[0;32m   1234\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_iter_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mn_iter_\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1235\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/anat/anaconda2/lib/python2.7/site-packages/sklearn/svm/base.pyc\u001b[0m in \u001b[0;36m_fit_liblinear\u001b[1;34m(X, y, C, fit_intercept, intercept_scaling, class_weight, penalty, dual, verbose, max_iter, tol, random_state, multi_class, loss, epsilon, sample_weight)\u001b[0m\n\u001b[0;32m    888\u001b[0m         \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_ind\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misspmatrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msolver_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtol\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mC\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    889\u001b[0m         \u001b[0mclass_weight_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_iter\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrnd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miinfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'i'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 890\u001b[1;33m         epsilon, sample_weight)\n\u001b[0m\u001b[0;32m    891\u001b[0m     \u001b[1;31m# Regarding rnd.randint(..) in the above signature:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    892\u001b[0m     \u001b[1;31m# seed for srand in range [0..INT_MAX); due to limitations in Numpy\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "test_model(ligands_features_df[ligand], ligands_negatives_df[ligand], ligand, downsample_method)\n",
    "  \n",
    "print \"Finished ligand \"+ligand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
