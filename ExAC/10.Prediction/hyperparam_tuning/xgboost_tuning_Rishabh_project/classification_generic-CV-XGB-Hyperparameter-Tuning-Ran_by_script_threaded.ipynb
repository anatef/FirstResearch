{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Basic imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from collections import defaultdict\n",
    "from os import environ\n",
    "import json\n",
    "\n",
    "#Classifier imports\n",
    "from sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, RandomForestRegressor\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from xgboost.sklearn import XGBClassifier\n",
    "\n",
    "#ML framework imports\n",
    "from sklearn.metrics import auc, roc_auc_score, precision_recall_curve, average_precision_score, make_scorer\n",
    "from sklearn.model_selection import StratifiedKFold,train_test_split,GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.preprocessing import scale\n",
    "import xgboost as xgb\n",
    "\n",
    "#import matplotlib.pylab as plt\n",
    "\n",
    "#from matplotlib.pylab import rcParams\n",
    "from sklearn import metrics   #Additional scklearn functions\n",
    "#from sklearn.grid_search import \n",
    "\n",
    "\n",
    "#Downsamplers imports - prototype generation\n",
    "from imblearn.under_sampling import ClusterCentroids\n",
    "\n",
    "#Downsamplers imports - prototype selection - controlled\n",
    "from imblearn.under_sampling import RandomUnderSampler, NearMiss\n",
    "\n",
    "#Downsamplers imports - prototype selection - Cleaning techniques\n",
    "from imblearn.under_sampling import TomekLinks, EditedNearestNeighbours, RepeatedEditedNearestNeighbours\n",
    "\n",
    "#Downsamplers imports - prototype selection - Cleaning techniques - Condensed nearest neighbors and derived algorithms\n",
    "from imblearn.under_sampling import CondensedNearestNeighbour, OneSidedSelection, NeighbourhoodCleaningRule\n",
    "\n",
    "#Downsamplers imports - prototype selection - Cleaning techniques\n",
    "from imblearn.under_sampling import InstanceHardnessThreshold\n",
    "\n",
    "from IPython.core.display import HTML\n",
    "#HTML(\"<style>.container { width:100% !important; }</style>\")\n",
    "\n",
    "ABSOLUTE_NEGATIVES = False\n",
    "FILTER_DOMAIN = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading the input dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all samples positions #: 38944\n"
     ]
    }
   ],
   "source": [
    "curr_dir = !pwd\n",
    "curr_dir[0] = curr_dir[0] + \"/..\"\n",
    "input_path = curr_dir[0]+\"/domains_similarity/filtered_features_table/\"\n",
    "filename = \"positions_features_mediode_filter_01.25.18.csv\"\n",
    "\n",
    "#input_path = curr_dir[0]+\"/../9.Features_exploration/binding_df/10/\"\n",
    "#filename = \"positions_features_01.25.18.csv\"\n",
    "\n",
    "bind_scores_num = 10\n",
    "\n",
    "#Features table\n",
    "features_all = pd.read_csv(input_path+filename, sep='\\t', index_col=0)\n",
    "features_cols = features_all.columns[1:-bind_scores_num] #removing binding scores and domain name\n",
    "ligands = [\"dna\", \"dnabase\", \"dnabackbone\", \"rna\", \"rnabase\", \"rnabackbone\", \"peptide\", \"ion\", \"metabolite\"]\n",
    "print \"all samples positions #: \"+str(features_all.shape[0])\n",
    "\n",
    "#lignd binding domains dictionary\n",
    "with open(curr_dir[0]+\"/ligands_negatives_domains_dict.pik\", 'rb') as handle:\n",
    "        negatives_dict = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataset of negative examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def filter_to_ligand_binding_domains(use_max_binding_score):\n",
    "    \n",
    "    ligands_negatives_df = {}\n",
    "    for ligand in ligands:\n",
    "        \n",
    "        ligands_negatives_df[ligand] = pd.DataFrame()\n",
    "        for domain in negatives_dict[ligand].keys():\n",
    "            if domain == 'negatives' or domain == 'domains':\n",
    "                continue\n",
    "            domain_all = features_all.loc[features_all.loc[:,\"domain_name\"] == domain,:]\n",
    "            \n",
    "            #In case this domain was previously filtered\n",
    "            if len(domain_all) == 0:\n",
    "                continue\n",
    "            \n",
    "            if (use_max_binding_score):\n",
    "                ligands_negatives_df[ligand] = pd.concat([ligands_negatives_df[ligand],domain_all.loc[domain_all.loc[:,\"max_binding_score\"] == 0,:]])\n",
    "            else:\n",
    "                ligand_bind_str = ligand+\"_binding_score\"\n",
    "                ligands_negatives_df[ligand] = pd.concat([ligands_negatives_df[ligand],domain_all.loc[domain_all.loc[:,ligand_bind_str] == 0,:]])\n",
    "        \n",
    "    #Handeling the ligand \"all_ligands\"\n",
    "    all_ligands_negatives_df = pd.concat([ligands_negatives_df[\"dna\"], ligands_negatives_df[\"dnabase\"], ligands_negatives_df[\"dnabackbone\"], ligands_negatives_df[\"rna\"], ligands_negatives_df[\"rnabase\"], \n",
    "                                 ligands_negatives_df[\"rnabackbone\"], ligands_negatives_df[\"ion\"], ligands_negatives_df[\"peptide\"], ligands_negatives_df[\"metabolite\"]])\n",
    "    all_ligands_negatives_df = all_ligands_negatives_df.drop_duplicates()\n",
    "    #Filter to just positions with max. binding score = 0\n",
    "    all_ligands_negatives_df = all_ligands_negatives_df[all_ligands_negatives_df[\"max_binding_score\"] == 0]\n",
    "    ligands_negatives_df[\"all_ligands\"] = all_ligands_negatives_df\n",
    "    \n",
    "    #Leaving just the features columns\n",
    "    for ligand in ligands_negatives_df.keys():   \n",
    "        ligands_negatives_df[ligand] = ligands_negatives_df[ligand][features_cols]\n",
    "        print(ligand+\" non-binding #:\"+str(len(ligands_negatives_df[ligand])))\n",
    "    \n",
    "    return ligands_negatives_df\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def negatives_by_binding_score(use_max_binding_score):\n",
    "    \n",
    "    ligands_negatives_df = {}\n",
    "    for ligand in ligands:\n",
    "        \n",
    "        if use_max_binding_score:\n",
    "            ligand_bind_str = \"max_binding_score\"\n",
    "        else:\n",
    "            ligand_bind_str = ligand+\"_binding_score\"\n",
    "        \n",
    "        ligands_negatives_df[ligand] = features_all[features_all[ligand_bind_str] == 0]\n",
    "        ligands_negatives_df[ligand] = ligands_negatives_df[ligand].loc[:,features_cols]\n",
    "        print(ligand+\" non-binding #:\"+str(len(ligands_negatives_df[ligand])))\n",
    "        \n",
    "    #Handeling the ligand \"all_ligands\"\n",
    "    ligands_negatives_df[\"all_ligands\"] = features_all[features_all[\"max_binding_score\"] == 0]\n",
    "    ligands_negatives_df[\"all_ligands\"] = ligands_negatives_df[\"all_ligands\"].loc[:,features_cols]\n",
    "    print(\"all_ligands non-binding #:\"+str(len(ligands_negatives_df[\"all_ligands\"])))\n",
    "    \n",
    "    return ligands_negatives_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dna non-binding #:38095\n",
      "dnabase non-binding #:38577\n",
      "dnabackbone non-binding #:38203\n",
      "rna non-binding #:38047\n",
      "rnabase non-binding #:38407\n",
      "rnabackbone non-binding #:38223\n",
      "peptide non-binding #:35437\n",
      "ion non-binding #:34488\n",
      "metabolite non-binding #:33971\n",
      "all_ligands non-binding #:27191\n"
     ]
    }
   ],
   "source": [
    "#Create negatives datasets\n",
    "if FILTER_DOMAIN:\n",
    "    if ABSOLUTE_NEGATIVES:\n",
    "        ligands_negatives_df = filter_to_ligand_binding_domains(True)\n",
    "    else:\n",
    "        ligands_negatives_df = filter_to_ligand_binding_domains(False)\n",
    "else:\n",
    "    if ABSOLUTE_NEGATIVES:\n",
    "        ligands_negatives_df = negatives_by_binding_score(True)\n",
    "    else:\n",
    "        ligands_negatives_df = negatives_by_binding_score(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Datasets of positive examples by ligand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dna #: 501\n",
      "dnabase #: 193\n",
      "dnabackbone #: 408\n",
      "rna #: 433\n",
      "rnabase #: 224\n",
      "rnabackbone #: 308\n",
      "peptide #: 1496\n",
      "ion #: 1093\n",
      "metabolite #: 1525\n"
     ]
    }
   ],
   "source": [
    "bind_th = 0.1\n",
    "ligands_features_df = {}\n",
    "    \n",
    "for ligand in ligands:\n",
    "    score_col_str = ligand+\"_binding_score\"\n",
    "    ligand_binding_df = features_all[features_all[score_col_str] >= bind_th]\n",
    "    print ligand+\" #: \"+str(ligand_binding_df.shape[0])\n",
    "    ligands_features_df[ligand] = ligand_binding_df.loc[:,features_cols]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataset of positive examples - all ligands combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all_ligands #: 4518\n"
     ]
    }
   ],
   "source": [
    "all_ligands_features_df = pd.concat([ligands_features_df[\"dna\"], ligands_features_df[\"dnabase\"], ligands_features_df[\"dnabackbone\"], ligands_features_df[\"rna\"], ligands_features_df[\"rnabase\"], \n",
    "                                     ligands_features_df[\"rnabackbone\"], ligands_features_df[\"ion\"], ligands_features_df[\"peptide\"], ligands_features_df[\"metabolite\"]])\n",
    "all_ligands_features_df = all_ligands_features_df.drop_duplicates()\n",
    "print \"all_ligands #: \"+str(all_ligands_features_df.shape[0])\n",
    "ligands_features_df[\"all_ligands\"] = all_ligands_features_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading env input for downsampler technique, ligand and classifier  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ligand = dna\n"
     ]
    }
   ],
   "source": [
    "classifier = \"XGB\"\n",
    "#Reading the ligand input\n",
    "try:\n",
    "    ligand = environ['ligand']\n",
    "except:\n",
    "    ligand = \"dna\"\n",
    "print \"ligand = \"+ligand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test_model(clf, pred_dict, auc_dict, auprc_dict, ligand_bind_features, ligand_negatives_features, features = []):\n",
    "    \"\"\"\n",
    "    Test different models in 10-folds cross-validation.\n",
    "    \"\"\"\n",
    "    \n",
    "    #Default: Exclude no features\n",
    "    if len(features) == 0:\n",
    "        features = np.ones([ligand_bind_features.shape[1],]).astype(bool)\n",
    "        \n",
    "    #Arranging the features table by the CV order, for each model\n",
    "    #features_pred_dfs = dict.fromkeys(classifiers.keys())\n",
    "    \n",
    "    models_req_scaling = [\"SVM\", \"KNN\"]\n",
    "    \n",
    "    #classifier = classifier_method\n",
    "    model = clf\n",
    "    #print \"classifier_method = \" + classifier_method\n",
    "    #print \"ligand = \" + ligand\n",
    "    print model.get_xgb_params()\n",
    "    #features_pred_dfs[classifier] = pd.DataFrame()\n",
    "        \n",
    "    #Create X and y with included features\n",
    "    X = pd.concat([ligand_bind_features.iloc[:,features], ligand_negatives_features.iloc[:,features]])\n",
    "\n",
    "    y = [1] * ligand_bind_features.shape[0]\n",
    "    y.extend([0] * ligand_negatives_features.shape[0])\n",
    "    y = np.array(y)\n",
    "\n",
    "    binding_skf = StratifiedKFold(n_splits=10, shuffle=True, random_state=0)\n",
    "    pred_idx = 1\n",
    "\n",
    "    for train_index, test_index in binding_skf.split(X, y):\n",
    "            \n",
    "        print \"fold #: \"+str(pred_idx)\n",
    "        X_train, X_test = X.iloc[train_index,:], X.iloc[test_index,:]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "        \n",
    "        X_train_sampled = X_train\n",
    "        y_train_sampled = y_train\n",
    "            \n",
    "            \n",
    "        #early_stopping validation set \n",
    "        #7.8 Early stopping and Algorithm 7.2\n",
    "        #http://egrcc.github.io/docs/dl/deeplearningbook-regularization.pdf\n",
    "            \n",
    "        #X_subtrain, X_valid, y_subtrain, y_valid = train_test_split(X_train_sampled, \n",
    "        #                                y_train_sampled, stratify=y_train_sampled, test_size = .1)\n",
    "            \n",
    "        #fit to training data\n",
    "        #model = classifiers[classifier]\n",
    "        model.fit(X_train_sampled, y_train_sampled)\n",
    "        \n",
    "        #model.fit(X_subtrain, y_subtrain, eval_set = [(X_valid,y_valid)],eval_metric = \"map\", early_stopping_rounds = 50,verbose = False)\n",
    "        #print model.best_ntree_limit\n",
    "        probs_list = []\n",
    "\n",
    "        #probs = model.predict(X_test)\n",
    "        #probs_list = probs\n",
    "            \n",
    "        probs = model.predict_proba(X_test) #,ntree_limit=model.best_ntree_limit)\n",
    "        for l in probs:\n",
    "            probs_list.append(l[1])\n",
    "                \n",
    "        pred_dict[\"obs\"].extend(y_test)\n",
    "        pred_dict[\"prob\"].extend(probs_list)\n",
    "        fold_list = [pred_idx] * len(probs_list)\n",
    "        pred_dict[\"fold\"].extend(fold_list)\n",
    "\n",
    "        model_list = [classifier] * len(probs_list)\n",
    "        pred_dict[\"model\"].extend(model_list)\n",
    "\n",
    "        #Update auc auprc dictionaries\n",
    "        auc_dict[classifier].append(roc_auc_score(y_test, probs[:, 1]))\n",
    "        precision, recall, _ = precision_recall_curve(y_test, probs[:, 1])\n",
    "            \n",
    "        #auc_dict[classifier].append(roc_auc_score(y_test, probs))\n",
    "        #precision, recall, _ = precision_recall_curve(y_test, probs)\n",
    "            \n",
    "        auprc_dict[classifier].append(auc(recall, precision))\n",
    "            \n",
    "        #Update features table\n",
    "        #features_pred_dfs[classifier] = features_pred_dfs[classifier].append(X_test)\n",
    "        pred_idx += 1\n",
    "            \n",
    "        print \"AUC = \"+str(auc_dict[classifier][-1])\n",
    "        print \"AUPRC = \"+str(auprc_dict[classifier][-1])\n",
    "\n",
    "    avg_auc = np.sum(auc_dict[classifier])/10.0\n",
    "    print \"avg auc = \"+str(avg_auc) \n",
    "        \n",
    "    avg_auprc = np.sum(auprc_dict[classifier])/10.0\n",
    "    print \"avg auprc = \"+str(avg_auprc)\n",
    "            \n",
    "    print \"Finished \"+ligand+\" \"+classifier\n",
    "        \n",
    "    #return Average AUPRC\n",
    "    return avg_auprc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test model functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#adapted from https://www.analyticsvidhya.com/blog/2016/03/complete-guide-parameter-tuning-xgboost-with-codes-python/\n",
    "#\n",
    "#Using cross-validation, continues adding estimators until auc does not improve within 50 rounds\n",
    "#\n",
    "#Towards the end of my runs I stopped using this in favor of using test_model with different value of n_estimators.\n",
    "def modelfit(alg, ligand_bind_features, ligand_negatives_features, ligand_name, useTrainCV=True, cv_folds=10, early_stopping_rounds=50):\n",
    "    \n",
    "    features = np.ones([ligand_bind_features.shape[1],]).astype(bool)\n",
    "    X = pd.concat([ligand_bind_features.iloc[:,features], ligand_negatives_features.iloc[:,features]])\n",
    "\n",
    "    y = [1] * ligand_bind_features.shape[0]\n",
    "    y.extend([0] * ligand_negatives_features.shape[0])\n",
    "    y = np.array(y)\n",
    "    \n",
    "    print \"modelfit\"\n",
    "    xgb_param = alg.get_xgb_params()\n",
    "    xgtrain = xgb.DMatrix(X, label=y)\n",
    "    #print alg.get_params()['n_estimators']\n",
    "    \n",
    "    #metrics can be changed to a variety of things including \"map\" for Mean Average Precision (same as Average AUPRC) \n",
    "    #I didn't have much luck with that though. \n",
    "    #See under eval_metric in https://github.com/dmlc/xgboost/blob/master/doc/parameter.md\n",
    "    cvresult = xgb.cv(xgb_param, xgtrain, num_boost_round=alg.get_params()['n_estimators'], nfold=cv_folds, \n",
    "                      metrics='auc', early_stopping_rounds=early_stopping_rounds,verbose_eval =True)\n",
    "    alg.set_params(n_estimators=cvresult.shape[0])\n",
    "    print \"Optimal n_estimators: \" + str(cvresult.shape[0])\n",
    "    \n",
    "    return alg,cvresult#,dtrain_predictions,dtrain_predprob,alg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "76\n"
     ]
    }
   ],
   "source": [
    "ligand_bind_features = ligands_features_df[ligand]\n",
    "ligand_negatives_features = ligands_negatives_df[ligand]\n",
    "features = features = np.ones([ligand_bind_features.shape[1],]).astype(bool)\n",
    "X = pd.concat([ligand_bind_features.iloc[:,features], ligand_negatives_features.iloc[:,features]])\n",
    "y = [1] * ligand_bind_features.shape[0]\n",
    "y.extend([0] * ligand_negatives_features.shape[0])\n",
    "y = np.array(y)\n",
    "\n",
    "#Calcuate a good value for scale_pos_weight, which gives that value to positive examples. \n",
    "#A common value is the ratio of number of negative examples to number of positive examples (#neg/#pos)\n",
    "\n",
    "val_scale_pos_weight = len([y[i] for i in range(len(y)) if y[i]==0])/len([y[i] for i in range(len(y)) if y[i]==1])\n",
    "print val_scale_pos_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 0 ns, sys: 0 ns, total: 0 ns\n",
      "Wall time: 51 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "#Uses cross validation and early stopping to find best number of estimators\n",
    "\n",
    "#I started using the cell below to find the best number of estimators, but I am keeping it here just as another method.\n",
    "\n",
    "#xgb1 = XGBClassifier(\n",
    "# learning_rate =0.1,\n",
    "# n_estimators=1000,\n",
    "# max_depth=6,\n",
    "# min_child_weight=1,\n",
    "# gamma=0,\n",
    "# subsample=0.8,\n",
    "# colsample_bytree=0.8,\n",
    "# objective= 'binary:logistic',\n",
    "# n_jobs=-1,\n",
    "# scale_pos_weight=val_scale_pos_weight,\n",
    "# random_state = 0)\n",
    "\n",
    "#returns = modelfit(xgb1, ligands_features_df[ligand], ligands_negatives_df[ligand], ligand)\n",
    "#print \"Optimal n_estimators for ligand(\"+str(ligand)+\"): \"+str(returns[1].shape[0]) \n",
    "#optimized_n_est = returns[1].shape[0] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'reg_alpha': 0, 'colsample_bytree': 0.8, 'silent': 1, 'colsample_bylevel': 1, 'scale_pos_weight': 76, 'learning_rate': 0.1, 'missing': None, 'max_delta_step': 0, 'base_score': 0.5, 'n_estimators': 1, 'subsample': 0.8, 'reg_lambda': 1, 'min_child_weight': 1, 'objective': 'binary:logistic', 'seed': 0, 'max_depth': 5, 'gamma': 0, 'booster': 'gbtree'}\n",
      "fold #: 1\n",
      "AUC = 0.9020148216767022\n",
      "AUPRC = 0.18338141268266292\n",
      "fold #: 2\n",
      "AUC = 0.8645354330708661\n",
      "AUPRC = 0.15289504422998046\n",
      "fold #: 3\n",
      "AUC = 0.9146430446194225\n",
      "AUPRC = 0.16807593840468993\n",
      "fold #: 4\n",
      "AUC = 0.8884173228346457\n",
      "AUPRC = 0.22211158402769038\n",
      "fold #: 5\n",
      "AUC = 0.8760498687664041\n",
      "AUPRC = 0.14509397936518023\n",
      "fold #: 6\n",
      "AUC = 0.8596613284326595\n",
      "AUPRC = 0.09222608828393275\n",
      "fold #: 7\n",
      "AUC = 0.912032029404043\n",
      "AUPRC = 0.19710307279178088\n",
      "fold #: 8\n",
      "AUC = 0.8587188238382778\n",
      "AUPRC = 0.15018286018568838\n",
      "fold #: 9\n",
      "AUC = 0.88789183512733\n",
      "AUPRC = 0.21054824539882766\n",
      "fold #: 10\n",
      "AUC = 0.8848227881333683\n",
      "AUPRC = 0.12844788618236735\n",
      "avg auc = 0.884878729590372\n",
      "avg auprc = 0.1650066111552801\n",
      "Finished dna XGB\n",
      "{n_estimators: 1}: 0.1650066111552801\n",
      "\n",
      "Best params by AUPRC: {n_estimators: 1}: 0.1650066111552801\n",
      "CPU times: user 41.6 s, sys: 7.08 s, total: 48.7 s\n",
      "Wall time: 14.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "#Search for best values of n_estimators\n",
    "#in the given range, which can be changed by changing the ranges in the for-loop\n",
    "#Multithreading is used to speed up the process\n",
    "\n",
    "from multiprocessing.dummy import Pool as ThreadPool\n",
    "#can change number of threads (problem was running out of memory on the cluster)\n",
    "pool = ThreadPool(20)\n",
    "\n",
    "def test_model_for_map(clf):\n",
    "    pred_dict = defaultdict(list)\n",
    "    auc_dict = defaultdict(list)\n",
    "    auprc_dict = defaultdict(list)\n",
    "    return test_model(clf, pred_dict, auc_dict, auprc_dict, ligands_features_df[ligand], ligands_negatives_df[ligand])\n",
    " \n",
    "classifiers = []\n",
    "params = {} \n",
    "\n",
    "for n_est in range(1,1025,25):\n",
    "    #create an XGB classifier with the given parameters.\n",
    "    classifiers.append(XGBClassifier(n_estimators=n_est, n_jobs = -1, random_state=0, \n",
    "            max_depth=5, min_child_weight=1, gamma = 0,\n",
    "            colsample_bytree=.8, subsample=.8, scale_pos_weight = val_scale_pos_weight))\n",
    "    \n",
    "results = pool.map(test_model_for_map,classifiers)\n",
    "pool.close()\n",
    "pool.join()\n",
    "    \n",
    "best_score = 0\n",
    "best_param = \"\"\n",
    "for i in range(0,len(classifiers),1):\n",
    "    print \"{n_estimators: \" + str(classifiers[i].get_params()['n_estimators']) + \"}\" + \": \"+  str(results[i])\n",
    "    if(results[i] > best_score):\n",
    "        optimized_n_est = classifiers[i].get_params()['n_estimators']\n",
    "        best_score = results[i]\n",
    "        \n",
    "print \"\\nBest params by AUPRC: \" + \"{n_estimators: \" + str(classifiers[i].get_params()['n_estimators']) + \"}\" + \": \" +  str(best_score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "       colsample_bytree=1.0, gamma=0.0, learning_rate=0.1,\n",
      "       max_delta_step=0, max_depth=3, min_child_weight=0, missing=None,\n",
      "       n_estimators=1, n_jobs=-1, nthread=None,\n",
      "       objective='binary:logistic', random_state=0, reg_alpha=0,\n",
      "       reg_lambda=1, scale_pos_weight=76, seed=None, silent=True,\n",
      "       subsample=1.0)]\n",
      "{'reg_alpha': 0, 'colsample_bytree': 1.0, 'silent': 1, 'colsample_bylevel': 1, 'scale_pos_weight': 76, 'learning_rate': 0.1, 'missing': None, 'max_delta_step': 0, 'base_score': 0.5, 'n_estimators': 1, 'subsample': 1.0, 'reg_lambda': 1, 'min_child_weight': 0, 'objective': 'binary:logistic', 'seed': 0, 'max_depth': 3, 'gamma': 0.0, 'booster': 'gbtree'}\n",
      "fold #: 1\n",
      "AUC = 0.8213627708301169\n",
      "AUPRC = 0.155121692799043\n",
      "fold #: 2\n",
      "AUC = 0.7931496062992127\n",
      "AUPRC = 0.24075672102751486\n",
      "fold #: 3\n",
      "AUC = 0.8046745406824147\n",
      "AUPRC = 0.2477251778766975\n",
      "fold #: 4\n",
      "AUC = 0.7735511811023623\n",
      "AUPRC = 0.21461369222185353\n",
      "fold #: 5\n",
      "AUC = 0.833007874015748\n",
      "AUPRC = 0.24904639728820585\n",
      "fold #: 6\n",
      "AUC = 0.7606248359149383\n",
      "AUPRC = 0.10974588454415037\n",
      "fold #: 7\n",
      "AUC = 0.802032029404043\n",
      "AUPRC = 0.2842989079487555\n",
      "fold #: 8\n",
      "AUC = 0.8445156208978734\n",
      "AUPRC = 0.11416124521375828\n",
      "fold #: 9\n",
      "AUC = 0.8364137568915726\n",
      "AUPRC = 0.35519045842945557\n",
      "fold #: 10\n",
      "AUC = 0.7768548175374113\n",
      "AUPRC = 0.0902462393612556\n",
      "avg auc = 0.8046187033575694\n",
      "avg auprc = 0.206090641671069\n",
      "Finished dna XGB\n",
      "{max_depth: 3, min_child_weight: 0, gamma: 0.0}: 0.206090641671069\n",
      "\n",
      "Best params by AUPRC: {max_depth: 3, min_child_weight: 0, gamma: 0.0}: 0.206090641671069\n"
     ]
    }
   ],
   "source": [
    "#Search for best values of max_depth, min_child_weight, and gamma (which are all related to complexity of the model)\n",
    "#in the given ranges, which can be changed by changing the ranges in the for-loops\n",
    "#Multithreading is used to speed up the process\n",
    "\n",
    "#Dummy module is the same as multiprocessing, but is for multithreading\n",
    "from multiprocessing.dummy import Pool as ThreadPool\n",
    "pool = ThreadPool(20)\n",
    "\n",
    "#Helper function that takes a classifier and runs test_model and returns the Average PRC (score)\n",
    "def test_model_for_map(clf):\n",
    "    pred_dict = defaultdict(list)\n",
    "    auc_dict = defaultdict(list)\n",
    "    auprc_dict = defaultdict(list)\n",
    "    return test_model(clf, pred_dict, auc_dict, auprc_dict, ligands_features_df[ligand], ligands_negatives_df[ligand])\n",
    "\n",
    "classifiers = []\n",
    "params = {}\n",
    "\n",
    "#11,6,11\n",
    "for max_depth in range(3,11,1):\n",
    "    for min_child_weight in range(0,6,1):\n",
    "        for gam in [i/10.0 for i in range(0,11,1)]:\n",
    "            #create an XGB classifier with the given parameters. \n",
    "            #colsample_bytree and subsample are fixed to 0.8.\n",
    "            #n_estimator from previous section.\n",
    "            classifiers.append(XGBClassifier(n_estimators=optimized_n_est, n_jobs = -1, random_state=0, \n",
    "                max_depth=max_depth, min_child_weight=min_child_weight, gamma = gam,\n",
    "                colsample_bytree=.8, subsample=.8, scale_pos_weight = val_scale_pos_weight))\n",
    "\n",
    "#runs test_model_for_map function on each classifier in classifiers\n",
    "results = pool.map(test_model_for_map,classifiers)\n",
    "pool.close()\n",
    "pool.join()\n",
    "\n",
    "\n",
    "#Print out params and their Average AUPRC and get parameter which had best Average AUPRC (score)\n",
    "best_score = 0\n",
    "best_param = \"\"\n",
    "for i in range(0,len(classifiers),1):\n",
    "    print \"{max_depth: \" + str(classifiers[i].get_params()['max_depth']) + \", min_child_weight: \" + str(classifiers[i].get_params()['min_child_weight']) +  \", gamma: \" + str(classifiers[i].get_params()['gamma'])+\"}\" + \": \"+  str(results[i])\n",
    "    if(results[i] > best_score):\n",
    "        opt_max_depth = classifiers[i].get_params()['max_depth']\n",
    "        opt_min_child_weight = classifiers[i].get_params()['min_child_weight']\n",
    "        opt_gamma = classifiers[i].get_params()['gamma']\n",
    "        best_score = results[i]\n",
    "        \n",
    "print \"\\nBest params by AUPRC: \" + \"{max_depth: \" + str(classifiers[i].get_params()['max_depth']) + \", min_child_weight: \" + str(classifiers[i].get_params()['min_child_weight']) +  \", gamma: \" + str(classifiers[i].get_params()['gamma'])+\"}\" + \": \" +  str(best_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'reg_alpha': 0, 'colsample_bytree': 0.1, 'silent': 1, 'colsample_bylevel': 1, 'scale_pos_weight': 76, 'learning_rate': 0.1, 'missing': None, 'max_delta_step': 0, 'base_score': 0.5, 'n_estimators': 1, 'subsample': 0.1, 'reg_lambda': 1, 'min_child_weight': 0, 'objective': 'binary:logistic', 'seed': 0, 'max_depth': 3, 'gamma': 0.1, 'booster': 'gbtree'}\n",
      "fold #: 1\n",
      "AUC = 0.6844346662549534\n",
      "AUPRC = 0.07516296957165385\n",
      "fold #: 2\n",
      "AUC = 0.7016797900262467\n",
      "AUPRC = 0.240605915373761\n",
      "fold #: 3\n",
      "AUC = 0.6508372703412073\n",
      "AUPRC = 0.05828533830461648\n",
      "fold #: 4\n",
      "AUC = 0.652496062992126\n",
      "AUPRC = 0.02199133757365954\n",
      "fold #: 5\n",
      "AUC = 0.6744514435695537\n",
      "AUPRC = 0.05023191685898522\n",
      "fold #: 6\n",
      "AUC = 0.6046573903911789\n",
      "AUPRC = 0.04405999546487148\n",
      "fold #: 7\n",
      "AUC = 0.5877658177999474\n",
      "AUPRC = 0.11099825384490054\n",
      "fold #: 8\n",
      "AUC = 0.6117327382515096\n",
      "AUPRC = 0.07007895062277963\n",
      "fold #: 9\n",
      "AUC = 0.5713993174061434\n",
      "AUPRC = 0.029572950545114034\n",
      "fold #: 10\n",
      "AUC = 0.6381937516408506\n",
      "AUPRC = 0.15912260469696665\n",
      "avg auc = 0.6377648248673717\n",
      "avg auprc = 0.08601102328573082\n",
      "Finished dna XGB\n",
      "{colsample_bytree: 0.1, subsample:0.1}: 0.08601102328573082\n",
      "\n",
      "Best params by AUPRC: {colsample_bytree: 0.1, subsample:0.1}: 0.08601102328573082\n",
      "CPU times: user 30.7 s, sys: 6.9 s, total: 37.6 s\n",
      "Wall time: 14.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "#Search for best values of subsample and colsample_bytree (which are all related to robustness of the model)\n",
    "#in the given ranges, which can be changed by changing the ranges in the for-loops\n",
    "#Multithreading is used to speed up the process\n",
    "\n",
    "pool2 = ThreadPool(20)\n",
    "classifiers = []\n",
    "params = {}\n",
    "\n",
    "#11,11\n",
    "for colsamp in range(1,11,1):\n",
    "    for subsamp in range(1,11,1):\n",
    "        #create an XGB classifier with the given parameters. \n",
    "        #n_estimators, max_depth, min_child_weight, and gamma are the best values from the previous sections\n",
    "        classifiers.append(XGBClassifier(n_estimators=optimized_n_est, n_jobs = -1, random_state=0, \n",
    "            max_depth=opt_max_depth, gamma = .1, min_child_weight=opt_min_child_weight, \n",
    "            colsample_bytree=colsamp/10.0, subsample=subsamp/10.0, scale_pos_weight = val_scale_pos_weight,))\n",
    "\n",
    "results = pool2.map(test_model_for_map,classifiers)\n",
    "pool2.close()\n",
    "pool2.join()\n",
    "\n",
    "best_score = 0\n",
    "best_param = \"\"\n",
    "for i in range(0,len(classifiers),1):\n",
    "    print \"{colsample_bytree: \" + str(classifiers[i].get_params()['colsample_bytree']) + \", subsample:\" + str(classifiers[i].get_params()['subsample']) + \"}\" + \": \"+  str(results[i])\n",
    "    if(results[i] > best_score):\n",
    "        opt_colsample_bytree = classifiers[i].get_params()['colsample_bytree']\n",
    "        opt_subsample = classifiers[i].get_params()['subsample']\n",
    "        best_score = results[i]\n",
    "        \n",
    "print \"\\nBest params by AUPRC: \" + \"{colsample_bytree: \" + str(classifiers[i].get_params()['colsample_bytree']) + \", subsample:\" + str(classifiers[i].get_params()['subsample']) + \"}\" + \": \" +  str(best_score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#I didn't do this in the last run I did on the cluster, but you could run either of the next two sections to retune the\n",
    "#n_estimator parameter with the new values of hyperparameters found above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "#Search for best values of n_estimators\n",
    "#in the given range, which can be changed by changing the ranges in the for-loop\n",
    "#Multithreading is used to speed up the process\n",
    "\n",
    "from multiprocessing.dummy import Pool as ThreadPool\n",
    "#can change number of threads (problem was running out of memory on the cluster)\n",
    "pool = ThreadPool(20)\n",
    "\n",
    "def test_model_for_map(clf):\n",
    "    pred_dict = defaultdict(list)\n",
    "    auc_dict = defaultdict(list)\n",
    "    auprc_dict = defaultdict(list)\n",
    "    return test_model(clf, pred_dict, auc_dict, auprc_dict, ligands_features_df[ligand], ligands_negatives_df[ligand])\n",
    " \n",
    "classifiers = []\n",
    "params = {} \n",
    "\n",
    "for n_est in range(1,1025,25):\n",
    "    #create an XGB classifier with the given parameters.\n",
    "    classifiers.append(XGBClassifier(n_estimators=n_est, n_jobs = -1, random_state=0, \n",
    "            max_depth=opt_max_depth, min_child_weight=opt_min_child_weight, gamma = opt_gamma,\n",
    "            colsample_bytree=opt_colsample_bytree, subsample=opt_subsample, scale_pos_weight = val_scale_pos_weight))\n",
    "    \n",
    "results = pool.map(test_model_for_map,classifiers)\n",
    "pool.close()\n",
    "pool.join()\n",
    "    \n",
    "best_score = 0\n",
    "best_param = \"\"\n",
    "for i in range(0,len(classifiers),1):\n",
    "    print \"{n_estimators: \" + str(classifiers[i].get_params()['n_estimators']) + \"}\" + \": \"+  str(results[i])\n",
    "    if(results[i] > best_score):\n",
    "        optimized_n_est_2 = classifiers[i].get_params()['n_estimators']\n",
    "        best_score = results[i]\n",
    "        \n",
    "print \"\\nBest params by AUPRC: \" + \"{n_estimators: \" + str(classifiers[i].get_params()['n_estimators']) + \"}\" + \": \" +  str(best_score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "#Uses cross validation and early stopping to find best number of estimators after finding best parameters\n",
    "xgb2 = XGBClassifier(\n",
    " learning_rate =0.1,\n",
    " n_estimators=1000,\n",
    " max_depth=opt_max_depth,\n",
    " min_child_weight=opt_min_child_weight,\n",
    " gamma=opt_gamma,\n",
    " subsample=opt_subsample,\n",
    " colsample_bytree=opt_colsample_bytree,\n",
    " objective= 'binary:logistic',\n",
    " n_jobs=-1,\n",
    " scale_pos_weight=val_scale_pos_weight,\n",
    " random_state = 0)\n",
    "\n",
    "#returns = modelfit(xgb2, ligands_features_df[ligand], ligands_negatives_df[ligand], ligand)\n",
    "#print \"Optimal n_estimators for ligand(\"+str(ligand)+\"): \"+str(returns[1].shape[0]) \n",
    "#optimized_n_est_2 = returns[1].shape[0] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params for dna:\n",
      "num_estimators = 1\n",
      "max_depth = 3\n",
      "min_child_weight = 0\n",
      "gamma = 0.0\n",
      "subsample = 0.1\n",
      "colsample_bytree = 0.1\n",
      "scale_pos_weight = 76\n"
     ]
    }
   ],
   "source": [
    "#If you do either of the above sections you can uncomment the next line\n",
    "#optimized_n_est = optimized_n_est_2\n",
    "print \"Best params for {0}:\\nnum_estimators = {6}\\nmax_depth = {1}\\nmin_child_weight = {2}\\ngamma = {3}\\nsubsample = {4}\\ncolsample_bytree = {5}\\nscale_pos_weight = {7}\".format(ligand,opt_max_depth,opt_min_child_weight,opt_gamma,opt_subsample,opt_colsample_bytree,optimized_n_est,val_scale_pos_weight)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
