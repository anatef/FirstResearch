{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Basic imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from collections import defaultdict\n",
    "from os import environ\n",
    "import json\n",
    "\n",
    "#Classifier imports\n",
    "from sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, RandomForestRegressor\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from xgboost.sklearn import XGBClassifier\n",
    "\n",
    "#ML framework imports\n",
    "from sklearn.metrics import auc, roc_auc_score, precision_recall_curve, average_precision_score, make_scorer\n",
    "from sklearn.model_selection import StratifiedKFold,train_test_split,GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.preprocessing import scale\n",
    "import xgboost as xgb\n",
    "\n",
    "#import matplotlib.pylab as plt\n",
    "\n",
    "#from matplotlib.pylab import rcParams\n",
    "from sklearn import metrics   #Additional scklearn functions\n",
    "#from sklearn.grid_search import \n",
    "\n",
    "\n",
    "#Downsamplers imports - prototype generation\n",
    "from imblearn.under_sampling import ClusterCentroids\n",
    "\n",
    "#Downsamplers imports - prototype selection - controlled\n",
    "from imblearn.under_sampling import RandomUnderSampler, NearMiss\n",
    "\n",
    "#Downsamplers imports - prototype selection - Cleaning techniques\n",
    "from imblearn.under_sampling import TomekLinks, EditedNearestNeighbours, RepeatedEditedNearestNeighbours\n",
    "\n",
    "#Downsamplers imports - prototype selection - Cleaning techniques - Condensed nearest neighbors and derived algorithms\n",
    "from imblearn.under_sampling import CondensedNearestNeighbour, OneSidedSelection, NeighbourhoodCleaningRule\n",
    "\n",
    "#Downsamplers imports - prototype selection - Cleaning techniques\n",
    "from imblearn.under_sampling import InstanceHardnessThreshold\n",
    "\n",
    "from IPython.core.display import HTML\n",
    "#HTML(\"<style>.container { width:100% !important; }</style>\")\n",
    "\n",
    "ABSOLUTE_NEGATIVES = False\n",
    "FILTER_DOMAIN = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading the input dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all samples positions #: 38944\n"
     ]
    }
   ],
   "source": [
    "curr_dir = !pwd\n",
    "curr_dir[0] = curr_dir[0] + \"/..\"\n",
    "input_path = curr_dir[0]+\"/domains_similarity/filtered_features_table/\"\n",
    "filename = \"positions_features_mediode_filter_01.25.18.csv\"\n",
    "\n",
    "#input_path = curr_dir[0]+\"/../9.Features_exploration/binding_df/10/\"\n",
    "#filename = \"positions_features_01.25.18.csv\"\n",
    "\n",
    "bind_scores_num = 10\n",
    "\n",
    "#Features table\n",
    "features_all = pd.read_csv(input_path+filename, sep='\\t', index_col=0)\n",
    "features_cols = features_all.columns[1:-bind_scores_num] #removing binding scores and domain name\n",
    "ligands = [\"dna\", \"dnabase\", \"dnabackbone\", \"rna\", \"rnabase\", \"rnabackbone\", \"peptide\", \"ion\", \"metabolite\"]\n",
    "print \"all samples positions #: \"+str(features_all.shape[0])\n",
    "\n",
    "#lignd binding domains dictionary\n",
    "with open(curr_dir[0]+\"/ligands_negatives_domains_dict.pik\", 'rb') as handle:\n",
    "        negatives_dict = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataset of negative examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def filter_to_ligand_binding_domains(use_max_binding_score):\n",
    "    \n",
    "    ligands_negatives_df = {}\n",
    "    for ligand in ligands:\n",
    "        \n",
    "        ligands_negatives_df[ligand] = pd.DataFrame()\n",
    "        for domain in negatives_dict[ligand].keys():\n",
    "            if domain == 'negatives' or domain == 'domains':\n",
    "                continue\n",
    "            domain_all = features_all.loc[features_all.loc[:,\"domain_name\"] == domain,:]\n",
    "            \n",
    "            #In case this domain was previously filtered\n",
    "            if len(domain_all) == 0:\n",
    "                continue\n",
    "            \n",
    "            if (use_max_binding_score):\n",
    "                ligands_negatives_df[ligand] = pd.concat([ligands_negatives_df[ligand],domain_all.loc[domain_all.loc[:,\"max_binding_score\"] == 0,:]])\n",
    "            else:\n",
    "                ligand_bind_str = ligand+\"_binding_score\"\n",
    "                ligands_negatives_df[ligand] = pd.concat([ligands_negatives_df[ligand],domain_all.loc[domain_all.loc[:,ligand_bind_str] == 0,:]])\n",
    "        \n",
    "    #Handeling the ligand \"all_ligands\"\n",
    "    all_ligands_negatives_df = pd.concat([ligands_negatives_df[\"dna\"], ligands_negatives_df[\"dnabase\"], ligands_negatives_df[\"dnabackbone\"], ligands_negatives_df[\"rna\"], ligands_negatives_df[\"rnabase\"], \n",
    "                                 ligands_negatives_df[\"rnabackbone\"], ligands_negatives_df[\"ion\"], ligands_negatives_df[\"peptide\"], ligands_negatives_df[\"metabolite\"]])\n",
    "    all_ligands_negatives_df = all_ligands_negatives_df.drop_duplicates()\n",
    "    #Filter to just positions with max. binding score = 0\n",
    "    all_ligands_negatives_df = all_ligands_negatives_df[all_ligands_negatives_df[\"max_binding_score\"] == 0]\n",
    "    ligands_negatives_df[\"all_ligands\"] = all_ligands_negatives_df\n",
    "    \n",
    "    #Leaving just the features columns\n",
    "    for ligand in ligands_negatives_df.keys():   \n",
    "        ligands_negatives_df[ligand] = ligands_negatives_df[ligand][features_cols]\n",
    "        print(ligand+\" non-binding #:\"+str(len(ligands_negatives_df[ligand])))\n",
    "    \n",
    "    return ligands_negatives_df\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def negatives_by_binding_score(use_max_binding_score):\n",
    "    \n",
    "    ligands_negatives_df = {}\n",
    "    for ligand in ligands:\n",
    "        \n",
    "        if use_max_binding_score:\n",
    "            ligand_bind_str = \"max_binding_score\"\n",
    "        else:\n",
    "            ligand_bind_str = ligand+\"_binding_score\"\n",
    "        \n",
    "        ligands_negatives_df[ligand] = features_all[features_all[ligand_bind_str] == 0]\n",
    "        ligands_negatives_df[ligand] = ligands_negatives_df[ligand].loc[:,features_cols]\n",
    "        print(ligand+\" non-binding #:\"+str(len(ligands_negatives_df[ligand])))\n",
    "        \n",
    "    #Handeling the ligand \"all_ligands\"\n",
    "    ligands_negatives_df[\"all_ligands\"] = features_all[features_all[\"max_binding_score\"] == 0]\n",
    "    ligands_negatives_df[\"all_ligands\"] = ligands_negatives_df[\"all_ligands\"].loc[:,features_cols]\n",
    "    print(\"all_ligands non-binding #:\"+str(len(ligands_negatives_df[\"all_ligands\"])))\n",
    "    \n",
    "    return ligands_negatives_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dna non-binding #:38095\n",
      "dnabase non-binding #:38577\n",
      "dnabackbone non-binding #:38203\n",
      "rna non-binding #:38047\n",
      "rnabase non-binding #:38407\n",
      "rnabackbone non-binding #:38223\n",
      "peptide non-binding #:35437\n",
      "ion non-binding #:34488\n",
      "metabolite non-binding #:33971\n",
      "all_ligands non-binding #:27191\n"
     ]
    }
   ],
   "source": [
    "#Create negatives datasets\n",
    "if FILTER_DOMAIN:\n",
    "    if ABSOLUTE_NEGATIVES:\n",
    "        ligands_negatives_df = filter_to_ligand_binding_domains(True)\n",
    "    else:\n",
    "        ligands_negatives_df = filter_to_ligand_binding_domains(False)\n",
    "else:\n",
    "    if ABSOLUTE_NEGATIVES:\n",
    "        ligands_negatives_df = negatives_by_binding_score(True)\n",
    "    else:\n",
    "        ligands_negatives_df = negatives_by_binding_score(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Datasets of positive examples by ligand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dna #: 501\n",
      "dnabase #: 193\n",
      "dnabackbone #: 408\n",
      "rna #: 433\n",
      "rnabase #: 224\n",
      "rnabackbone #: 308\n",
      "peptide #: 1496\n",
      "ion #: 1093\n",
      "metabolite #: 1525\n"
     ]
    }
   ],
   "source": [
    "bind_th = 0.1\n",
    "ligands_features_df = {}\n",
    "    \n",
    "for ligand in ligands:\n",
    "    score_col_str = ligand+\"_binding_score\"\n",
    "    ligand_binding_df = features_all[features_all[score_col_str] >= bind_th]\n",
    "    print ligand+\" #: \"+str(ligand_binding_df.shape[0])\n",
    "    ligands_features_df[ligand] = ligand_binding_df.loc[:,features_cols]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataset of positive examples - all ligands combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all_ligands #: 4518\n"
     ]
    }
   ],
   "source": [
    "all_ligands_features_df = pd.concat([ligands_features_df[\"dna\"], ligands_features_df[\"dnabase\"], ligands_features_df[\"dnabackbone\"], ligands_features_df[\"rna\"], ligands_features_df[\"rnabase\"], \n",
    "                                     ligands_features_df[\"rnabackbone\"], ligands_features_df[\"ion\"], ligands_features_df[\"peptide\"], ligands_features_df[\"metabolite\"]])\n",
    "all_ligands_features_df = all_ligands_features_df.drop_duplicates()\n",
    "print \"all_ligands #: \"+str(all_ligands_features_df.shape[0])\n",
    "ligands_features_df[\"all_ligands\"] = all_ligands_features_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading env input for downsampler technique, ligand and classifier  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ligand = dna\n",
      "downsample_method = NoDown\n",
      "classifier_method = XGB\n"
     ]
    }
   ],
   "source": [
    "#Reading the ligand input\n",
    "try:\n",
    "    ligand = environ['ligand']\n",
    "except:\n",
    "    ligand = \"dna\"\n",
    "print \"ligand = \"+ligand\n",
    "\n",
    "#Reading the max_depth input\n",
    "try:\n",
    "    max_depth = environ['max_depth']\n",
    "except:\n",
    "    max_depth = 1\n",
    "    \n",
    "#Reading the min_child_weight input\n",
    "try:\n",
    "    min_child_weight = environ['min_child_weight']\n",
    "except:\n",
    "    min_child_weight = 0\n",
    "\n",
    "#Reading the gamma input\n",
    "try:\n",
    "    gam = environ['gamma']\n",
    "except:\n",
    "    gam = 0\n",
    "    \n",
    "#Reading the colsample_bytree input\n",
    "try:\n",
    "    colsample_bytree = environ['colsample_bytree']\n",
    "except:\n",
    "    colsample_bytree = 1.0\n",
    "\n",
    "#Reading the subsample input\n",
    "try:\n",
    "    subsample = environ['subsample']\n",
    "except:\n",
    "    subsample = 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test_model(clf, pred_dict, auc_dict, auprc_dict, ligand_bind_features, ligand_negatives_features, features = []):\n",
    "    \"\"\"\n",
    "    Test different models in 10-folds cross-validation.\n",
    "    \"\"\"\n",
    "    \n",
    "    #Default: Exclude no features\n",
    "    if len(features) == 0:\n",
    "        features = np.ones([ligand_bind_features.shape[1],]).astype(bool)\n",
    "        \n",
    "    #Arranging the features table by the CV order, for each model\n",
    "    #features_pred_dfs = dict.fromkeys(classifiers.keys())\n",
    "    \n",
    "    models_req_scaling = [\"SVM\", \"KNN\"]\n",
    "    \n",
    "    classifier = classifier_method\n",
    "    model = clf\n",
    "    #print \"classifier_method = \" + classifier_method\n",
    "    #print \"ligand = \" + ligand\n",
    "    print model.get_xgb_params()\n",
    "    #features_pred_dfs[classifier] = pd.DataFrame()\n",
    "        \n",
    "    #Create X and y with included features\n",
    "    X = pd.concat([ligand_bind_features.iloc[:,features], ligand_negatives_features.iloc[:,features]])\n",
    "\n",
    "    y = [1] * ligand_bind_features.shape[0]\n",
    "    y.extend([0] * ligand_negatives_features.shape[0])\n",
    "    y = np.array(y)\n",
    "\n",
    "    binding_skf = StratifiedKFold(n_splits=10, shuffle=True, random_state=0)\n",
    "    pred_idx = 1\n",
    "\n",
    "    for train_index, test_index in binding_skf.split(X, y):\n",
    "            \n",
    "        print \"fold #: \"+str(pred_idx)\n",
    "        X_train, X_test = X.iloc[train_index,:], X.iloc[test_index,:]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "        \n",
    "        X_train_sampled = X_train\n",
    "        y_train_sampled = y_train\n",
    "            \n",
    "            \n",
    "        #early_stopping validation set \n",
    "        #7.8 Early stopping and Algorithm 7.2\n",
    "        #http://egrcc.github.io/docs/dl/deeplearningbook-regularization.pdf\n",
    "            \n",
    "        #X_subtrain, X_valid, y_subtrain, y_valid = train_test_split(X_train_sampled, \n",
    "        #                                y_train_sampled, stratify=y_train_sampled, test_size = .1)\n",
    "            \n",
    "        #fit to training data\n",
    "        #model = classifiers[classifier]\n",
    "        model.fit(X_train_sampled, y_train_sampled)#,\n",
    "                      #eval_set = [(X_valid,y_valid)],eval_metric = \"map\", early_stopping_rounds = 50,verbose = False)\n",
    "        #print model.best_ntree_limit\n",
    "        probs_list = []\n",
    "\n",
    "        #probs = model.predict(X_test)\n",
    "        #probs_list = probs\n",
    "            \n",
    "        probs = model.predict_proba(X_test) #,ntree_limit=model.best_ntree_limit)\n",
    "        for l in probs:\n",
    "            probs_list.append(l[1])\n",
    "                \n",
    "        pred_dict[\"obs\"].extend(y_test)\n",
    "        pred_dict[\"prob\"].extend(probs_list)\n",
    "        fold_list = [pred_idx] * len(probs_list)\n",
    "        pred_dict[\"fold\"].extend(fold_list)\n",
    "\n",
    "        model_list = [classifier] * len(probs_list)\n",
    "        pred_dict[\"model\"].extend(model_list)\n",
    "\n",
    "        #Update auc auprc dictionaries\n",
    "        auc_dict[classifier].append(roc_auc_score(y_test, probs[:, 1]))\n",
    "        precision, recall, _ = precision_recall_curve(y_test, probs[:, 1])\n",
    "            \n",
    "        #auc_dict[classifier].append(roc_auc_score(y_test, probs))\n",
    "        #precision, recall, _ = precision_recall_curve(y_test, probs)\n",
    "            \n",
    "        auprc_dict[classifier].append(auc(recall, precision))\n",
    "            \n",
    "        #Update features table\n",
    "        #features_pred_dfs[classifier] = features_pred_dfs[classifier].append(X_test)\n",
    "        pred_idx += 1\n",
    "            \n",
    "        print \"AUC = \"+str(auc_dict[classifier][-1])\n",
    "        print \"AUPRC = \"+str(auprc_dict[classifier][-1])\n",
    "\n",
    "    avg_auc = np.sum(auc_dict[classifier])/10.0\n",
    "    print \"avg auc = \"+str(avg_auc) \n",
    "        \n",
    "    avg_auprc = np.sum(auprc_dict[classifier])/10.0\n",
    "    print \"avg auprc = \"+str(avg_auprc)\n",
    "            \n",
    "    print \"Finished \"+ligand+\" \"+classifier\n",
    "        \n",
    "    #return Average AUPRC\n",
    "    return avg_auprc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test model functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#adapted from https://www.analyticsvidhya.com/blog/2016/03/complete-guide-parameter-tuning-xgboost-with-codes-python/\n",
    "#\n",
    "#Using cross-validation, continues adding estimators until auc does not improve within 50 rounds\n",
    "def modelfit(alg, ligand_bind_features, ligand_negatives_features, ligand_name, useTrainCV=True, cv_folds=5, early_stopping_rounds=50):\n",
    "    \n",
    "    features = np.ones([ligand_bind_features.shape[1],]).astype(bool)\n",
    "    X = pd.concat([ligand_bind_features.iloc[:,features], ligand_negatives_features.iloc[:,features]])\n",
    "\n",
    "    y = [1] * ligand_bind_features.shape[0]\n",
    "    y.extend([0] * ligand_negatives_features.shape[0])\n",
    "    y = np.array(y)\n",
    "    \n",
    "    print \"modelfit\"\n",
    "    xgb_param = alg.get_xgb_params()\n",
    "    xgtrain = xgb.DMatrix(X, label=y)\n",
    "    #print alg.get_params()['n_estimators']\n",
    "    \n",
    "    #metrics can be changed to a variety of things including \"map\" for Mean Average Precision (same as AveragePRC) \n",
    "    cvresult = xgb.cv(xgb_param, xgtrain, num_boost_round=alg.get_params()['n_estimators'], nfold=cv_folds, \n",
    "                      metrics='auc', early_stopping_rounds=early_stopping_rounds,verbose_eval =True)\n",
    "    alg.set_params(n_estimators=cvresult.shape[0])\n",
    "    print \"Optimal n_estimators: \" + str(cvresult.shape[0])\n",
    "    \n",
    "    return alg,cvresult#,dtrain_predictions,dtrain_predprob,alg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "76\n"
     ]
    }
   ],
   "source": [
    "ligand_bind_features = ligands_features_df[ligand]\n",
    "ligand_negatives_features = ligands_negatives_df[ligand]\n",
    "features = features = np.ones([ligand_bind_features.shape[1],]).astype(bool)\n",
    "X = pd.concat([ligand_bind_features.iloc[:,features], ligand_negatives_features.iloc[:,features]])\n",
    "y = [1] * ligand_bind_features.shape[0]\n",
    "y.extend([0] * ligand_negatives_features.shape[0])\n",
    "y = np.array(y)\n",
    "\n",
    "#Calcuate a good value for scale_pos_weight, which gives that value to positive examples. \n",
    "#A common value is the ratio of number of negative examples to number of positive examples (#neg/#pos)\n",
    "\n",
    "val_scale_pos_weight = len([y[i] for i in range(len(y)) if y[i]==0])/len([y[i] for i in range(len(y)) if y[i]==1])\n",
    "print val_scale_pos_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "modelfit\n",
      "[0]\ttrain-map:0.364542+0.0191594\ttest-map:0.148676+0.0308632\n",
      "[1]\ttrain-map:0.60257+0.0505605\ttest-map:0.211796+0.0406174\n",
      "[2]\ttrain-map:0.735801+0.0374324\ttest-map:0.250428+0.0251336\n",
      "[3]\ttrain-map:0.807666+0.0230271\ttest-map:0.285107+0.024937\n",
      "[4]\ttrain-map:0.849375+0.0257286\ttest-map:0.300232+0.0250184\n",
      "[5]\ttrain-map:0.887256+0.0231454\ttest-map:0.328638+0.0338323\n",
      "[6]\ttrain-map:0.904911+0.022045\ttest-map:0.359593+0.040778\n",
      "[7]\ttrain-map:0.920043+0.0137323\ttest-map:0.35917+0.0427617\n",
      "[8]\ttrain-map:0.933557+0.0181996\ttest-map:0.368494+0.038123\n",
      "[9]\ttrain-map:0.943885+0.0141974\ttest-map:0.386678+0.0384341\n",
      "[10]\ttrain-map:0.951101+0.0128876\ttest-map:0.401048+0.0419737\n",
      "[11]\ttrain-map:0.955613+0.0108501\ttest-map:0.411558+0.0438196\n",
      "[12]\ttrain-map:0.960014+0.00860242\ttest-map:0.415325+0.0373681\n",
      "[13]\ttrain-map:0.96522+0.00916513\ttest-map:0.431717+0.0343965\n",
      "[14]\ttrain-map:0.969651+0.00992425\ttest-map:0.440803+0.0383049\n",
      "[15]\ttrain-map:0.973572+0.00710258\ttest-map:0.442401+0.0374592\n",
      "[16]\ttrain-map:0.97648+0.00616803\ttest-map:0.445647+0.03835\n",
      "[17]\ttrain-map:0.981237+0.00567651\ttest-map:0.455743+0.0400304\n",
      "[18]\ttrain-map:0.983328+0.00405508\ttest-map:0.453221+0.0419141\n",
      "[19]\ttrain-map:0.983979+0.00498614\ttest-map:0.460104+0.039952\n",
      "[20]\ttrain-map:0.986126+0.0047045\ttest-map:0.462806+0.0392338\n",
      "[21]\ttrain-map:0.987431+0.00390727\ttest-map:0.459544+0.0388399\n",
      "[22]\ttrain-map:0.988994+0.00263919\ttest-map:0.463123+0.0388748\n",
      "[23]\ttrain-map:0.990328+0.00199278\ttest-map:0.464974+0.0384767\n",
      "[24]\ttrain-map:0.991006+0.00183887\ttest-map:0.469241+0.0339896\n",
      "[25]\ttrain-map:0.991494+0.00179113\ttest-map:0.470051+0.0331144\n",
      "[26]\ttrain-map:0.992461+0.00149363\ttest-map:0.478149+0.0349581\n",
      "[27]\ttrain-map:0.993446+0.00080345\ttest-map:0.483245+0.0335428\n",
      "[28]\ttrain-map:0.994598+0.00152781\ttest-map:0.484401+0.0317791\n",
      "[29]\ttrain-map:0.994917+0.00150063\ttest-map:0.484712+0.0311365\n",
      "[30]\ttrain-map:0.995587+0.00186227\ttest-map:0.486629+0.0328157\n",
      "[31]\ttrain-map:0.996075+0.00177484\ttest-map:0.493306+0.0332679\n",
      "[32]\ttrain-map:0.996819+0.00134332\ttest-map:0.495997+0.036565\n",
      "[33]\ttrain-map:0.997134+0.00125673\ttest-map:0.500997+0.0329967\n",
      "[34]\ttrain-map:0.997711+0.00107495\ttest-map:0.508759+0.0296055\n",
      "[35]\ttrain-map:0.998164+0.00102289\ttest-map:0.510848+0.0326783\n",
      "[36]\ttrain-map:0.998402+0.000892997\ttest-map:0.509363+0.032685\n",
      "[37]\ttrain-map:0.998683+0.000718012\ttest-map:0.509837+0.0322906\n",
      "[38]\ttrain-map:0.998972+0.000535304\ttest-map:0.511446+0.0281835\n",
      "[39]\ttrain-map:0.999196+0.000451049\ttest-map:0.511916+0.0270763\n",
      "[40]\ttrain-map:0.999313+0.000400588\ttest-map:0.513426+0.0274216\n",
      "[41]\ttrain-map:0.999398+0.000391347\ttest-map:0.513057+0.0252601\n",
      "[42]\ttrain-map:0.999479+0.000367294\ttest-map:0.516597+0.0247646\n",
      "[43]\ttrain-map:0.999577+0.000398563\ttest-map:0.51812+0.0239245\n",
      "[44]\ttrain-map:0.999653+0.000395791\ttest-map:0.520929+0.0266317\n",
      "[45]\ttrain-map:0.999744+0.000272943\ttest-map:0.525728+0.0240179\n",
      "[46]\ttrain-map:0.999763+0.000287214\ttest-map:0.526688+0.0224661\n",
      "[47]\ttrain-map:0.999867+0.000154717\ttest-map:0.52688+0.0234611\n",
      "[48]\ttrain-map:0.999917+8.05571e-05\ttest-map:0.529032+0.0250402\n",
      "[49]\ttrain-map:0.999945+5.5171e-05\ttest-map:0.529577+0.0259956\n",
      "[50]\ttrain-map:0.999954+4.86728e-05\ttest-map:0.530651+0.0261528\n",
      "[51]\ttrain-map:0.999972+3.51704e-05\ttest-map:0.534088+0.0284736\n",
      "[52]\ttrain-map:0.999975+3.01503e-05\ttest-map:0.533945+0.0283258\n",
      "[53]\ttrain-map:0.999974+3.03618e-05\ttest-map:0.533861+0.0284531\n",
      "[54]\ttrain-map:0.999974+2.86956e-05\ttest-map:0.533434+0.029688\n",
      "[55]\ttrain-map:0.999977+2.8886e-05\ttest-map:0.533159+0.0288435\n",
      "[56]\ttrain-map:0.999992+1.20764e-05\ttest-map:0.532196+0.0298947\n",
      "[57]\ttrain-map:0.999993+9.86712e-06\ttest-map:0.534561+0.0324786\n",
      "[58]\ttrain-map:0.999996+5.15364e-06\ttest-map:0.53599+0.0328732\n",
      "[59]\ttrain-map:0.999998+2.93939e-06\ttest-map:0.536596+0.0319458\n",
      "[60]\ttrain-map:0.999998+2.93939e-06\ttest-map:0.537126+0.0321291\n",
      "[61]\ttrain-map:1+0\ttest-map:0.537309+0.0320035\n",
      "[62]\ttrain-map:1+0\ttest-map:0.536975+0.0346184\n",
      "[63]\ttrain-map:1+0\ttest-map:0.538259+0.032349\n",
      "[64]\ttrain-map:1+0\ttest-map:0.539616+0.0327735\n",
      "[65]\ttrain-map:1+0\ttest-map:0.540103+0.0323198\n",
      "[66]\ttrain-map:1+0\ttest-map:0.541743+0.0330375\n",
      "[67]\ttrain-map:1+0\ttest-map:0.543178+0.0330071\n",
      "[68]\ttrain-map:1+0\ttest-map:0.54358+0.0337823\n",
      "[69]\ttrain-map:1+0\ttest-map:0.542177+0.0331156\n",
      "[70]\ttrain-map:1+0\ttest-map:0.54277+0.0322193\n",
      "[71]\ttrain-map:1+0\ttest-map:0.543034+0.0326217\n",
      "[72]\ttrain-map:1+0\ttest-map:0.542293+0.0321746\n",
      "[73]\ttrain-map:1+0\ttest-map:0.542898+0.0326581\n",
      "[74]\ttrain-map:1+0\ttest-map:0.544416+0.0331469\n",
      "[75]\ttrain-map:1+0\ttest-map:0.545052+0.0340435\n",
      "[76]\ttrain-map:1+0\ttest-map:0.546062+0.0339128\n",
      "[77]\ttrain-map:1+0\ttest-map:0.545991+0.0329034\n",
      "[78]\ttrain-map:1+0\ttest-map:0.544823+0.0343676\n",
      "[79]\ttrain-map:1+0\ttest-map:0.54556+0.0336732\n",
      "[80]\ttrain-map:1+0\ttest-map:0.544132+0.0335531\n",
      "[81]\ttrain-map:1+0\ttest-map:0.546682+0.0332696\n",
      "[82]\ttrain-map:1+0\ttest-map:0.545599+0.0332774\n",
      "[83]\ttrain-map:1+0\ttest-map:0.545243+0.0330702\n",
      "[84]\ttrain-map:1+0\ttest-map:0.544646+0.0335558\n",
      "[85]\ttrain-map:1+0\ttest-map:0.545521+0.032932\n",
      "[86]\ttrain-map:1+0\ttest-map:0.54463+0.0343268\n",
      "[87]\ttrain-map:1+0\ttest-map:0.545721+0.0346808\n",
      "[88]\ttrain-map:1+0\ttest-map:0.545863+0.0340393\n",
      "[89]\ttrain-map:1+0\ttest-map:0.545033+0.0340031\n",
      "[90]\ttrain-map:1+0\ttest-map:0.545961+0.034576\n",
      "[91]\ttrain-map:1+0\ttest-map:0.547454+0.0345923\n",
      "[92]\ttrain-map:1+0\ttest-map:0.549195+0.0347853\n",
      "[93]\ttrain-map:1+0\ttest-map:0.549843+0.0349888\n",
      "[94]\ttrain-map:1+0\ttest-map:0.549146+0.0352731\n",
      "[95]\ttrain-map:1+0\ttest-map:0.547923+0.0352485\n",
      "[96]\ttrain-map:1+0\ttest-map:0.547536+0.0363554\n",
      "[97]\ttrain-map:1+0\ttest-map:0.548673+0.0362294\n",
      "[98]\ttrain-map:1+0\ttest-map:0.549699+0.0369894\n",
      "[99]\ttrain-map:1+0\ttest-map:0.550054+0.0370875\n",
      "[100]\ttrain-map:1+0\ttest-map:0.550466+0.0375757\n",
      "[101]\ttrain-map:1+0\ttest-map:0.550001+0.0370271\n",
      "[102]\ttrain-map:1+0\ttest-map:0.550822+0.0383255\n",
      "[103]\ttrain-map:1+0\ttest-map:0.549367+0.0383134\n",
      "[104]\ttrain-map:1+0\ttest-map:0.54935+0.037812\n",
      "[105]\ttrain-map:1+0\ttest-map:0.549638+0.0383903\n",
      "[106]\ttrain-map:1+0\ttest-map:0.550172+0.0382175\n",
      "[107]\ttrain-map:1+0\ttest-map:0.550733+0.0381191\n",
      "[108]\ttrain-map:1+0\ttest-map:0.55034+0.0391518\n",
      "[109]\ttrain-map:1+0\ttest-map:0.550485+0.0394581\n",
      "[110]\ttrain-map:1+0\ttest-map:0.550885+0.0396488\n",
      "[111]\ttrain-map:1+0\ttest-map:0.550522+0.0383512\n",
      "[112]\ttrain-map:1+0\ttest-map:0.551096+0.0400119\n",
      "[113]\ttrain-map:1+0\ttest-map:0.551719+0.0393561\n",
      "[114]\ttrain-map:1+0\ttest-map:0.551605+0.0388807\n",
      "[115]\ttrain-map:1+0\ttest-map:0.55192+0.0384667\n",
      "[116]\ttrain-map:1+0\ttest-map:0.551782+0.0391412\n",
      "[117]\ttrain-map:1+0\ttest-map:0.55228+0.0377601\n",
      "[118]\ttrain-map:1+0\ttest-map:0.553359+0.036908\n",
      "[119]\ttrain-map:1+0\ttest-map:0.553549+0.038334\n",
      "[120]\ttrain-map:1+0\ttest-map:0.552895+0.0368666\n",
      "[121]\ttrain-map:1+0\ttest-map:0.552869+0.0361095\n",
      "[122]\ttrain-map:1+0\ttest-map:0.55394+0.0364296\n",
      "[123]\ttrain-map:1+0\ttest-map:0.553739+0.0365865\n",
      "[124]\ttrain-map:1+0\ttest-map:0.554758+0.036466\n",
      "[125]\ttrain-map:1+0\ttest-map:0.55585+0.0363136\n",
      "[126]\ttrain-map:1+0\ttest-map:0.556695+0.0365856\n",
      "[127]\ttrain-map:1+0\ttest-map:0.557827+0.0366652\n",
      "[128]\ttrain-map:1+0\ttest-map:0.558478+0.0370264\n",
      "[129]\ttrain-map:1+0\ttest-map:0.558743+0.0366879\n",
      "[130]\ttrain-map:1+0\ttest-map:0.558511+0.0369681\n",
      "[131]\ttrain-map:1+0\ttest-map:0.559332+0.0375299\n",
      "[132]\ttrain-map:1+0\ttest-map:0.560179+0.0367386\n",
      "[133]\ttrain-map:1+0\ttest-map:0.56113+0.0356162\n",
      "[134]\ttrain-map:1+0\ttest-map:0.56205+0.0359706\n",
      "[135]\ttrain-map:1+0\ttest-map:0.561568+0.0350269\n",
      "[136]\ttrain-map:1+0\ttest-map:0.561218+0.0338126\n",
      "[137]\ttrain-map:1+0\ttest-map:0.560986+0.0328183\n",
      "[138]\ttrain-map:1+0\ttest-map:0.561423+0.0325369\n",
      "[139]\ttrain-map:1+0\ttest-map:0.561549+0.0333213\n",
      "[140]\ttrain-map:1+0\ttest-map:0.561535+0.0327742\n",
      "[141]\ttrain-map:1+0\ttest-map:0.561005+0.0333227\n",
      "[142]\ttrain-map:1+0\ttest-map:0.560903+0.032814\n",
      "[143]\ttrain-map:1+0\ttest-map:0.561079+0.0332561\n",
      "[144]\ttrain-map:1+0\ttest-map:0.561605+0.032057\n",
      "[145]\ttrain-map:1+0\ttest-map:0.561898+0.032474\n",
      "[146]\ttrain-map:1+0\ttest-map:0.560271+0.0311188\n",
      "[147]\ttrain-map:1+0\ttest-map:0.561696+0.0305863\n",
      "[148]\ttrain-map:1+0\ttest-map:0.561563+0.03009\n",
      "[149]\ttrain-map:1+0\ttest-map:0.562196+0.0301671\n",
      "[150]\ttrain-map:1+0\ttest-map:0.563033+0.0308382\n",
      "[151]\ttrain-map:1+0\ttest-map:0.563211+0.0306958\n",
      "[152]\ttrain-map:1+0\ttest-map:0.564226+0.0311035\n",
      "[153]\ttrain-map:1+0\ttest-map:0.563903+0.0312941\n",
      "[154]\ttrain-map:1+0\ttest-map:0.564287+0.0311668\n",
      "[155]\ttrain-map:1+0\ttest-map:0.564621+0.0308791\n",
      "[156]\ttrain-map:1+0\ttest-map:0.563427+0.0303886\n",
      "[157]\ttrain-map:1+0\ttest-map:0.564537+0.0294431\n",
      "[158]\ttrain-map:1+0\ttest-map:0.563803+0.0302543\n",
      "[159]\ttrain-map:1+0\ttest-map:0.564314+0.0310769\n",
      "[160]\ttrain-map:1+0\ttest-map:0.564101+0.0309771\n",
      "[161]\ttrain-map:1+0\ttest-map:0.564606+0.0310495\n",
      "[162]\ttrain-map:1+0\ttest-map:0.564667+0.0315951\n",
      "[163]\ttrain-map:1+0\ttest-map:0.564677+0.0316138\n",
      "[164]\ttrain-map:1+0\ttest-map:0.56535+0.0314413\n",
      "[165]\ttrain-map:1+0\ttest-map:0.564758+0.0319594\n",
      "[166]\ttrain-map:1+0\ttest-map:0.565386+0.0310388\n",
      "[167]\ttrain-map:1+0\ttest-map:0.564367+0.0312858\n",
      "[168]\ttrain-map:1+0\ttest-map:0.564237+0.0306233\n",
      "[169]\ttrain-map:1+0\ttest-map:0.564304+0.0318224\n",
      "[170]\ttrain-map:1+0\ttest-map:0.563545+0.0320279\n",
      "[171]\ttrain-map:1+0\ttest-map:0.564214+0.0316058\n",
      "[172]\ttrain-map:1+0\ttest-map:0.564599+0.0320886\n",
      "[173]\ttrain-map:1+0\ttest-map:0.56406+0.0324675\n",
      "[174]\ttrain-map:1+0\ttest-map:0.564184+0.0321629\n",
      "[175]\ttrain-map:1+0\ttest-map:0.564219+0.0323136\n",
      "[176]\ttrain-map:1+0\ttest-map:0.563931+0.0321978\n",
      "[177]\ttrain-map:1+0\ttest-map:0.56384+0.032124\n",
      "[178]\ttrain-map:1+0\ttest-map:0.564307+0.032203\n",
      "[179]\ttrain-map:1+0\ttest-map:0.56465+0.0318824\n",
      "[180]\ttrain-map:1+0\ttest-map:0.564578+0.0314525\n",
      "[181]\ttrain-map:1+0\ttest-map:0.565177+0.0321434\n",
      "[182]\ttrain-map:1+0\ttest-map:0.565454+0.0322046\n",
      "[183]\ttrain-map:1+0\ttest-map:0.565158+0.032261\n",
      "[184]\ttrain-map:1+0\ttest-map:0.5654+0.0327286\n",
      "[185]\ttrain-map:1+0\ttest-map:0.565317+0.0326516\n",
      "[186]\ttrain-map:1+0\ttest-map:0.565334+0.0326733\n",
      "[187]\ttrain-map:1+0\ttest-map:0.565041+0.0326566\n",
      "[188]\ttrain-map:1+0\ttest-map:0.565321+0.0324895\n",
      "[189]\ttrain-map:1+0\ttest-map:0.565028+0.032323\n",
      "[190]\ttrain-map:1+0\ttest-map:0.564251+0.0321083\n",
      "[191]\ttrain-map:1+0\ttest-map:0.564703+0.0320865\n",
      "[192]\ttrain-map:1+0\ttest-map:0.564735+0.0321865\n",
      "[193]\ttrain-map:1+0\ttest-map:0.564676+0.032322\n",
      "[194]\ttrain-map:1+0\ttest-map:0.564731+0.0321216\n",
      "[195]\ttrain-map:1+0\ttest-map:0.564638+0.03254\n",
      "[196]\ttrain-map:1+0\ttest-map:0.564717+0.032614\n",
      "[197]\ttrain-map:1+0\ttest-map:0.564634+0.0321338\n",
      "[198]\ttrain-map:1+0\ttest-map:0.564463+0.0320363\n",
      "[199]\ttrain-map:1+0\ttest-map:0.564958+0.0323949\n",
      "[200]\ttrain-map:1+0\ttest-map:0.565529+0.0327814\n",
      "[201]\ttrain-map:1+0\ttest-map:0.565367+0.0326333\n",
      "[202]\ttrain-map:1+0\ttest-map:0.565066+0.0324007\n",
      "[203]\ttrain-map:1+0\ttest-map:0.565098+0.0322542\n",
      "[204]\ttrain-map:1+0\ttest-map:0.565273+0.0321962\n",
      "[205]\ttrain-map:1+0\ttest-map:0.565417+0.0322108\n",
      "[206]\ttrain-map:1+0\ttest-map:0.566248+0.0324176\n",
      "[207]\ttrain-map:1+0\ttest-map:0.566365+0.0320656\n",
      "[208]\ttrain-map:1+0\ttest-map:0.566822+0.031641\n",
      "[209]\ttrain-map:1+0\ttest-map:0.566793+0.0319447\n",
      "[210]\ttrain-map:1+0\ttest-map:0.566765+0.0320059\n",
      "[211]\ttrain-map:1+0\ttest-map:0.567035+0.0318728\n",
      "[212]\ttrain-map:1+0\ttest-map:0.566733+0.0311609\n",
      "[213]\ttrain-map:1+0\ttest-map:0.566493+0.031139\n",
      "[214]\ttrain-map:1+0\ttest-map:0.567156+0.0313899\n",
      "[215]\ttrain-map:1+0\ttest-map:0.566214+0.0310658\n",
      "[216]\ttrain-map:1+0\ttest-map:0.566553+0.0308775\n",
      "[217]\ttrain-map:1+0\ttest-map:0.566258+0.0314368\n",
      "[218]\ttrain-map:1+0\ttest-map:0.566185+0.0311851\n",
      "[219]\ttrain-map:1+0\ttest-map:0.566141+0.0311919\n",
      "[220]\ttrain-map:1+0\ttest-map:0.566085+0.0305084\n",
      "[221]\ttrain-map:1+0\ttest-map:0.565456+0.0313364\n",
      "[222]\ttrain-map:1+0\ttest-map:0.565195+0.0316509\n",
      "[223]\ttrain-map:1+0\ttest-map:0.564703+0.032215\n",
      "[224]\ttrain-map:1+0\ttest-map:0.564995+0.03173\n",
      "[225]\ttrain-map:1+0\ttest-map:0.565367+0.0315367\n",
      "[226]\ttrain-map:1+0\ttest-map:0.565119+0.0312155\n",
      "[227]\ttrain-map:1+0\ttest-map:0.565502+0.031048\n",
      "[228]\ttrain-map:1+0\ttest-map:0.565499+0.031385\n",
      "[229]\ttrain-map:1+0\ttest-map:0.565267+0.0316724\n",
      "[230]\ttrain-map:1+0\ttest-map:0.56521+0.0316826\n",
      "[231]\ttrain-map:1+0\ttest-map:0.565439+0.0316282\n",
      "[232]\ttrain-map:1+0\ttest-map:0.565532+0.0318825\n",
      "[233]\ttrain-map:1+0\ttest-map:0.565252+0.0322562\n",
      "[234]\ttrain-map:1+0\ttest-map:0.565621+0.032407\n",
      "[235]\ttrain-map:1+0\ttest-map:0.565874+0.031788\n",
      "[236]\ttrain-map:1+0\ttest-map:0.565698+0.0319248\n",
      "[237]\ttrain-map:1+0\ttest-map:0.565963+0.0318928\n",
      "[238]\ttrain-map:1+0\ttest-map:0.566138+0.032078\n",
      "[239]\ttrain-map:1+0\ttest-map:0.566038+0.0320124\n",
      "[240]\ttrain-map:1+0\ttest-map:0.565842+0.0317108\n",
      "[241]\ttrain-map:1+0\ttest-map:0.566495+0.0318332\n",
      "[242]\ttrain-map:1+0\ttest-map:0.566631+0.0321527\n",
      "[243]\ttrain-map:1+0\ttest-map:0.566357+0.0326742\n",
      "[244]\ttrain-map:1+0\ttest-map:0.566629+0.0326911\n",
      "[245]\ttrain-map:1+0\ttest-map:0.566239+0.0323741\n",
      "[246]\ttrain-map:1+0\ttest-map:0.56558+0.0321411\n",
      "[247]\ttrain-map:1+0\ttest-map:0.565546+0.032098\n",
      "[248]\ttrain-map:1+0\ttest-map:0.565929+0.0325534\n",
      "[249]\ttrain-map:1+0\ttest-map:0.566215+0.0326083\n",
      "[250]\ttrain-map:1+0\ttest-map:0.566572+0.0322758\n",
      "[251]\ttrain-map:1+0\ttest-map:0.566816+0.0319379\n",
      "[252]\ttrain-map:1+0\ttest-map:0.566682+0.0319245\n",
      "[253]\ttrain-map:1+0\ttest-map:0.566355+0.0314227\n",
      "[254]\ttrain-map:1+0\ttest-map:0.566713+0.0314357\n",
      "[255]\ttrain-map:1+0\ttest-map:0.56686+0.0312971\n",
      "[256]\ttrain-map:1+0\ttest-map:0.566965+0.0310316\n",
      "[257]\ttrain-map:1+0\ttest-map:0.566894+0.0310454\n",
      "[258]\ttrain-map:1+0\ttest-map:0.566978+0.0311564\n",
      "[259]\ttrain-map:1+0\ttest-map:0.567062+0.0310426\n",
      "[260]\ttrain-map:1+0\ttest-map:0.567068+0.0309593\n",
      "[261]\ttrain-map:1+0\ttest-map:0.566949+0.0309808\n",
      "[262]\ttrain-map:1+0\ttest-map:0.566998+0.0314089\n",
      "[263]\ttrain-map:1+0\ttest-map:0.566969+0.0313687\n",
      "Optimal n_estimators: 215\n",
      "Optimal n_estimators for ligand(dna): 215\n",
      "CPU times: user 9min 38s, sys: 2.04 s, total: 9min 40s\n",
      "Wall time: 2min 30s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "#Uses cross validation and early stopping to find best number of estimators\n",
    "xgb1 = XGBClassifier(\n",
    "     learning_rate =0.1,\n",
    "     n_estimators=1000,\n",
    "     max_depth=9,\n",
    "     min_child_weight=0,\n",
    "     gamma=0,\n",
    "     subsample=1.0,\n",
    "     colsample_bytree=1.0,\n",
    "     objective= 'binary:logistic',\n",
    "     n_jobs=-1,\n",
    "     scale_pos_weight=val_scale_pos_weight,\n",
    "     random_state=0)\n",
    "\n",
    "#returns = modelfit(xgb1, ligands_features_df[ligand], ligands_negatives_df[ligand], ligand)\n",
    "returns = [xgb1,np.zeros(141)]\n",
    "print \"Optimal n_estimators for ligand(\"+str(ligand)+\"): \"+str(returns[1].shape[0]) \n",
    "optimized_n_est = returns[1].shape[0] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "#Search for best values of max_depth, min_child_weight, and gamma (which are all related to complexity of the model)\n",
    "#in the given ranges, which can be changed by changing the ranges in the for-loops\n",
    "#Multithreading is used to speed up the process\n",
    "\n",
    "#Dummy module is the same as multiprocessing, but is for multithreading\n",
    "#from multiprocessing.dummy import Pool as ThreadPool\n",
    "#pool = ThreadPool(20)\n",
    "\n",
    "#Helper function that takes a classifier and runs test_model and returns the Average PRC (score)\n",
    "def test_model_for_map(clf):\n",
    "    pred_dict = defaultdict(list)\n",
    "    auc_dict = defaultdict(list)\n",
    "    auprc_dict = defaultdict(list)\n",
    "    return test_model(clf, pred_dict, auc_dict, auprc_dict, ligands_features_df[ligand], ligands_negatives_df[ligand])\n",
    "\n",
    "classifiers = []\n",
    "params = {}\n",
    "\n",
    "#create an XGB classifier with the given parameters. \n",
    "#colsample_bytree and subsample are fixed to 0.8.\n",
    "classifiers.append(XGBClassifier(n_estimators=optimized_n_est, n_jobs = -1, random_state=0, \n",
    "    max_depth=max_depth, min_child_weight=min_child_weight, gamma = gam,\n",
    "    colsample_bytree=.8, subsample=.8, scale_pos_weight = val_scale_pos_weight))\n",
    "#Keep a dictionary of a string of the parameters and the actual values\n",
    "#params[\"{max_depth: \" + str(max_depth) + \", min_child_weight: \" + str(min_child_weight) +  \", gamma: \" + str(gam)+\"}\"] = [max_depth,min_child_weight,gam]\n",
    "\n",
    "#runs test_model_for_map function on each classifier in classifiers\n",
    "#results = pool.map(test_model_for_map,classifiers)\n",
    "#pool.close()\n",
    "#pool.join()\n",
    "\n",
    "\n",
    "#Print out params and their Average AUPRC and get parameter which had best Average AUPRC (score)\n",
    "#best_score = 0\n",
    "#best_param = \"\"\n",
    "#ps = params.keys()\n",
    "#for i in range(0,len(classifiers),1):\n",
    "#   print str(ps[i]) + \": \"+  str(results[i])\n",
    "#    if(results[i] > best_score):\n",
    "#        best_param = ps[i]\n",
    "#        best_score = results[i]\n",
    "        \n",
    "print \"\\nBest params by AUPRC: \" + best_param + \": \" +  str(best_score)\n",
    "\n",
    "best_vals = params[best_param]\n",
    "opt_max_depth = best_vals[0]\n",
    "opt_min_child_weight = best_vals[1]\n",
    "opt_gamma = best_vals[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "%%time\n",
    "\n",
    "#Search for best values of max_depth, min_child_weight, and gamma (which are all related to complexity of the model)\n",
    "#in the given ranges, which can be changed by changing the ranges in the for-loops\n",
    "#Multithreading is used to speed up the process\n",
    "\n",
    "#Dummy module is the same as multiprocessing, but is for multithreading\n",
    "from multiprocessing.dummy import Pool as ThreadPool\n",
    "pool = ThreadPool(20)\n",
    "\n",
    "#Helper function that takes a classifier and runs test_model and returns the Average PRC (score)\n",
    "def test_model_for_map(clf):\n",
    "    pred_dict = defaultdict(list)\n",
    "    auc_dict = defaultdict(list)\n",
    "    auprc_dict = defaultdict(list)\n",
    "    return test_model(clf, pred_dict, auc_dict, auprc_dict, ligands_features_df[ligand], ligands_negatives_df[ligand])\n",
    "\n",
    "classifiers = []\n",
    "params = {}\n",
    "\n",
    "#10,6,10\n",
    "for max_depth in range(3,10,1):\n",
    "    for min_child_weight in range(0,6,1):\n",
    "        for gam in [i/10.0 for i in range(0,10,1)]:\n",
    "            #create an XGB classifier with the given parameters. \n",
    "            #colsample_bytree and subsample are fixed to 0.8.\n",
    "            classifiers.append(XGBClassifier(n_estimators=optimized_n_est, n_jobs = -1, random_state=0, \n",
    "                max_depth=max_depth, min_child_weight=min_child_weight, gamma = gam,\n",
    "                colsample_bytree=1.0, subsample=1.0, scale_pos_weight = val_scale_pos_weight))\n",
    "            #Keep a dictionary of a string of the parameters and the actual values\n",
    "            params[\"{max_depth: \" + str(max_depth) + \", min_child_weight: \" + str(min_child_weight) +  \", gamma: \" + str(gam)+\"}\"] = [max_depth,min_child_weight,gam]\n",
    "\n",
    "#runs test_model_for_map function on each classifier in classifiers\n",
    "results = pool.map(test_model_for_map,classifiers)\n",
    "pool.close()\n",
    "pool.join()\n",
    "\n",
    "\n",
    "#Print out params and their Average AUPRC and get parameter which had best Average AUPRC (score)\n",
    "best_score = 0\n",
    "best_param = \"\"\n",
    "ps = params.keys()\n",
    "for i in range(0,len(classifiers),1):\n",
    "    print str(ps[i]) + \": \"+  str(results[i])\n",
    "    if(results[i] > best_score):\n",
    "        best_param = ps[i]\n",
    "        best_score = results[i]\n",
    "        \n",
    "print \"\\nBest params by AUPRC: \" + best_param + \": \" +  str(best_score)\n",
    "\n",
    "best_vals = params[best_param]\n",
    "opt_max_depth = best_vals[0]\n",
    "opt_min_child_weight = best_vals[1]\n",
    "opt_gamma = best_vals[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "%%time\n",
    "\n",
    "#Search for best values of subsample and colsample_bytree (which are all related to robustness of the model)\n",
    "#in the given ranges, which can be changed by changing the ranges in the for-loops\n",
    "#Multithreading is used to speed up the process\n",
    "\n",
    "pool2 = ThreadPool(20)\n",
    "classifiers = []\n",
    "params = {}\n",
    "\n",
    "#10,10\n",
    "for colsamp in range(1,10,1):\n",
    "    for subsamp in range(1,10,1):\n",
    "        #create an XGB classifier with the given parameters. \n",
    "        #max_depth, min_child_weight, and gamma are the best values from the previous section\n",
    "        classifiers.append(XGBClassifier(n_estimators=optimized_n_est, n_jobs = -1, random_state=0, \n",
    "            max_depth=opt_max_depth, gamma = .1, min_child_weight=opt_min_child_weight, \n",
    "            colsample_bytree=colsamp/10.0, subsample=subsamp/10.0, scale_pos_weight = val_scale_pos_weight,))\n",
    "        params[\"{colsample_bytree: \" + str(colsamp/10.0) + \", subsample:\" + str(subsamp/10.0) + \"}\"] = [colsamp/10.0,subsamp/10.0]\n",
    "\n",
    "results = pool2.map(test_model_for_map,classifiers)\n",
    "pool2.close()\n",
    "pool2.join()\n",
    "\n",
    "best_score = 0\n",
    "best_param = \"\"\n",
    "ps = params.keys()\n",
    "for i in range(0,len(classifiers),1):\n",
    "    print str(ps[i]) + \": \"+  str(results[i])\n",
    "    if(results[i] > best_score):\n",
    "        best_param = ps[i]\n",
    "        best_score = results[i]\n",
    "        \n",
    "print \"\\nBest params by AUPRC: \" + best_param + \": \" +  str(best_score)\n",
    "\n",
    "best_vals = params[best_param]\n",
    "opt_colsample_bytree = best_vals[0]\n",
    "opt_subsample = best_vals[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print \"Best params for {0}:\\nmax_depth = {1}\\nmin_child_weight = {2}\\ngamma = {3}\\nsubsample = {4}\\ncolsample_bytree = {5}\".format(ligand,opt_max_depth,opt_min_child_weight,opt_gamma,opt_subsample,opt_colsample_bytree)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
