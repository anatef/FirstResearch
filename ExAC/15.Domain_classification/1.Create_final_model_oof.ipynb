{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create oof predictions for each test fold, using final model hyperparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Basic imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from collections import defaultdict\n",
    "from os import environ, getcwd\n",
    "import sys\n",
    "\n",
    "curr_dir = getcwd()\n",
    "\n",
    "#Import 10.Prediction utils functions\n",
    "sys.path.append(curr_dir+\"/../10.Prediction/utils\")\n",
    "from prop_threshold_funcs import create_positives_datasets_combined, create_negatives_datasets_combined\n",
    "from prediction_general_funcs import get_features_cols, remove_unimportant_features\n",
    "from CV_funcs import calc_CV_idx_iterative\n",
    "from tuning_helper_functions import generate_model\n",
    "\n",
    "#Import 10.Prediction/stacking utils functions\n",
    "sys.path.append(curr_dir+\"/../10.Prediction/stacking/utils\")\n",
    "from stacking_funcs import create_stacked_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ligand = ion\n",
      "ens = MODEL\n",
      "heldout fold = 5\n",
      "test fold = 4\n",
      "classifier_method = XGB\n"
     ]
    }
   ],
   "source": [
    "#Reading the ligand input\n",
    "try:\n",
    "    ligand = environ['ligand']\n",
    "except:\n",
    "    ligand = \"ion\"\n",
    "print \"ligand = \"+ligand\n",
    "\n",
    "#Reading the ensemble type\n",
    "try: \n",
    "    ens = environ['ens']\n",
    "except:\n",
    "    ens = \"MODEL\"\n",
    "print \"ens = \"+ens\n",
    "\n",
    "#Reading the heldout fold\n",
    "try: \n",
    "    heldout_fold = environ['heldout_fold']\n",
    "except:\n",
    "    heldout_fold = \"5\"\n",
    "print \"heldout fold = \"+heldout_fold\n",
    "\n",
    "#Reading the test fold\n",
    "try:\n",
    "    test_fold = environ['test_fold']\n",
    "except:\n",
    "    test_fold = \"4\"\n",
    "print \"test fold = \"+test_fold\n",
    "\n",
    "#Reading the classifier input\n",
    "try: \n",
    "    classifier_method = environ['classifier']\n",
    "except:\n",
    "    classifier_method = \"XGB\"\n",
    "print \"classifier_method = \"+classifier_method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters = {}\n",
    "ligands = [\"rna\", \"dna\", \"ion\", \"peptide\", \"sm\"]\n",
    "all_models = [\"XGB\", \"SVM\", \"RF\", \"Logistic\", \"NN\"]\n",
    "if (ens == \"LIGAND\"):\n",
    "    from generate_stacking2nd_ligand_features_dict_global_auprc import generate_models_dict\n",
    "    hyperparameters[\"ligands\"] = [ligand]\n",
    "    hyperparameters[\"models\"] = all_models\n",
    "    hyperparameters[\"keep\"] = True\n",
    "elif (ens == \"PROB\"):\n",
    "    from generate_stacking2nd_just_probs_dict_global_auprc import generate_models_dict\n",
    "    hyperparameters[\"ligands\"] = ligands\n",
    "    hyperparameters[\"models\"] = all_models\n",
    "    hyperparameters[\"keep\"] = False\n",
    "elif (ens == \"MODEL\"):\n",
    "    from generate_stacking2nd_model_features_dict_global_auprc import generate_models_dict\n",
    "    hyperparameters[\"ligands\"] = ligands\n",
    "    hyperparameters[\"models\"] = [\"XGB\"]\n",
    "    hyperparameters[\"keep\"] = True\n",
    "else:\n",
    "    from generate_stacking2nd_all_features_dict_global_auprc import generate_models_dict\n",
    "    hyperparameters[\"ligands\"] = ligands\n",
    "    hyperparameters[\"models\"] = all_models\n",
    "    hyperparameters[\"keep\"] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all samples positions #: 44872\n"
     ]
    }
   ],
   "source": [
    "pfam_version = \"31\"\n",
    "datafile_date = \"08.06.18\"\n",
    "input_path = curr_dir+\"/../10.Prediction/domains_similarity/filtered_features_table/\"\n",
    "filename = \"windowed_positions_features_mediode_filter_\"+datafile_date+\".csv\"\n",
    "\n",
    "zero_prop = True\n",
    "no_prop = True\n",
    "all_ligands = False\n",
    "folds_num = 5\n",
    "\n",
    "\n",
    "#Features table\n",
    "features_all = pd.read_csv(input_path+filename, sep='\\t', index_col=0)\n",
    "#Features columns names, without the labels (the binding scores)\n",
    "features_cols = get_features_cols(features_all)\n",
    "print \"all samples positions #: \"+str(features_all.shape[0])\n",
    "\n",
    "#CV splits dictionary\n",
    "with open(curr_dir+\"/../10.Prediction/CV_splits/pfam-v\"+pfam_version+\"/domain_\"+str(folds_num)+\"_folds_combined_dna0.5_rna0.5_ion0.75_prec_dict.pik\", 'rb') as handle:\n",
    "    splits_dict = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of features before removal: 761\n",
      "# of features after removal: 753\n"
     ]
    }
   ],
   "source": [
    "print \"# of features before removal: \"+str(len(features_cols))\n",
    "remove_unimportant_features(features_all, features_cols, update_features_cols=True)\n",
    "print \"# of features after removal: \"+str(len(features_cols))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dna non-binding #:43886\n",
      "dnabase non-binding #:44418\n",
      "dnabackbone non-binding #:44021\n",
      "dna combined non binding #: 43884\n",
      "rna non-binding #:43727\n",
      "rnabase non-binding #:44154\n",
      "rnabackbone non-binding #:43944\n",
      "rna combined non binding #: 43720\n",
      "peptide non-binding #:41105\n",
      "ion non-binding #:39630\n",
      "metabolite non-binding #:39638\n",
      "druglike non-binding #:35018\n",
      "sm non-binding #:32697\n"
     ]
    }
   ],
   "source": [
    "ligands_negatives_df = create_negatives_datasets_combined(zero_prop, no_prop, features_all, features_cols, all_ligands)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dna #: 369\n",
      "dnabase #: 161\n",
      "dnabackbone #: 245\n",
      "dna combined #: 397\n",
      "rna #: 206\n",
      "rnabase #: 118\n",
      "rnabackbone #: 136\n",
      "rna combined #: 247\n",
      "peptide #: 436\n",
      "ion #: 351\n",
      "metabolite #: 522\n",
      "druglike #: 763\n",
      "sm #: 825\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/anat/anaconda2/lib/python2.7/site-packages/pandas/core/frame.py:1997: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  \"DataFrame index.\", UserWarning)\n"
     ]
    }
   ],
   "source": [
    "ligands_positives_df = create_positives_datasets_combined(features_all, features_cols, all_ligands)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_features = len(features_cols)\n",
    "models_dict = generate_models_dict(ligand, classifier_method, ligands, ligands_positives_df, ligands_negatives_df, folds_num, no_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#(features) = 758\n"
     ]
    }
   ],
   "source": [
    "training_stacking_path = curr_dir+\"/../10.Prediction/stacking/1st_level_pred/08.06.18_comb_dna0.5_rna0.5_ion0.75/global_auprc/all_combined/\"+heldout_fold+\"/\"\n",
    "(training_positivies, training_negatives) = create_stacked_dataset(training_stacking_path, hyperparameters[\"ligands\"], hyperparameters[\"models\"], \n",
    "                                                                   ligands_positives_df[ligand], ligands_negatives_df[ligand], all_models, keep_original_features= hyperparameters[\"keep\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model_iterative_fixed(pred_dict, ligand_bind_features, ligand_negatives_features, ligand_name, features=[]):\n",
    "    \n",
    "    \"\"\"\n",
    "    Test different models in 10-folds cross-validation.\n",
    "    \"\"\"\n",
    "    \n",
    "    #Default: Exclude no features\n",
    "    if len(features) == 0:\n",
    "        features = np.ones([ligand_bind_features.shape[1],]).astype(bool)\n",
    "        \n",
    "    #Arranging the features table by the CV order, for each model\n",
    "    features_pred_dfs = dict.fromkeys(models_dict.keys())\n",
    "    \n",
    "    models_req_scaling = [\"SVM\", \"KNN\", \"Logistic\", \"NN\"]\n",
    "\n",
    "    classifier = classifier_method\n",
    "    features_pred_dfs[classifier] = pd.DataFrame()\n",
    "\n",
    "    #Create X and y with included features\n",
    "    X = pd.concat([ligand_bind_features.iloc[:,features], ligand_negatives_features.iloc[:,features]])\n",
    "    y = [1] * ligand_bind_features.shape[0]\n",
    "    y.extend([0] * ligand_negatives_features.shape[0])\n",
    "    y = np.array(y)\n",
    "    y_df = pd.DataFrame(y)\n",
    "    y_df.index = X.index\n",
    "    y_df.columns = [\"label\"]\n",
    "    \n",
    "    #Get the heldout fold indices and seperate them from the rest of the folds\n",
    "    cv_idx = calc_CV_idx_iterative(X, splits_dict)\n",
    "    heldout_k = (int(heldout_fold)-1)\n",
    "    heldout_idx = heldout_k+1\n",
    "    print \"heldout fold #: \"+str(heldout_idx)\n",
    "    #Divide the data accordingly\n",
    "    heldout_index = cv_idx[heldout_k][\"test\"]\n",
    "    all_train_index = cv_idx[heldout_k][\"train\"]\n",
    "    X_all_train, X_heldout = X.loc[all_train_index,:], X.loc[heldout_index,:]\n",
    "    y_all_train, y_heldout = y_df.loc[all_train_index,:], y_df.loc[heldout_index,:]\n",
    "    print \"all training size = \"+str(X_all_train.shape)\n",
    "    \n",
    "    #Get the test fold indices and define the other training folds as the training\n",
    "    test_k = (int(test_fold)-1)\n",
    "    test_idx = test_k+1\n",
    "    print \"test fold #: \"+str(test_idx)\n",
    "    #Divide the data accordingly\n",
    "    test_index = cv_idx[test_k][\"test\"]\n",
    "    train_index = pd.Index.difference(X_all_train.index, cv_idx[test_k][\"test\"]) #remove from \"all_training\" idx the test indices\n",
    "    X_train, X_test = X.loc[train_index,:], X.loc[test_index,:]\n",
    "    y_train, y_test = y_df.loc[train_index,:], y_df.loc[test_index,:]\n",
    "    print \"train size = \"+str(X_train.shape)\n",
    "    print \"test size = \"+str(X_test.shape)\n",
    "    \n",
    "    if (classifier in models_req_scaling):\n",
    "        cols = X_train.columns\n",
    "        scaler = StandardScaler() \n",
    "        #scale only using the training data\n",
    "        scaler.fit(X_train) \n",
    "        X_train = pd.DataFrame(scaler.transform(X_train))\n",
    "        # apply same transformation to test data\n",
    "        X_test = pd.DataFrame(scaler.transform(X_test))\n",
    "        #Restoring indices after scaling\n",
    "        X_train.index = train_index \n",
    "        X_test.index = test_index \n",
    "        #Restoring features names\n",
    "        X_train.columns = cols\n",
    "        X_test.columns = cols\n",
    "\n",
    "    #No down-sampling\n",
    "    X_train_sampled = X_train\n",
    "    y_train_sampled = y_train\n",
    "    \n",
    "    #Shuffle training data rows\n",
    "    rseed=0\n",
    "    np.random.seed(rseed)\n",
    "    idx_perm_train = np.random.permutation(X_train_sampled.index)\n",
    "    X_train_sampled_perm = X_train_sampled.reindex(idx_perm_train)\n",
    "    y_train_sampled_perm = y_train_sampled.reindex(idx_perm_train)\n",
    "        \n",
    "    #Shuffle test data rows\n",
    "    np.random.seed(rseed)\n",
    "    idx_perm_test = np.random.permutation(X_test.index)\n",
    "    X_test_perm = X_test.reindex(idx_perm_test)\n",
    "    y_test_perm = y_test.reindex(idx_perm_test)\n",
    "    \n",
    "    #pos and neg numbers in the training\n",
    "    no_pos = np.count_nonzero(y_train_sampled[\"label\"] == 1)\n",
    "    no_neg = np.count_nonzero(y_train_sampled[\"label\"] == 0)\n",
    "    \n",
    "    #Generate model\n",
    "    model = models_dict[classifier][ligand][int(heldout_fold)]\n",
    "    \n",
    "    if classifier == \"NN\":   \n",
    "        model = model.to(device=curr_device)\n",
    "        #weight vector for NN\n",
    "        if model.weight == \"balanced\":              \n",
    "            #weight vector\n",
    "            neg_weight = float(no_pos) / float(no_neg + no_pos) \n",
    "            pos_weight = 1 - neg_weight\n",
    "        elif model.weight == 0.1:\n",
    "            neg_weight = 10\n",
    "            pos_weight = 1\n",
    "        elif model.weight == None:\n",
    "            neg_weight = 1\n",
    "            pos_weight = 1\n",
    "\n",
    "        weight = torch.Tensor([neg_weight, pos_weight]).to(device=curr_device)\n",
    "        model.fit(X_train_sampled, y_train_sampled[\"label\"], weight)\n",
    "        probs_list = model.predict_proba(X_test)\n",
    "    \n",
    "    else:\n",
    "        model.fit(X_train_sampled, y_train_sampled[\"label\"])\n",
    "        probs_list = []\n",
    "        probs = model.predict_proba(X_test)\n",
    "        for l in probs:\n",
    "            probs_list.append(l[1])\n",
    "\n",
    "    pred_dict[\"obs\"].extend(y_test[\"label\"])\n",
    "    pred_dict[\"prob\"].extend(probs_list)\n",
    "    fold_list = [test_idx] * len(probs_list)\n",
    "    pred_dict[\"fold\"].extend(fold_list)\n",
    "\n",
    "    model_list = [classifier] * len(probs_list)\n",
    "    pred_dict[\"model\"].extend(model_list)\n",
    "    \n",
    "    #Adding the position number to the table to help with analysis\n",
    "    pred_dict[\"idx\"].extend(test_index)\n",
    "\n",
    "    print \"Finished \"+ligand+\" \"+classifier+\" heldout fold: \"+heldout_fold+\" test fold: \"+test_fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "heldout fold #: 5\n",
      "all training size = (31655, 758)\n",
      "test fold #: 4\n",
      "train size = (23758, 758)\n",
      "test size = (7897, 758)\n",
      "Finished ion XGB heldout fold: 5 test fold: 4\n"
     ]
    }
   ],
   "source": [
    "pred_dict = defaultdict(list)\n",
    "\n",
    "test_model_iterative_fixed(pred_dict, training_positivies, training_negatives, ligand)\n",
    "pred_df = pd.DataFrame.from_dict(pred_dict)\n",
    "\n",
    "#Save to file\n",
    "pred_df.to_csv(curr_dir+\"/stacked_oof_predictions/\"+heldout_fold+\"/\"+ligand+\"_\"+classifier_method+\"_test_fold\"+test_fold+\"_\"+str(folds_num)+\"w.csv\", sep='\\t')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
