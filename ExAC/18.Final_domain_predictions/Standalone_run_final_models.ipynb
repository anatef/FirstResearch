{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Basic imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from collections import defaultdict\n",
    "import sys\n",
    "from os import getcwd\n",
    "import subprocess\n",
    "\n",
    "curr_dir = getcwd()\n",
    "#14.Final model utils functions\n",
    "sys.path.append(curr_dir+\"/../14.Final_Model/utils\")\n",
    "from final_model_funcs import ligands, all_models_list\n",
    "\n",
    "sys.path.append(curr_dir+\"/../10.Prediction/stacking/utils\")\n",
    "from stacking_funcs import create_stacked_dataset\n",
    "\n",
    "sys.path.append(curr_dir+\"/../10.Prediction/utils\")\n",
    "from prediction_general_funcs import get_features_cols, remove_unimportant_features\n",
    "from tuning_helper_functions import models_req_scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pickled_model(ens_model, ens_ligand, stacked=False, ens_dir=\"\"):\n",
    "    \n",
    "    #Get model filename\n",
    "    if (stacked):\n",
    "        models_path = curr_dir+\"/stacked_pik_models/\"+ens_dir\n",
    "    else:\n",
    "        models_path = curr_dir+\"/pik_models\"\n",
    "    \n",
    "    cmd = \"ls \"+models_path+\" | grep \"+ens_ligand+\"_\"+ens_model+\"* | cut -d'_' -f3 | cut -d'.' -f1\"\n",
    "    ls_cmd_out = subprocess.Popen(cmd, shell=True, stdout=subprocess.PIPE)\n",
    "    for line in ls_cmd_out.stdout.readlines():\n",
    "        trial = line[:-1]\n",
    "        break\n",
    "    \n",
    "    #Read the pickeled model\n",
    "    print \"Reading \"+ens_ligand+\"_\"+ens_model+\"_\"+trial+\".pik\"\n",
    "    with open(models_path+\"/\"+ens_ligand+\"_\"+ens_model+\"_\"+trial+\".pik\", 'rb') as handle:\n",
    "        pik_model = pickle.load(handle)\n",
    "    \n",
    "    return (pik_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predcit_using_pickeled_model(pred_dict, pik_model, classifier_method, data, stacked=False):\n",
    "    \n",
    "    #Scale the data if the classifier is one of: SVM, LG, NN\n",
    "    data_index = data.index\n",
    "    if (classifier_method in models_req_scaling):\n",
    "        cols = data.columns\n",
    "        #Read the saved Scaler\n",
    "        if (stacked):\n",
    "            with open(curr_dir+\"/stacked_pik_models/\"+ens+\"/scaler.pik\", 'rb') as handle:\n",
    "                scaler = pickle.load(handle)\n",
    "        else:\n",
    "            with open(curr_dir+\"/pik_models/scaler.pik\", 'rb') as handle:\n",
    "                scaler = pickle.load(handle)\n",
    "        # apply same transformation to data\n",
    "        data = pd.DataFrame(scaler.transform(data))\n",
    "        #Restoring indices after scaling\n",
    "        data.index = data_index \n",
    "        #Restoring features names\n",
    "        data.columns = cols\n",
    "        \n",
    "    #Predict using the pickeled model\n",
    "    \n",
    "    probs = pik_model.predict_proba(data)\n",
    "    if (classifier_method == \"NN\"): \n",
    "        probs_list = probs\n",
    "    else:\n",
    "        probs_list = []\n",
    "        for l in probs:\n",
    "            probs_list.append(l[1])\n",
    "    \n",
    "    #Arrange predictions in the output dictionary\n",
    "    pred_dict[\"idx\"].extend(data_index)\n",
    "    pred_dict[\"prob\"].extend(probs_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_stacked_dataset(stacking_path, stacking_ligands, stacking_models, features_data, stacking_filename, keep_original_features=True):\n",
    "    \n",
    "    df_stacking_combined = pd.DataFrame()\n",
    "    \n",
    "    for stack_ligand in stacking_ligands:\n",
    "        for stack_model in stacking_models:\n",
    "\n",
    "            #Read the stacking-1st level probs of all the ligands\n",
    "            staking1_filename = stack_ligand+\"_\"+stack_model+\"_\"+stacking_filename+\".csv\"\n",
    "            stacking1_df = pd.read_csv(stacking_path+staking1_filename, sep='\\t',index_col=0)\n",
    "            stacking1_df.index = stacking1_df[\"idx\"]\n",
    "            stacking1_df.columns = [\"idx\", stack_model+\"_\"+stack_ligand+\"_prob\"]\n",
    "    \n",
    "            #Add to the combined df tables\n",
    "            if (df_stacking_combined.shape[0] == 0):\n",
    "                df_stacking_combined = stacking1_df\n",
    "            else:\n",
    "                df_stacking_combined = pd.merge(df_stacking_combined, stacking1_df, on=\"idx\")\n",
    "    \n",
    "    #Remving the idx column after all the merging\n",
    "    df_stacking_combined.index = stacking1_df[\"idx\"]\n",
    "    del df_stacking_combined[\"idx\"]\n",
    "    \n",
    "    #Adding the original features\n",
    "    if (keep_original_features):\n",
    "        df_stacking_combined = pd.concat([df_stacking_combined, features_data], axis=1)\n",
    "    \n",
    "    print \"#(features) = \"+str(df_stacking_combined.shape[1])\n",
    "    \n",
    "    return df_stacking_combined"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading flags input and determine run parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ligand = sm\n",
      "ens = ALL\n",
      "layer = 2\n"
     ]
    }
   ],
   "source": [
    "#Reading the ligand input\n",
    "try:\n",
    "    ligand = environ['ligand']\n",
    "except:\n",
    "    ligand = \"sm\"\n",
    "print \"ligand = \"+ligand\n",
    "\n",
    "#Reading the ensemble type\n",
    "try: \n",
    "    ens = environ['ens']\n",
    "except:\n",
    "    ens = \"ALL\"\n",
    "print \"ens = \"+ens\n",
    "\n",
    "#Reading stacking layer\n",
    "try:\n",
    "    layer = environ['layer']\n",
    "except:\n",
    "    layer = \"2\"\n",
    "print \"layer = \"+layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'models': ['XGB', 'RF', 'SVM', 'Logistic', 'NN'], 'ligands': ['dna', 'rna', 'ion', 'peptide', 'sm']}\n"
     ]
    }
   ],
   "source": [
    "#Determine models and ligands based on ensemble type\n",
    "hyperparameters = dict()\n",
    "#all_models_list_change_order = [\"SVM\", \"XGB\", \"NN\", \"RF\", \"Logistic\"]\n",
    "if (ens == \"LIGAND\"):\n",
    "    hyperparameters[\"models\"] = all_models_list\n",
    "    hyperparameters[\"ligands\"] = [ligand]\n",
    "    out_dir = \"ligand_features_probs\"\n",
    "elif (ens == \"MODEL\"):\n",
    "    hyperparameters[\"models\"] = [\"XGB\"]\n",
    "    hyperparameters[\"ligands\"] = ligands\n",
    "    out_dir = \"model_features\"\n",
    "elif (ens == \"ALL\"):\n",
    "    hyperparameters[\"models\"] = all_models_list\n",
    "    hyperparameters[\"ligands\"] = ligands\n",
    "    out_dir = \"all_features_probs\"\n",
    "else:\n",
    "    hyperparameters[\"models\"] = all_models_list\n",
    "    hyperparameters[\"ligands\"] = ligands\n",
    "    out_dir = \"just_probs\"\n",
    "print hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get features data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test samples positions #: 7102\n"
     ]
    }
   ],
   "source": [
    "#Get data\n",
    "filename_number = \"fixed\"\n",
    "data_filename = \"windowed_positions_features_\"+filename_number+\"_10.24.19.csv\"\n",
    "data_path = curr_dir+\"/../9.Features_exploration/features_tables_v32/\"\n",
    "features_data = pd.read_csv(data_path+data_filename, sep='\\t', index_col=0)\n",
    "print \"test samples positions #: \"+str(features_data.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove the domains with spider problems\n",
    "if (filename_number != \"fixed\"):\n",
    "    spider_problems_domains_list = [\"Ank_2\", \"Ank_4\", \"Ank_5\", \"Asp\", \"CD45\", \"Cys_knot\", \"DENN\", \"DUF1908\", \"DUF4187\", \"EF-hand_1\", \"EF-hand_5\", \"EF-hand_6\", \"EF-hand_7\", \"EF-hand_8\", \"EF-hand_9\", \"EFhand_Ca_insen\", \"ELM2\", \"Exo_endo_phos\", \"FYVE\", \"G-patch\", \"G-patch_2\", \"GRAM\", \"GSHPx\", \"IQ_SEC7_PH\", \"LRR_1\", \"LRR_12\", \"LRR_6\", \"LRR_8\", \"MFS_1\", \"Myb_DNA-binding\", \"Myotub-related\", \"PDZ\", \"PDZ_6\", \"PH\", \"PNMA\", \"PTP_N\", \"Pkinase\", \"Pkinase_Tyr\", \"RNase_T\", \"RUN\", \"Rdx\", \"SBF2\", \"SKICH\", \"Sec7\", \"SelP_C\", \"SelP_N\", \"SelR\", \"Sep15_SelM\", \"T4_deiodinase\", \"TAXi_N\", \"TIR\", \"Trefoil\", \"V-set\", \"WAP\", \"Y_phosphatase\", \"dDENN\", \"fn3\", \"uDENN\", \"zf-C2H2\", \"zf-C2H2_4\", \"zf-CCCH\", \"zf-H2C2_5\"]\n",
    "    features_data = features_data[~features_data[\"domain_name\"].isin(spider_problems_domains_list)]\n",
    "    print \"test samples positions #: \"+str(features_data.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of features before removal: 761\n",
      "# of features after removal: 753\n"
     ]
    }
   ],
   "source": [
    "#Get list of features that we use\n",
    "features_cols = get_features_cols(features_data)\n",
    "print \"# of features before removal: \"+str(len(features_cols))\n",
    "remove_unimportant_features(features_data, features_cols, update_features_cols=True)\n",
    "print \"# of features after removal: \"+str(len(features_cols))\n",
    "\n",
    "#Filter data to just these features\n",
    "features_data = features_data.loc[:,features_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in features_data.columns:\n",
    "    if (col == \"domain_name\"):\n",
    "        continue\n",
    "    nan_idx = np.where(np.isnan(features_data[col].tolist()) == True)[0]\n",
    "    if (len(nan_idx) > 0):\n",
    "        print col+\" has NaNs\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1st layer predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get Models\n",
    "if (layer == \"1\"):\n",
    "    first_layer_models = dict()\n",
    "    for ens_model in hyperparameters[\"models\"]:\n",
    "        if (ens_model == \"NN\"):\n",
    "            continue #This should be run on the gpu\n",
    "        for ens_ligand in hyperparameters[\"ligands\"]:\n",
    "            key_str = ens_ligand+\"_\"+ens_model\n",
    "            first_layer_models[key_str] = get_pickled_model(ens_model, ens_ligand)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Predict using all the models for this ligand\n",
    "if (layer == \"1\"):\n",
    "    for ligand_model_key in first_layer_models.keys():\n",
    "\n",
    "        classifier_method = ligand_model_key.split(\"_\")[1]\n",
    "        pred_dict = defaultdict(list)\n",
    "        predcit_using_pickeled_model(pred_dict, first_layer_models[ligand_model_key], classifier_method, features_data)\n",
    "        pred_df = pd.DataFrame.from_dict(pred_dict)                  \n",
    "\n",
    "        pred_df.to_csv(curr_dir+\"/1st_level_pred/\"+ligand_model_key+\"_\"+filename_number+\".csv\", sep='\\t')\n",
    "        print \"Finished predicting using \"+ligand_model_key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2nd layer predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading sm_XGB_690.pik\n"
     ]
    }
   ],
   "source": [
    "#Get the stacked model\n",
    "if (layer == \"2\"):\n",
    "    second_layer_model = get_pickled_model(\"XGB\", ligand, stacked=True, ens_dir=ens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#(features) = 778\n"
     ]
    }
   ],
   "source": [
    "#Create the 2nd layer features table\n",
    "if (layer == \"2\"):\n",
    "    stacking_path = curr_dir+\"/1st_level_pred/\"\n",
    "    stacked_features_df = create_stacked_dataset(stacking_path, hyperparameters[\"ligands\"], hyperparameters[\"models\"], features_data, filename_number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished predicting using sm_ALL\n"
     ]
    }
   ],
   "source": [
    "#Predict using the stacked model\n",
    "if (layer == \"2\"):\n",
    "    pred_dict = defaultdict(list)\n",
    "    predcit_using_pickeled_model(pred_dict, second_layer_model, \"XGB\", stacked_features_df, stacked=True)\n",
    "    pred_df = pd.DataFrame.from_dict(pred_dict) \n",
    "    \n",
    "    pred_df.to_csv(curr_dir+\"/2nd_level_pred/\"+ligand+\"_\"+ens+\"_\"+filename_number+\".csv\", sep='\\t')\n",
    "    print \"Finished predicting using \"+ligand+\"_\"+ens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
